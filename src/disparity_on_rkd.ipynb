{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cc7ce15-50bd-423d-ac58-c2bd0eb916ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n",
      "Training Teacher with Lambda Value of 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Training: 100%|█████████████████████████████████████████████████████████████████| 162/162 [01:53<00:00,  1.42it/s]\n",
      "Epoch 1/2, Validation: 100%|█████████████████████████████████████████████████████████████████| 54/54 [00:18<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEACHER - Lambda 0 - Epoch 1 Metrics:\n",
      "--------------------------------------------------\n",
      "TRAINING Accuracy: 0.507369, VALIDATION Accuracy: 0.7709\n",
      "TRAINING Disparity: 0.090466, VALIDATION Disparity: 0.0970\n",
      "TRAINING Main Loss: 1.613124, VALIDATION Main Loss: 0.8017\n",
      "--------------------------------------------------\n",
      "\n",
      "Class Team_Sports: Val Disparity = 0.016137566137566006\n",
      "Class Celebration: Val Disparity = 0.08019216555801922\n",
      "Class Parade: Val Disparity = 0.1138861138861138\n",
      "Class Waiter_Or_Waitress: Val Disparity = -0.0010504201680674452\n",
      "Class Individual_Sports: Val Disparity = -0.15587956377430046\n",
      "Class Surgeons: Val Disparity = 0.034965034965035\n",
      "Class Spa: Val Disparity = 0.05128205128205132\n",
      "Class Law_Enforcement: Val Disparity = 0.020016339869281086\n",
      "Class Business: Val Disparity = 0.31356209150326786\n",
      "Class Dresses: Val Disparity = -0.1174777603349032\n",
      "Class Water_Activities: Val Disparity = 0.12236445783132532\n",
      "Class Picnic: Val Disparity = -0.04999999999999993\n",
      "Class Rescue: Val Disparity = 0.21494404213298224\n",
      "Class Cheering: Val Disparity = 0.03956700261291535\n",
      "Class Performance_And_Entertainment: Val Disparity = 0.16002747252747251\n",
      "Class Family: Val Disparity = 0.06095924870031855\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Training: 100%|█████████████████████████████████████████████████████████████████| 162/162 [01:51<00:00,  1.45it/s]\n",
      "Epoch 2/2, Validation: 100%|█████████████████████████████████████████████████████████████████| 54/54 [00:17<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TEACHER - Lambda 0 - Epoch 2 Metrics:\n",
      "--------------------------------------------------\n",
      "TRAINING Accuracy: 0.729302, VALIDATION Accuracy: 0.9032\n",
      "TRAINING Disparity: 0.091760, VALIDATION Disparity: 0.0680\n",
      "TRAINING Main Loss: 0.890763, VALIDATION Main Loss: 0.3764\n",
      "--------------------------------------------------\n",
      "\n",
      "Class Team_Sports: Val Disparity = -0.005026455026455157\n",
      "Class Celebration: Val Disparity = 0.059543606799704385\n",
      "Class Parade: Val Disparity = 0.020535020535020543\n",
      "Class Waiter_Or_Waitress: Val Disparity = -0.040966386554621925\n",
      "Class Individual_Sports: Val Disparity = -0.018018018018018167\n",
      "Class Surgeons: Val Disparity = 0.006993006993007089\n",
      "Class Spa: Val Disparity = 0.0\n",
      "Class Law_Enforcement: Val Disparity = 0.05964052287581689\n",
      "Class Business: Val Disparity = 0.20081699346405235\n",
      "Class Dresses: Val Disparity = -0.09497645211930916\n",
      "Class Water_Activities: Val Disparity = 0.15850903614457845\n",
      "Class Picnic: Val Disparity = 0.030952380952380953\n",
      "Class Rescue: Val Disparity = 0.17972350230414746\n",
      "Class Cheering: Val Disparity = -0.12280701754385959\n",
      "Class Performance_And_Entertainment: Val Disparity = 0.013221153846153966\n",
      "Class Family: Val Disparity = 0.007965789032366333\n",
      "==================================================\n",
      "\n",
      "Finished Training TEACHER with lambda value of 0. Best epoch number: 2\n"
     ]
    }
   ],
   "source": [
    "from utils.disparity_tools import one_hot_encode, calculate_recall_multiclass, evaluate_model_with_gender_multiclass \n",
    "from train_teacher import train_teacher\n",
    "from data_tools.wider_dataloader import remap_classes, custom_collate, make_wider_datasets\n",
    "from utils.disparity_tools import one_hot_encode, calculate_recall_multiclass, evaluate_model_with_gender_multiclass \n",
    "import critic\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from torchvision.models import EfficientNet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import warnings\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters / Inputs\n",
    "teacher_learning_rate = 0.0005 # 0.096779\n",
    "student_learning_rate = teacher_learning_rate/5\n",
    "teacher_epochs = 2\n",
    "student_epochs = teacher_epochs\n",
    "teacher_patience = 1\n",
    "student_patience = 10\n",
    "temperature = 4.0\n",
    "alpha = 0.9\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "weight_decay = 1e-5\n",
    "epsilon = 0.05\n",
    "margin = 0.01\n",
    "num_classes = 16\n",
    "base_save_dir = \"Test_Dir2\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# list of lambda values to loop through for grid search\n",
    "teacher_lambda_factor_list = [0]\n",
    "student_lambda_factor_list = [0.5,1,1.5,2,3]\n",
    "\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = make_wider_datasets()\n",
    "\n",
    "# store class id mappings\n",
    "class_idx = remap_classes()\n",
    "\n",
    "num_classes = 16\n",
    "\n",
    "# Create dict to store best model states\n",
    "teacher_model_states_best = {}\n",
    "student_model_states_best = {}\n",
    "\n",
    "# Loop through the lambda_factor_list for teacher debiasing\n",
    "for lambda_factor in teacher_lambda_factor_list:\n",
    "    \n",
    "    # Load EfficientNet B3 model for Teacher\n",
    "    teacher = models.efficientnet_b3(pretrained=True)\n",
    "    \n",
    "    # Determine the number of output features from the feature extractor part of EfficientNet B3\n",
    "    num_ftrs = teacher.classifier[1].in_features\n",
    "    \n",
    "    # Modify the classifier layer of the EfficientNet model to match the number of classes\n",
    "    teacher.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "    \n",
    "    # Redefine the main model optimizer if needed\n",
    "    teacher_optimizer = optim.Adam(teacher.parameters(), lr=teacher_learning_rate)\n",
    "    teacher_loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Train the teacher \n",
    "    best_model_state = train_teacher(teacher, teacher_optimizer, teacher_loss_fn, batch_size, train_dataset, test_dataset, lambda_factor, num_classes, class_idx,\n",
    "                                   teacher_patience, teacher_epochs, device, base_save_dir=base_save_dir, plot = False)\n",
    "    \n",
    "    teacher_model_states_best[lambda_factor] = best_model_state\n",
    "\n",
    "# Save the collective best model states to a file\n",
    "teacher_collective_save_path = os.path.join(base_save_dir, 'teacher_model_states_best.pth')\n",
    "torch.save(teacher_model_states_best, teacher_collective_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d39977-e400-47bf-a204-0d025b0b4c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# import Models\n",
    "# from Models.embtrans_cifar import EmbTrans\n",
    "# from Dataset import CIFAR\n",
    "# from utils.norm_dir_utils import colorstr, Save_Checkpoint, AverageMeter, DirectNormLoss, KDLoss\n",
    "from data_tools.wider_dataloader import remap_classes, custom_collate, make_wider_datasets\n",
    "from utils.disparity_tools import one_hot_encode, calculate_recall_multiclass, evaluate_model_with_gender_multiclass \n",
    "import critic\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pdb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from torchvision.models import EfficientNet\n",
    "\n",
    "\n",
    "\n",
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    inputs, _ = next(iter(dataloader))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _, teacher_outputs = teacher(inputs)\n",
    "    teacher_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _, student_outputs = student(inputs)\n",
    "    student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n",
    "\n",
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        with torch.no_grad():\n",
    "            _, teacher_outputs = teacher(inputs.to(device))\n",
    "            _, student_outputs = student(inputs.to(device))\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(torch.argmax(teacher_outputs, dim=1).cpu().numpy())\n",
    "        all_student_preds.append(torch.argmax(student_outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),  # Updated line\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def train(student, teacher, T_EMB, train_dataloader, optimizer, criterion, kd_loss, nd_loss, args, epoch):\n",
    "    train_loss = AverageMeter()\n",
    "    train_error = AverageMeter()\n",
    "\n",
    "    Cls_loss = AverageMeter()\n",
    "    Div_loss = AverageMeter()\n",
    "    Norm_Dir_loss = AverageMeter()\n",
    "\n",
    "    # Model on train mode\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    step_per_epoch = len(train_dataloader)\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_dataloader):\n",
    "        start = time.time()\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # compute output\n",
    "        s_emb, s_logits = student(images, embed=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_emb, t_logits = teacher(images, embed=True)\n",
    "\n",
    "        # cls loss\n",
    "        cls_loss = criterion(s_logits, labels) * args.cls_loss_factor\n",
    "        # KD loss\n",
    "        div_loss = kd_loss(s_logits, t_logits) * min(1.0, epoch/args.warm_up)\n",
    "        # ND loss\n",
    "        norm_dir_loss = nd_loss(s_emb=s_emb, t_emb=t_emb, T_EMB=T_EMB, labels=labels)\n",
    "\n",
    "        loss = cls_loss + div_loss + norm_dir_loss\n",
    "        # measure accuracy and record loss\n",
    "        batch_size = images.size(0)\n",
    "        _, pred = s_logits.data.cpu().topk(1, dim=1)\n",
    "        train_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "        train_loss.update(loss.item(), batch_size)\n",
    "\n",
    "        Cls_loss.update(cls_loss.item(), batch_size)\n",
    "        Div_loss.update(div_loss.item(), batch_size)\n",
    "        Norm_Dir_loss.update(norm_dir_loss.item(), batch_size)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "        s1 = '\\r{} [{}/{}]'.format(t, step+1, step_per_epoch)\n",
    "        s2 = ' - {:.2f}ms/step - nd_loss: {:.3f} - kd_loss: {:.3f} - cls_loss: {:.3f} - train_loss: {:.3f} - train_acc: {:.3f}'.format(\n",
    "             1000 * (time.time() - start), norm_dir_loss.item(), div_loss.item(), cls_loss.item(), train_loss.val, 1-train_error.val)\n",
    "\n",
    "        print(s1+s2, end='', flush=True)\n",
    "\n",
    "    print()\n",
    "    return Norm_Dir_loss.avg, Div_loss.avg, Cls_loss.avg, train_loss.avg, train_error.avg\n",
    "\n",
    "\n",
    "def test(student, test_dataloader, criterion):\n",
    "    test_loss = AverageMeter()\n",
    "    test_error = AverageMeter()\n",
    "\n",
    "    # Model on eval mode\n",
    "    student.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # compute logits\n",
    "            logits = student(images, embed=False)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            batch_size = images.size(0)\n",
    "            _, pred = logits.data.cpu().topk(1, dim=1)\n",
    "            test_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "            test_loss.update(loss.item(), batch_size)\n",
    "    \n",
    "\n",
    "    return test_loss.avg, test_error.avg\n",
    "\n",
    "\n",
    "def epoch_loop(student, teacher, train_set, test_set, args):\n",
    "    # data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=args.workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=args.workers)\n",
    "\n",
    "    # student\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # student = nn.DataParallel(student, device_ids=args.gpus)\n",
    "    student = nn.DataParallel(student)\n",
    "    student.to(device)\n",
    "    # teacher = nn.DataParallel(teacher, device_ids=args.gpus)\n",
    "    teacher = nn.DataParallel(teacher)\n",
    "    teacher.to(device)\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    kd_loss = KDLoss(kl_loss_factor=args.kd_loss_factor, T=args.t).to(device)\n",
    "    nd_loss = DirectNormLoss(num_class=100, nd_loss_factor=args.nd_loss_factor).to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.SGD(params=student.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)\n",
    "\n",
    "    # weights\n",
    "    save_dir = Path(args.save_dir)\n",
    "    weights = save_dir / 'weights'\n",
    "    weights.mkdir(parents=True, exist_ok=True)\n",
    "    last = weights / 'last'\n",
    "    best = weights / 'best'\n",
    "\n",
    "    # acc,loss\n",
    "    acc_loss = save_dir / 'acc_loss'\n",
    "    acc_loss.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_acc_savepath = acc_loss / 'train_acc.npy'\n",
    "    train_loss_savepath = acc_loss / 'train_loss.npy'\n",
    "    val_acc_savepath = acc_loss / 'val_acc.npy'\n",
    "    val_loss_savepath = acc_loss / 'val_loss.npy'\n",
    "\n",
    "    # tensorboard\n",
    "    logdir = save_dir / 'logs'\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "    summary_writer = SummaryWriter(logdir, flush_secs=120)\n",
    "\n",
    "    # resume\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        student.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_error = checkpoint['best_error']\n",
    "        train_acc = checkpoint['train_acc']\n",
    "        train_loss = checkpoint['train_loss']\n",
    "        test_acc = checkpoint['test_acc']\n",
    "        test_loss = checkpoint['test_loss']\n",
    "        logger.info(colorstr('green', 'Resuming training from {} epoch'.format(start_epoch)))\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_error = 0\n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        test_acc = []\n",
    "        test_loss = []\n",
    "        test_precision = []\n",
    "        test_recall = []\n",
    "        test_f1 = []\n",
    "\n",
    "    # Train student\n",
    "    best_error = 1\n",
    "    ##\n",
    "    patience = args.patience\n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = float('inf')\n",
    "    epoch_val_losses = []\n",
    "    epoch_val_accuracies = []\n",
    "    ##\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        if epoch in [150, 180, 210]:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, args.epochs))\n",
    "        norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error = train(student=student,\n",
    "                                                                                 teacher=teacher,\n",
    "                                                                                 T_EMB=T_EMB,\n",
    "                                                                                 train_dataloader=train_loader,\n",
    "                                                                                 optimizer=optimizer,\n",
    "                                                                                 criterion=criterion,\n",
    "                                                                                 kd_loss=kd_loss,\n",
    "                                                                                 nd_loss=nd_loss,\n",
    "                                                                                 args=args,\n",
    "                                                                                 epoch=epoch)\n",
    "        test_epoch_loss, test_error = test(student=student,\n",
    "                                        test_dataloader=test_loader,\n",
    "                                        criterion=criterion)\n",
    "\n",
    "        s = \"Train Loss: {:.3f}, Train Acc: {:.3f}, Test Loss: {:.3f}, Test Acc: {:.3f}, lr: {:.5f}\".format(\n",
    "            train_epoch_loss, 1-train_error, test_epoch_loss, 1-test_error, optimizer.param_groups[0]['lr'])\n",
    "        logger.info(colorstr('green', s))\n",
    "\n",
    "        # save acc,loss\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        train_acc.append(1-train_error)\n",
    "        test_loss.append(test_epoch_loss)\n",
    "        test_acc.append(1-test_error)\n",
    "\n",
    "        epoch_val_accuracies.append(1-test_error)\n",
    "        epoch_val_losses.append(test_epoch_loss)\n",
    "\n",
    "        # save student\n",
    "        is_best = test_error < best_error\n",
    "        best_error = min(best_error, test_error)\n",
    "        state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': student.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_error': best_error,\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'test_acc': test_acc,\n",
    "                'test_loss': test_loss,\n",
    "            }\n",
    "\n",
    "        last_path = last / 'epoch_{}_loss_{:.3f}_acc_{:.3f}'.format(\n",
    "            epoch + 1, test_epoch_loss, 1-test_error)\n",
    "        best_path = best / 'epoch_{}_acc_{:.3f}'.format(\n",
    "                epoch + 1, 1-best_error)\n",
    "\n",
    "        Save_Checkpoint(state, last, last_path, best, best_path, is_best)\n",
    "\n",
    "        # tensorboard\n",
    "        if epoch == 1:\n",
    "            images, labels = next(iter(train_loader))\n",
    "            img_grid = torchvision.utils.make_grid(images)\n",
    "            summary_writer.add_image('Cifar Image', img_grid)\n",
    "        summary_writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "        summary_writer.add_scalar('train_loss', train_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('train_error', train_error, epoch)\n",
    "        summary_writer.add_scalar('val_loss', test_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('val_error', test_error, epoch)\n",
    "\n",
    "        summary_writer.add_scalar('nd_loss', norm_dir_loss, epoch)\n",
    "        summary_writer.add_scalar('kd_loss', div_loss, epoch)\n",
    "        summary_writer.add_scalar('cls_loss', cls_loss, epoch)\n",
    "\n",
    "        ### \n",
    "        # Check if current validation combined loss is lower than the best combined loss\n",
    "        if test_epoch_loss < best_val_loss:\n",
    "            best_val_loss = test_epoch_loss\n",
    "            best_val_accuracy = 1-test_error\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            # print results\n",
    "            best_checkpoint = torch.load(os.path.join(best_path, 'ckpt.pth'))['model_state_dict']\n",
    "            student.load_state_dict(best_checkpoint)\n",
    "            metrics = compare_performance_metrics(teacher, student, test_loader)\n",
    "            teacher_time, student_time = compare_inference_time(teacher, student, test_loader)\n",
    "            teacher_size, student_size = compare_model_size(teacher, student)\n",
    "\n",
    "            final_report_banner = '- - - - - METRICS REPORT - - - - -'\n",
    "            teacher_metrics = \"TEACHER: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, teacher_inf: {:.3f}, teacher_size: {:.3f}\".format(\n",
    "                metrics['accuracy'][0], metrics['precision'][0], metrics['recall'][0], metrics['f1'][0], \n",
    "                teacher_time, teacher_size,)\n",
    "            student_metrics = \"STUDENT: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, student_inf: {:.3f}, student_size: {:.3f}\".format(\n",
    "                metrics['accuracy'][1], metrics['precision'][1], metrics['recall'][1], metrics['f1'][1], \n",
    "                 student_time, student_size)\n",
    "            logger.info(colorstr('green', final_report_banner))\n",
    "            logger.info(colorstr('green', teacher_metrics))\n",
    "            logger.info(colorstr('green', student_metrics))\n",
    "            break\n",
    "\n",
    "        if epoch == (args.epochs - 1):\n",
    "            best_checkpoint = torch.load(os.path.join(best_path, 'ckpt.pth'))['model_state_dict']\n",
    "            student.load_state_dict(best_checkpoint)\n",
    "            metrics = compare_performance_metrics(teacher, student, test_loader)\n",
    "            teacher_time, student_time = compare_inference_time(teacher, student, test_loader)\n",
    "            teacher_size, student_size = compare_model_size(teacher, student)\n",
    "\n",
    "            final_report_banner = '- - - - - METRICS REPORT - - - - -'\n",
    "            teacher_metrics = \"TEACHER: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, teacher_inf: {:.3f}, teacher_size: {:.3f}\".format(\n",
    "                100*metrics['accuracy'][0], 100*metrics['precision'][0], 100*metrics['recall'][0], 100*metrics['f1'][0], \n",
    "                teacher_time, teacher_size,)\n",
    "            student_metrics = \"STUDENT: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, student_inf: {:.3f}, student_size: {:.3f}\".format(\n",
    "                100*metrics['accuracy'][1], 100*metrics['precision'][1], 100*metrics['recall'][1], 100*metrics['f1'][1], \n",
    "                 student_time, student_size)\n",
    "            logger.info(colorstr('green', final_report_banner))\n",
    "            logger.info(colorstr('green', teacher_metrics))\n",
    "            logger.info(colorstr('green', student_metrics))\n",
    "            \n",
    "        \n",
    "        ###\n",
    "\n",
    "    summary_writer.close()\n",
    "    if not os.path.exists(train_acc_savepath) or not os.path.exists(train_loss_savepath):\n",
    "        np.save(train_acc_savepath, train_acc)\n",
    "        np.save(train_loss_savepath, train_loss)\n",
    "        np.save(val_acc_savepath, test_acc)\n",
    "        np.save(val_loss_savepath, test_loss)\n",
    "    \n",
    "    return student, teacher, test_loader\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    student_names = sorted(name for name in Models.__dict__\n",
    "                         if name.islower() and not name.startswith(\"__\")\n",
    "                         and callable(Models.__dict__[name]))\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Cifar Training')\n",
    "    parser.add_argument('-f') # add to make this run in collab\n",
    "    parser.add_argument(\"--student_name\", type=str, default=\"efficientnetb0\", choices=student_names, help=\"student architecture\")\n",
    "    # parser.add_argument(\"--dataset\", type=str, default='cifar100')\n",
    "    parser.add_argument(\"--epochs\", type=int, default=15)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size per gpu\")\n",
    "    parser.add_argument('--workers', default=32, type=int, help='number of data loading workers')\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.1)\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum')\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=5e-4)\n",
    "\n",
    "    parser.add_argument(\"--teacher\", type=str, default=\"efficientnetb3\", help=\"teacher architecture\")\n",
    "    # parser.add_argument(\"--teacher_weights\", type=str, default=\"./ckpt/cifar_teachers/resnet56_vanilla/ckpt_epoch_240.pth\", help=\"teacher weights path\")\n",
    "    parser.add_argument(\"--cls_loss_factor\", type=float, default=1.0, help=\"cls loss weight factor\")\n",
    "    parser.add_argument(\"--kd_loss_factor\", type=float, default=1.0, help=\"KD loss weight factor\")\n",
    "    parser.add_argument(\"--t\", type=float, default=4.0, help=\"temperature\")\n",
    "    parser.add_argument(\"--nd_loss_factor\", type=float, default=1.0, help=\"ND loss weight factor\")\n",
    "    parser.add_argument(\"--warm_up\", type=float, default=20.0, help='loss weight warm up epochs')\n",
    "    parser.add_argument(\"--patience\", type=int, default=5, help='loss weight warm up epochs')\n",
    "    parser.add_argument(\"--teacher_lambda_factor_list\", type=list, default=[0,0.5,1,2], help='lambda factor to reduce disparity')\n",
    "\n",
    "    # parser.add_argument(\"--gpus\", type=list, default=[0, 1])\n",
    "    \n",
    "    parser.add_argument('--seed', default=None, type=int, help='seed for initializing training.')\n",
    "    parser.add_argument(\"--resume\", type=str, help=\"best ckpt's path to resume most recent training\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"./run/KD++\", help=\"save path, eg, acc_loss, weights, tensorboard, and so on\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        cudnn.benchmark = False\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s [line:%(lineno)d] %(message)s', \n",
    "                        datefmt='%d %b %Y %H:%M:%S')\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # args.batch_size = args.batch_size * len(args.gpus)\n",
    "    args.batch_size = args.batch_size * 1\n",
    "\n",
    "    # logger.info(colorstr('green', \"Distribute train, gpus:{}, total batch size:{}, epoch:{}\".format(args.gpus, args.batch_size, args.epochs)))\n",
    "    logger.info(colorstr('green', \"Distribute train, total batch size:{}, epoch:{}\".format(args.batch_size, args.epochs)))\n",
    "\n",
    "    # pull in datasets\n",
    "    train_dataset, test_dataset = make_wider_datasets()\n",
    "\n",
    "    # store class id mappings\n",
    "    class_idx = remap_classes()\n",
    "\n",
    "    num_classes = 16\n",
    "    \n",
    "    # Create dict to store best model states\n",
    "    teacher_model_states_best = {}\n",
    "    student_model_states_best = {}\n",
    "\n",
    "    # Loop through the lambda_factor_list for teacher debiasing\n",
    "    for lambda_factor in args.teacher_lambda_factor_list:\n",
    "        \n",
    "        # Load EfficientNet B3 model for Teacher\n",
    "        teacher = models.efficientnet_b3(pretrained=True)\n",
    "        \n",
    "        # Determine the number of output features from the feature extractor part of EfficientNet B3\n",
    "        num_ftrs = teacher.classifier[1].in_features\n",
    "        \n",
    "        # Modify the classifier layer of the EfficientNet model to match the number of classes\n",
    "        teacher.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "        # Redefine the main model optimizer if needed\n",
    "        teacher_optimizer = optim.Adam(teacher.parameters(), lr=teacher_learning_rate)\n",
    "        teacher_loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Train the teacher\n",
    "        best_model_state = train_teacher(teacher, teacher_optimizer, teacher_loss_fn,\n",
    "                                       teacher_patience, teacher_epochs, device, base_save_dir=base_save_dir)\n",
    "        \n",
    "        teacher_model_states_best[lambda_factor] = best_model_state\n",
    "\n",
    "    # Save the collective best model states to a file\n",
    "    teacher_collective_save_path = os.path.join(base_save_dir, 'teacher_model_states_best.pth')\n",
    "    torch.save(teacher_model_states_best, teacher_collective_save_path)\n",
    "\n",
    "    # Specify the lambda_factor for the teacher model to load\n",
    "    lambda_factor = 0\n",
    "    # Define the path to the saved model file for this lambda_factor\n",
    "    lambda_dir = os.path.join(base_save_dir, f'TEACHER_lambda_{lambda_factor}')\n",
    "    teacher_path = os.path.join(lambda_dir, f'TEACHER_best_model_lambda_{lambda_factor}.pth')\n",
    "    \n",
    "    # Load the model state\n",
    "    teacher_best_model_state = torch.load(teacher_path)\n",
    "    teacher.load_state_dict(teacher_best_model_state['teacher_state_dict'])\n",
    "\n",
    "    \n",
    "    # Loop through the lambda_factor_list for student debiasing\n",
    "    for lambda_factor in student_lambda_factor_list:\n",
    "            # Load EfficientNet B0 model for Student\n",
    "        student = models.efficientnet_b0(pretrained=True)\n",
    "        \n",
    "        # Determine the number of output features from the feature extractor part of EfficientNet B0\n",
    "        num_ftrs = student.classifier[1].in_features  # This is the correct number of input features for the adversarial classifier\n",
    "        \n",
    "        # Modify the classifier layer of the EfficientNet model to match the number of classes\n",
    "        student.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "        # Initialize the Critic model\n",
    "        critic = critic.Critic(input_size=num_classes) # Adjust the input size based on the model's output\n",
    "        critic_optimizer = optim.Adam(critic.parameters(), lr=student_learning_rate, weight_decay=weight_decay)\n",
    "        critic_scheduler = lr_scheduler.ReduceLROnPlateau(critic_optimizer, mode='min', factor=0.2, patience=5, min_lr=0.0001)\n",
    "        critic_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "        # Redefine the main model optimizer if needed\n",
    "        student_optimizer = optim.Adam(student.parameters(), lr=student_learning_rate, weight_decay=weight_decay)\n",
    "        student_scheduler = lr_scheduler.ReduceLROnPlateau(student_optimizer, mode='min', factor=0.2, patience=5, min_lr=0.00001)\n",
    "        student_loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "        # Train the model\n",
    "        student_best_model_state = train_student(student, teacher, student_optimizer, student_loss_fn, critic, critic_optimizer, critic_loss_fn,\n",
    "                                       lambda_factor, temperature, alpha, epsilon, margin, student_patience, student_epochs, device, base_save_dir=base_save_dir, \n",
    "                                        student_scheduler=student_scheduler, critic_scheduler=critic_scheduler)\n",
    "        student_model_states_best[lambda_factor] = student_best_model_state\n",
    "    \n",
    "\n",
    "    # Save the collective best model states to a file\n",
    "    student_collective_save_path = os.path.join(base_save_dir, 'student_model_states_best.pth')\n",
    "    torch.save(student_model_states_best, student_collective_save_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if args.student_name in ['wrn40_1_cifar', 'mobilenetv2', 'shufflev1_cifar', 'shufflev2_cifar']:\n",
    "    #     student = EmbTrans(student=student, student_name=args.student_name)\n",
    "\n",
    "\n",
    "    # if args.teacher_weights:\n",
    "    #     print('Load Teacher Weights')\n",
    "    #     teacher_ckpt = torch.load(args.teacher_weights)['model']\n",
    "    #     teacher.load_state_dict(teacher_ckpt)\n",
    "\n",
    "        # for param in teacher.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "    # res56    ./ckpt/teacher/resnet56/center_emb_train.json\n",
    "    # res32x4  ./ckpt/teacher/resnet32x4/center_emb_train.json\n",
    "    # wrn40_2  ./ckpt/teacher/wrn_40_2/center_emb_train.json\n",
    "    # res50    ./ckpt/teacher/resnet50/center_emb_train.json\n",
    "    # class-mean\n",
    "\n",
    "    ####\n",
    "    # with open(\"./ckpt/teacher/resnet56/center_emb_train.json\", 'r') as f:\n",
    "    #     T_EMB = json.load(f)\n",
    "    # f.close()\n",
    "    ####\n",
    "\n",
    "    logger.info(colorstr('green', 'Use ' + args.teacher + ' Training ' + args.student_name + ' ...'))\n",
    "    # Train the student\n",
    "    student, teacher, test_loader = epoch_loop(student=student, teacher=teacher, train_set=train_set, test_set=test_set, args=args)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb908f8-90f8-4690-85a0-eba210e36111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import json\n",
    "import warnings\n",
    "import torchvision\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import EfficientNet\n",
    "from utils.loss_functions import tkd_kdloss\n",
    "from utils.loss_functions import tkd_kdloss, DD_loss, AD_loss, RKDDistanceLoss, RKDAngleLoss\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66ce7e4c-6acd-4ab7-b847-c1845b0f53d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 20:41:44 [line:423] \u001b[32mDistribute train, total batch size:128, epoch:1\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 20:41:46 [line:451] \u001b[32mUse resnet56_cifar Training resnet20_cifar ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Teacher Weights\n",
      "Epoch 1/1\n",
      "2023-12-11 20:42:10 [391/391] - 83.88ms/step - nd_loss: 0.270 - kd_loss: 0.000 - cls_loss: 3.529 - train_loss: 3.799 - train_acc: 0.162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 20:42:11 [line:261] \u001b[32mTrain Loss: 4.213, Train Acc: 0.103, Test Loss: 3.676, Test Acc: 0.155, lr: 0.10000\u001b[0m\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "11 Dec 2023 20:42:14 [line:352] \u001b[32m- - - - - METRICS REPORT - - - - -\u001b[0m\n",
      "11 Dec 2023 20:42:14 [line:353] \u001b[32mTEACHER: accuracy: 72.410, precision: 72.663, recall: 72.410, f1: 72.424, teacher_inf: 0.007, teacher_size: 861620.000\u001b[0m\n",
      "11 Dec 2023 20:42:14 [line:354] \u001b[32mSTUDENT: accuracy: 15.460, precision: 18.674, recall: 15.460, f1: 11.380, student_inf: 0.003, student_size: 278324.000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import Models\n",
    "from Models.embtrans_cifar import EmbTrans\n",
    "from Dataset import CIFAR\n",
    "from norm_dir_utils import colorstr, Save_Checkpoint, AverageMeter, DirectNormLoss, KDLoss\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pdb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from data_tools.wider_dataloader import remap_classes, custom_collate, make_wider_datasets\n",
    "\n",
    "\n",
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    inputs, _ = next(iter(dataloader))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _, teacher_outputs = teacher(inputs)\n",
    "    teacher_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _, student_outputs = student(inputs)\n",
    "    student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n",
    "\n",
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        with torch.no_grad():\n",
    "            _, teacher_outputs = teacher(inputs.to(device))\n",
    "            _, student_outputs = student(inputs.to(device))\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(torch.argmax(teacher_outputs, dim=1).cpu().numpy())\n",
    "        all_student_preds.append(torch.argmax(student_outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),  # Updated line\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def train(student, teacher, T_EMB, train_dataloader, optimizer, criterion, kd_loss, nd_loss, args, epoch):\n",
    "    train_loss = AverageMeter()\n",
    "    train_error = AverageMeter()\n",
    "\n",
    "    Cls_loss = AverageMeter()\n",
    "    Div_loss = AverageMeter()\n",
    "    Norm_Dir_loss = AverageMeter()\n",
    "\n",
    "    # Model on train mode\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    step_per_epoch = len(train_dataloader)\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_dataloader):\n",
    "        start = time.time()\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # compute output\n",
    "        s_emb, s_logits = student(images, embed=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_emb, t_logits = teacher(images, embed=True)\n",
    "\n",
    "        # cls loss\n",
    "        cls_loss = criterion(s_logits, labels) * args.cls_loss_factor\n",
    "        # KD loss\n",
    "        div_loss = kd_loss(s_logits, t_logits) * min(1.0, epoch/args.warm_up)\n",
    "        # ND loss\n",
    "        norm_dir_loss = nd_loss(s_emb=s_emb, t_emb=t_emb, T_EMB=T_EMB, labels=labels)\n",
    "\n",
    "        loss = cls_loss + div_loss + norm_dir_loss\n",
    "        # measure accuracy and record loss\n",
    "        batch_size = images.size(0)\n",
    "        _, pred = s_logits.data.cpu().topk(1, dim=1)\n",
    "        train_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "        train_loss.update(loss.item(), batch_size)\n",
    "\n",
    "        Cls_loss.update(cls_loss.item(), batch_size)\n",
    "        Div_loss.update(div_loss.item(), batch_size)\n",
    "        Norm_Dir_loss.update(norm_dir_loss.item(), batch_size)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "        s1 = '\\r{} [{}/{}]'.format(t, step+1, step_per_epoch)\n",
    "        s2 = ' - {:.2f}ms/step - nd_loss: {:.3f} - kd_loss: {:.3f} - cls_loss: {:.3f} - train_loss: {:.3f} - train_acc: {:.3f}'.format(\n",
    "             1000 * (time.time() - start), norm_dir_loss.item(), div_loss.item(), cls_loss.item(), train_loss.val, 1-train_error.val)\n",
    "\n",
    "        print(s1+s2, end='', flush=True)\n",
    "\n",
    "    print()\n",
    "    return Norm_Dir_loss.avg, Div_loss.avg, Cls_loss.avg, train_loss.avg, train_error.avg\n",
    "\n",
    "\n",
    "def test(student, test_dataloader, criterion):\n",
    "    test_loss = AverageMeter()\n",
    "    test_error = AverageMeter()\n",
    "\n",
    "    # Model on eval mode\n",
    "    student.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # compute logits\n",
    "            logits = student(images, embed=False)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            batch_size = images.size(0)\n",
    "            _, pred = logits.data.cpu().topk(1, dim=1)\n",
    "            test_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "            test_loss.update(loss.item(), batch_size)\n",
    "    \n",
    "\n",
    "    return test_loss.avg, test_error.avg\n",
    "\n",
    "\n",
    "def epoch_loop(student, teacher, train_set, test_set, args):\n",
    "    # data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=args.workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=args.workers)\n",
    "\n",
    "    # student\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # student = nn.DataParallel(student, device_ids=args.gpus)\n",
    "    student = nn.DataParallel(student)\n",
    "    student.to(device)\n",
    "    # teacher = nn.DataParallel(teacher, device_ids=args.gpus)\n",
    "    teacher = nn.DataParallel(teacher)\n",
    "    teacher.to(device)\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    kd_loss = KDLoss(kl_loss_factor=args.kd_loss_factor, T=args.t).to(device)\n",
    "    nd_loss = DirectNormLoss(num_class=100, nd_loss_factor=args.nd_loss_factor).to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.SGD(params=student.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)\n",
    "\n",
    "    # weights\n",
    "    save_dir = Path(args.save_dir)\n",
    "    weights = save_dir / 'weights'\n",
    "    weights.mkdir(parents=True, exist_ok=True)\n",
    "    last = weights / 'last'\n",
    "    best = weights / 'best'\n",
    "\n",
    "    # acc,loss\n",
    "    acc_loss = save_dir / 'acc_loss'\n",
    "    acc_loss.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_acc_savepath = acc_loss / 'train_acc.npy'\n",
    "    train_loss_savepath = acc_loss / 'train_loss.npy'\n",
    "    val_acc_savepath = acc_loss / 'val_acc.npy'\n",
    "    val_loss_savepath = acc_loss / 'val_loss.npy'\n",
    "\n",
    "    # tensorboard\n",
    "    logdir = save_dir / 'logs'\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "    summary_writer = SummaryWriter(logdir, flush_secs=120)\n",
    "\n",
    "    # resume\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        student.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_error = checkpoint['best_error']\n",
    "        train_acc = checkpoint['train_acc']\n",
    "        train_loss = checkpoint['train_loss']\n",
    "        test_acc = checkpoint['test_acc']\n",
    "        test_loss = checkpoint['test_loss']\n",
    "        logger.info(colorstr('green', 'Resuming training from {} epoch'.format(start_epoch)))\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_error = 0\n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        test_acc = []\n",
    "        test_loss = []\n",
    "        test_precision = []\n",
    "        test_recall = []\n",
    "        test_f1 = []\n",
    "\n",
    "    # Train student\n",
    "    best_error = 1\n",
    "    ##\n",
    "    patience = args.patience\n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = float('inf')\n",
    "    epoch_val_losses = []\n",
    "    epoch_val_accuracies = []\n",
    "    ##\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        if epoch in [150, 180, 210]:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, args.epochs))\n",
    "        norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error = train(student=student,\n",
    "                                                                                 teacher=teacher,\n",
    "                                                                                 T_EMB=T_EMB,\n",
    "                                                                                 train_dataloader=train_loader,\n",
    "                                                                                 optimizer=optimizer,\n",
    "                                                                                 criterion=criterion,\n",
    "                                                                                 kd_loss=kd_loss,\n",
    "                                                                                 nd_loss=nd_loss,\n",
    "                                                                                 args=args,\n",
    "                                                                                 epoch=epoch)\n",
    "        test_epoch_loss, test_error = test(student=student,\n",
    "                                        test_dataloader=test_loader,\n",
    "                                        criterion=criterion)\n",
    "\n",
    "        s = \"Train Loss: {:.3f}, Train Acc: {:.3f}, Test Loss: {:.3f}, Test Acc: {:.3f}, lr: {:.5f}\".format(\n",
    "            train_epoch_loss, 1-train_error, test_epoch_loss, 1-test_error, optimizer.param_groups[0]['lr'])\n",
    "        logger.info(colorstr('green', s))\n",
    "\n",
    "        # save acc,loss\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        train_acc.append(1-train_error)\n",
    "        test_loss.append(test_epoch_loss)\n",
    "        test_acc.append(1-test_error)\n",
    "\n",
    "        epoch_val_accuracies.append(1-test_error)\n",
    "        epoch_val_losses.append(test_epoch_loss)\n",
    "\n",
    "        # save student\n",
    "        is_best = test_error < best_error\n",
    "        best_error = min(best_error, test_error)\n",
    "        state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': student.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_error': best_error,\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'test_acc': test_acc,\n",
    "                'test_loss': test_loss,\n",
    "            }\n",
    "\n",
    "        last_path = last / 'epoch_{}_loss_{:.3f}_acc_{:.3f}'.format(\n",
    "            epoch + 1, test_epoch_loss, 1-test_error)\n",
    "        best_path = best / 'epoch_{}_acc_{:.3f}'.format(\n",
    "                epoch + 1, 1-best_error)\n",
    "\n",
    "        Save_Checkpoint(state, last, last_path, best, best_path, is_best)\n",
    "\n",
    "        # tensorboard\n",
    "        if epoch == 1:\n",
    "            images, labels = next(iter(train_loader))\n",
    "            img_grid = torchvision.utils.make_grid(images)\n",
    "            summary_writer.add_image('Cifar Image', img_grid)\n",
    "        summary_writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "        summary_writer.add_scalar('train_loss', train_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('train_error', train_error, epoch)\n",
    "        summary_writer.add_scalar('val_loss', test_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('val_error', test_error, epoch)\n",
    "\n",
    "        summary_writer.add_scalar('nd_loss', norm_dir_loss, epoch)\n",
    "        summary_writer.add_scalar('kd_loss', div_loss, epoch)\n",
    "        summary_writer.add_scalar('cls_loss', cls_loss, epoch)\n",
    "\n",
    "        ### \n",
    "        # Check if current validation combined loss is lower than the best combined loss\n",
    "        if test_epoch_loss < best_val_loss:\n",
    "            best_val_loss = test_epoch_loss\n",
    "            best_val_accuracy = 1-test_error\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            # print results\n",
    "            best_checkpoint = torch.load(os.path.join(best_path, 'ckpt.pth'))['model_state_dict']\n",
    "            student.load_state_dict(best_checkpoint)\n",
    "            metrics = compare_performance_metrics(teacher, student, test_loader)\n",
    "            teacher_time, student_time = compare_inference_time(teacher, student, test_loader)\n",
    "            teacher_size, student_size = compare_model_size(teacher, student)\n",
    "\n",
    "            final_report_banner = '- - - - - METRICS REPORT - - - - -'\n",
    "            teacher_metrics = \"TEACHER: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, teacher_inf: {:.3f}, teacher_size: {:.3f}\".format(\n",
    "                metrics['accuracy'][0], metrics['precision'][0], metrics['recall'][0], metrics['f1'][0], \n",
    "                teacher_time, teacher_size,)\n",
    "            student_metrics = \"STUDENT: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, student_inf: {:.3f}, student_size: {:.3f}\".format(\n",
    "                metrics['accuracy'][1], metrics['precision'][1], metrics['recall'][1], metrics['f1'][1], \n",
    "                 student_time, student_size)\n",
    "            logger.info(colorstr('green', final_report_banner))\n",
    "            logger.info(colorstr('green', teacher_metrics))\n",
    "            logger.info(colorstr('green', student_metrics))\n",
    "            break\n",
    "\n",
    "        if epoch == (args.epochs - 1):\n",
    "            best_checkpoint = torch.load(os.path.join(best_path, 'ckpt.pth'))['model_state_dict']\n",
    "            student.load_state_dict(best_checkpoint)\n",
    "            metrics = compare_performance_metrics(teacher, student, test_loader)\n",
    "            teacher_time, student_time = compare_inference_time(teacher, student, test_loader)\n",
    "            teacher_size, student_size = compare_model_size(teacher, student)\n",
    "\n",
    "            final_report_banner = '- - - - - METRICS REPORT - - - - -'\n",
    "            teacher_metrics = \"TEACHER: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, teacher_inf: {:.3f}, teacher_size: {:.3f}\".format(\n",
    "                100*metrics['accuracy'][0], 100*metrics['precision'][0], 100*metrics['recall'][0], 100*metrics['f1'][0], \n",
    "                teacher_time, teacher_size,)\n",
    "            student_metrics = \"STUDENT: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, student_inf: {:.3f}, student_size: {:.3f}\".format(\n",
    "                100*metrics['accuracy'][1], 100*metrics['precision'][1], 100*metrics['recall'][1], 100*metrics['f1'][1], \n",
    "                 student_time, student_size)\n",
    "            logger.info(colorstr('green', final_report_banner))\n",
    "            logger.info(colorstr('green', teacher_metrics))\n",
    "            logger.info(colorstr('green', student_metrics))\n",
    "            \n",
    "        \n",
    "        ###\n",
    "\n",
    "    summary_writer.close()\n",
    "    if not os.path.exists(train_acc_savepath) or not os.path.exists(train_loss_savepath):\n",
    "        np.save(train_acc_savepath, train_acc)\n",
    "        np.save(train_loss_savepath, train_loss)\n",
    "        np.save(val_acc_savepath, test_acc)\n",
    "        np.save(val_loss_savepath, test_loss)\n",
    "    \n",
    "    return student, teacher, test_loader\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    student_names = sorted(name for name in Models.__dict__\n",
    "                         if name.islower() and not name.startswith(\"__\")\n",
    "                         and callable(Models.__dict__[name]))\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Cifar Training')\n",
    "    parser.add_argument('-f') # add to make this run in collab\n",
    "    parser.add_argument(\"--student_name\", type=str, default=\"resnet20_cifar\", choices=student_names, help=\"student architecture\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default='cifar100')\n",
    "    parser.add_argument(\"--epochs\", type=int, default=240)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size per gpu\")\n",
    "    parser.add_argument('--workers', default=32, type=int, help='number of data loading workers')\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.1)\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum')\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=5e-4)\n",
    "\n",
    "    parser.add_argument(\"--teacher\", type=str, default=\"resnet56_cifar\", help=\"teacher architecture\")\n",
    "    parser.add_argument(\"--teacher_weights\", type=str, default=\"./ckpt/cifar_teachers/resnet56_vanilla/ckpt_epoch_240.pth\", help=\"teacher weights path\")\n",
    "    parser.add_argument(\"--cls_loss_factor\", type=float, default=1.0, help=\"cls loss weight factor\")\n",
    "    parser.add_argument(\"--kd_loss_factor\", type=float, default=1.0, help=\"KD loss weight factor\")\n",
    "    parser.add_argument(\"--t\", type=float, default=4.0, help=\"temperature\")\n",
    "    parser.add_argument(\"--nd_loss_factor\", type=float, default=1.0, help=\"ND loss weight factor\")\n",
    "    parser.add_argument(\"--warm_up\", type=float, default=20.0, help='loss weight warm up epochs')\n",
    "    parser.add_argument(\"--patience\", type=int, default=5, help='loss weight warm up epochs')\n",
    "\n",
    "\n",
    "    # parser.add_argument(\"--gpus\", type=list, default=[0, 1])\n",
    "    \n",
    "    parser.add_argument('--seed', default=None, type=int, help='seed for initializing training.')\n",
    "    parser.add_argument(\"--resume\", type=str, help=\"best ckpt's path to resume most recent training\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"./run/KD++\", help=\"save path, eg, acc_loss, weights, tensorboard, and so on\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        cudnn.benchmark = False\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s [line:%(lineno)d] %(message)s', \n",
    "                        datefmt='%d %b %Y %H:%M:%S')\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # args.batch_size = args.batch_size * len(args.gpus)\n",
    "    args.batch_size = args.batch_size * 1\n",
    "\n",
    "    # logger.info(colorstr('green', \"Distribute train, gpus:{}, total batch size:{}, epoch:{}\".format(args.gpus, args.batch_size, args.epochs)))\n",
    "    logger.info(colorstr('green', \"Distribute train, total batch size:{}, epoch:{}\".format(args.batch_size, args.epochs)))\n",
    "\n",
    "\n",
    "    train_set, test_set, num_class = CIFAR(name=args.dataset)\n",
    "    student = Models.__dict__[args.student_name](num_class=num_class)\n",
    "\n",
    "    if args.student_name in ['wrn40_1_cifar', 'mobilenetv2', 'shufflev1_cifar', 'shufflev2_cifar']:\n",
    "        student = EmbTrans(student=student, student_name=args.student_name)\n",
    "\n",
    "    teacher = Models.__dict__[args.teacher](num_class=num_class)\n",
    "\n",
    "    if args.teacher_weights:\n",
    "        print('Load Teacher Weights')\n",
    "        teacher_ckpt = torch.load(args.teacher_weights)['model']\n",
    "        teacher.load_state_dict(teacher_ckpt)\n",
    "\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # res56    ./ckpt/teacher/resnet56/center_emb_train.json\n",
    "    # res32x4  ./ckpt/teacher/resnet32x4/center_emb_train.json\n",
    "    # wrn40_2  ./ckpt/teacher/wrn_40_2/center_emb_train.json\n",
    "    # res50    ./ckpt/teacher/resnet50/center_emb_train.json\n",
    "    # class-mean\n",
    "    with open(\"./ckpt/teacher/resnet56/center_emb_train.json\", 'r') as f:\n",
    "        T_EMB = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    logger.info(colorstr('green', 'Use ' + args.teacher + ' Training ' + args.student_name + ' ...'))\n",
    "    # Train the student\n",
    "    student, teacher, test_loader = epoch_loop(student=student, teacher=teacher, train_set=train_set, test_set=test_set, args=args)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22878a66-ae33-4f25-8fdd-c006794878dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
