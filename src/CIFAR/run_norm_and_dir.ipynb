{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ec628d-6f2f-47ff-ad7e-b66a67b2f83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaraies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66ce7e4c-6acd-4ab7-b847-c1845b0f53d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 18:59:52 [line:423] \u001b[32mDistribute train, total batch size:128, epoch:240\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 18:59:53 [line:451] \u001b[32mUse resnet56_cifar Training resnet20_cifar ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Teacher Weights\n",
      "Epoch 1/240\n",
      "2023-12-11 19:00:17 [391/391] - 39.60ms/step - nd_loss: 0.258 - kd_loss: 0.000 - cls_loss: 3.465 - train_loss: 3.723 - train_acc: 0.162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:00:19 [line:261] \u001b[32mTrain Loss: 4.268, Train Acc: 0.097, Test Loss: 3.711, Test Acc: 0.151, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/240\n",
      "2023-12-11 19:00:43 [391/391] - 40.13ms/step - nd_loss: 0.251 - kd_loss: 0.501 - cls_loss: 2.950 - train_loss: 3.702 - train_acc: 0.262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:00:45 [line:261] \u001b[32mTrain Loss: 4.080, Train Acc: 0.205, Test Loss: 3.235, Test Acc: 0.231, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/240\n",
      "2023-12-11 19:01:11 [391/391] - 39.46ms/step - nd_loss: 0.249 - kd_loss: 0.914 - cls_loss: 2.790 - train_loss: 3.952 - train_acc: 0.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:01:12 [line:261] \u001b[32mTrain Loss: 3.959, Train Acc: 0.297, Test Loss: 3.086, Test Acc: 0.273, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/240\n",
      "2023-12-11 19:01:37 [391/391] - 39.51ms/step - nd_loss: 0.252 - kd_loss: 1.140 - cls_loss: 2.502 - train_loss: 3.893 - train_acc: 0.363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:01:38 [line:261] \u001b[32mTrain Loss: 3.965, Train Acc: 0.358, Test Loss: 3.207, Test Acc: 0.322, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/240\n",
      "2023-12-11 19:02:02 [391/391] - 39.60ms/step - nd_loss: 0.235 - kd_loss: 1.488 - cls_loss: 2.463 - train_loss: 4.186 - train_acc: 0.287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:02:04 [line:261] \u001b[32mTrain Loss: 4.033, Train Acc: 0.401, Test Loss: 3.382, Test Acc: 0.312, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/240\n",
      "2023-12-11 19:02:29 [391/391] - 39.91ms/step - nd_loss: 0.245 - kd_loss: 1.851 - cls_loss: 2.335 - train_loss: 4.430 - train_acc: 0.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:02:30 [line:261] \u001b[32mTrain Loss: 4.102, Train Acc: 0.440, Test Loss: 3.206, Test Acc: 0.335, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/240\n",
      "2023-12-11 19:02:55 [391/391] - 40.17ms/step - nd_loss: 0.263 - kd_loss: 2.213 - cls_loss: 2.311 - train_loss: 4.787 - train_acc: 0.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:02:56 [line:261] \u001b[32mTrain Loss: 4.177, Train Acc: 0.464, Test Loss: 2.893, Test Acc: 0.348, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/240\n",
      "2023-12-11 19:03:20 [391/391] - 39.19ms/step - nd_loss: 0.246 - kd_loss: 1.943 - cls_loss: 1.893 - train_loss: 4.082 - train_acc: 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:03:22 [line:261] \u001b[32mTrain Loss: 4.315, Train Acc: 0.483, Test Loss: 2.957, Test Acc: 0.379, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/240\n",
      "2023-12-11 19:03:46 [391/391] - 38.91ms/step - nd_loss: 0.247 - kd_loss: 2.352 - cls_loss: 2.147 - train_loss: 4.746 - train_acc: 0.450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:03:47 [line:261] \u001b[32mTrain Loss: 4.440, Train Acc: 0.502, Test Loss: 2.504, Test Acc: 0.422, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/240\n",
      "2023-12-11 19:04:11 [391/391] - 39.33ms/step - nd_loss: 0.254 - kd_loss: 2.390 - cls_loss: 1.784 - train_loss: 4.427 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:04:13 [line:261] \u001b[32mTrain Loss: 4.583, Train Acc: 0.514, Test Loss: 2.492, Test Acc: 0.447, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/240\n",
      "2023-12-11 19:04:37 [391/391] - 39.05ms/step - nd_loss: 0.252 - kd_loss: 2.692 - cls_loss: 1.743 - train_loss: 4.688 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:04:38 [line:261] \u001b[32mTrain Loss: 4.728, Train Acc: 0.528, Test Loss: 2.594, Test Acc: 0.443, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/240\n",
      "2023-12-11 19:05:02 [391/391] - 39.25ms/step - nd_loss: 0.257 - kd_loss: 3.000 - cls_loss: 2.171 - train_loss: 5.428 - train_acc: 0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:05:04 [line:261] \u001b[32mTrain Loss: 4.891, Train Acc: 0.536, Test Loss: 2.314, Test Acc: 0.478, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/240\n",
      "2023-12-11 19:05:28 [391/391] - 39.04ms/step - nd_loss: 0.265 - kd_loss: 2.814 - cls_loss: 1.810 - train_loss: 4.889 - train_acc: 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:05:29 [line:261] \u001b[32mTrain Loss: 5.004, Train Acc: 0.548, Test Loss: 2.338, Test Acc: 0.470, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/240\n",
      "2023-12-11 19:05:53 [391/391] - 37.65ms/step - nd_loss: 0.274 - kd_loss: 3.073 - cls_loss: 1.597 - train_loss: 4.944 - train_acc: 0.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:05:54 [line:261] \u001b[32mTrain Loss: 5.171, Train Acc: 0.555, Test Loss: 2.431, Test Acc: 0.468, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/240\n",
      "2023-12-11 19:06:18 [391/391] - 38.87ms/step - nd_loss: 0.279 - kd_loss: 3.334 - cls_loss: 1.805 - train_loss: 5.419 - train_acc: 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:06:19 [line:261] \u001b[32mTrain Loss: 5.360, Train Acc: 0.557, Test Loss: 2.505, Test Acc: 0.478, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/240\n",
      "2023-12-11 19:06:43 [391/391] - 37.49ms/step - nd_loss: 0.278 - kd_loss: 3.451 - cls_loss: 1.459 - train_loss: 5.188 - train_acc: 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:06:44 [line:261] \u001b[32mTrain Loss: 5.510, Train Acc: 0.566, Test Loss: 2.314, Test Acc: 0.497, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/240\n",
      "2023-12-11 19:07:08 [391/391] - 38.03ms/step - nd_loss: 0.281 - kd_loss: 3.910 - cls_loss: 1.477 - train_loss: 5.668 - train_acc: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:07:09 [line:261] \u001b[32mTrain Loss: 5.685, Train Acc: 0.570, Test Loss: 2.247, Test Acc: 0.509, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/240\n",
      "2023-12-11 19:07:32 [391/391] - 37.37ms/step - nd_loss: 0.307 - kd_loss: 3.856 - cls_loss: 1.929 - train_loss: 6.092 - train_acc: 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:07:34 [line:261] \u001b[32mTrain Loss: 5.849, Train Acc: 0.576, Test Loss: 2.505, Test Acc: 0.466, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/240\n",
      "2023-12-11 19:07:57 [391/391] - 37.23ms/step - nd_loss: 0.310 - kd_loss: 3.893 - cls_loss: 1.592 - train_loss: 5.795 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:07:58 [line:261] \u001b[32mTrain Loss: 6.020, Train Acc: 0.582, Test Loss: 2.263, Test Acc: 0.490, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/240\n",
      "2023-12-11 19:08:21 [391/391] - 37.87ms/step - nd_loss: 0.320 - kd_loss: 4.322 - cls_loss: 1.574 - train_loss: 6.216 - train_acc: 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:08:23 [line:261] \u001b[32mTrain Loss: 6.184, Train Acc: 0.583, Test Loss: 2.422, Test Acc: 0.490, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/240\n",
      "2023-12-11 19:08:46 [391/391] - 37.16ms/step - nd_loss: 0.321 - kd_loss: 4.236 - cls_loss: 1.485 - train_loss: 6.042 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:08:47 [line:261] \u001b[32mTrain Loss: 6.357, Train Acc: 0.586, Test Loss: 2.104, Test Acc: 0.516, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/240\n",
      "2023-12-11 19:09:10 [391/391] - 37.58ms/step - nd_loss: 0.313 - kd_loss: 4.275 - cls_loss: 1.461 - train_loss: 6.049 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:09:12 [line:261] \u001b[32mTrain Loss: 6.283, Train Acc: 0.588, Test Loss: 2.149, Test Acc: 0.514, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/240\n",
      "2023-12-11 19:09:35 [391/391] - 37.20ms/step - nd_loss: 0.347 - kd_loss: 4.794 - cls_loss: 1.417 - train_loss: 6.558 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:09:36 [line:261] \u001b[32mTrain Loss: 6.227, Train Acc: 0.593, Test Loss: 2.613, Test Acc: 0.476, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/240\n",
      "2023-12-11 19:09:59 [391/391] - 37.59ms/step - nd_loss: 0.329 - kd_loss: 4.024 - cls_loss: 1.456 - train_loss: 5.809 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:10:01 [line:261] \u001b[32mTrain Loss: 6.163, Train Acc: 0.596, Test Loss: 2.417, Test Acc: 0.494, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/240\n",
      "2023-12-11 19:10:23 [391/391] - 37.52ms/step - nd_loss: 0.330 - kd_loss: 4.239 - cls_loss: 1.032 - train_loss: 5.601 - train_acc: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:10:25 [line:261] \u001b[32mTrain Loss: 6.122, Train Acc: 0.598, Test Loss: 1.898, Test Acc: 0.551, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/240\n",
      "2023-12-11 19:10:48 [391/391] - 37.62ms/step - nd_loss: 0.326 - kd_loss: 4.790 - cls_loss: 1.567 - train_loss: 6.683 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:10:49 [line:261] \u001b[32mTrain Loss: 6.106, Train Acc: 0.600, Test Loss: 2.401, Test Acc: 0.495, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/240\n",
      "2023-12-11 19:11:13 [391/391] - 38.02ms/step - nd_loss: 0.323 - kd_loss: 4.116 - cls_loss: 1.738 - train_loss: 6.177 - train_acc: 0.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:11:14 [line:261] \u001b[32mTrain Loss: 6.079, Train Acc: 0.604, Test Loss: 2.360, Test Acc: 0.487, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/240\n",
      "2023-12-11 19:11:37 [391/391] - 37.25ms/step - nd_loss: 0.339 - kd_loss: 4.834 - cls_loss: 1.828 - train_loss: 7.000 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:11:38 [line:261] \u001b[32mTrain Loss: 6.048, Train Acc: 0.600, Test Loss: 2.022, Test Acc: 0.532, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/240\n",
      "2023-12-11 19:12:02 [391/391] - 37.11ms/step - nd_loss: 0.329 - kd_loss: 4.448 - cls_loss: 1.519 - train_loss: 6.296 - train_acc: 0.6006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:12:03 [line:261] \u001b[32mTrain Loss: 6.026, Train Acc: 0.601, Test Loss: 2.286, Test Acc: 0.511, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/240\n",
      "2023-12-11 19:12:26 [391/391] - 36.22ms/step - nd_loss: 0.337 - kd_loss: 4.368 - cls_loss: 1.455 - train_loss: 6.160 - train_acc: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:12:27 [line:261] \u001b[32mTrain Loss: 6.003, Train Acc: 0.603, Test Loss: 2.010, Test Acc: 0.545, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 30\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'run/KD++/weights/best/epoch_30_acc_0.551/ckpt.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 453\u001b[0m\n\u001b[1;32m    451\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(colorstr(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m args\u001b[38;5;241m.\u001b[39mteacher \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Training \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m args\u001b[38;5;241m.\u001b[39mstudent_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# Train the student\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m student, teacher, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mepoch_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 320\u001b[0m, in \u001b[0;36mepoch_loop\u001b[0;34m(student, teacher, train_set, test_set, args)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEarly stopping triggered at epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# print results\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m best_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mckpt.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    321\u001b[0m student\u001b[38;5;241m.\u001b[39mload_state_dict(best_checkpoint)\n\u001b[1;32m    322\u001b[0m metrics \u001b[38;5;241m=\u001b[39m compare_performance_metrics(teacher, student, test_loader)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'run/KD++/weights/best/epoch_30_acc_0.551/ckpt.pth'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import Models\n",
    "from Models.embtrans_cifar import EmbTrans\n",
    "from Dataset import CIFAR\n",
    "from utils import colorstr, Save_Checkpoint, AverageMeter, DirectNormLoss, KDLoss\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pdb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    inputs, _ = next(iter(dataloader))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _, teacher_outputs = teacher(inputs)\n",
    "    teacher_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _, student_outputs = student(inputs)\n",
    "    student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n",
    "\n",
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        with torch.no_grad():\n",
    "            _, teacher_outputs = teacher(inputs.to(device))\n",
    "            _, student_outputs = student(inputs.to(device))\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(torch.argmax(teacher_outputs, dim=1).cpu().numpy())\n",
    "        all_student_preds.append(torch.argmax(student_outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),  # Updated line\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def train(student, teacher, T_EMB, train_dataloader, optimizer, criterion, kd_loss, nd_loss, args, epoch):\n",
    "    train_loss = AverageMeter()\n",
    "    train_error = AverageMeter()\n",
    "\n",
    "    Cls_loss = AverageMeter()\n",
    "    Div_loss = AverageMeter()\n",
    "    Norm_Dir_loss = AverageMeter()\n",
    "\n",
    "    # Model on train mode\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    step_per_epoch = len(train_dataloader)\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_dataloader):\n",
    "        start = time.time()\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # compute output\n",
    "        s_emb, s_logits = student(images, embed=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_emb, t_logits = teacher(images, embed=True)\n",
    "\n",
    "        # cls loss\n",
    "        cls_loss = criterion(s_logits, labels) * args.cls_loss_factor\n",
    "        # KD loss\n",
    "        div_loss = kd_loss(s_logits, t_logits) * min(1.0, epoch/args.warm_up)\n",
    "        # ND loss\n",
    "        norm_dir_loss = nd_loss(s_emb=s_emb, t_emb=t_emb, T_EMB=T_EMB, labels=labels)\n",
    "\n",
    "        loss = cls_loss + div_loss + norm_dir_loss\n",
    "        # measure accuracy and record loss\n",
    "        batch_size = images.size(0)\n",
    "        _, pred = s_logits.data.cpu().topk(1, dim=1)\n",
    "        train_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "        train_loss.update(loss.item(), batch_size)\n",
    "\n",
    "        Cls_loss.update(cls_loss.item(), batch_size)\n",
    "        Div_loss.update(div_loss.item(), batch_size)\n",
    "        Norm_Dir_loss.update(norm_dir_loss.item(), batch_size)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "        s1 = '\\r{} [{}/{}]'.format(t, step+1, step_per_epoch)\n",
    "        s2 = ' - {:.2f}ms/step - nd_loss: {:.3f} - kd_loss: {:.3f} - cls_loss: {:.3f} - train_loss: {:.3f} - train_acc: {:.3f}'.format(\n",
    "             1000 * (time.time() - start), norm_dir_loss.item(), div_loss.item(), cls_loss.item(), train_loss.val, 1-train_error.val)\n",
    "\n",
    "        print(s1+s2, end='', flush=True)\n",
    "\n",
    "    print()\n",
    "    return Norm_Dir_loss.avg, Div_loss.avg, Cls_loss.avg, train_loss.avg, train_error.avg\n",
    "\n",
    "\n",
    "def test(student, test_dataloader, criterion):\n",
    "    test_loss = AverageMeter()\n",
    "    test_error = AverageMeter()\n",
    "\n",
    "    # Model on eval mode\n",
    "    student.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # compute logits\n",
    "            logits = student(images, embed=False)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            batch_size = images.size(0)\n",
    "            _, pred = logits.data.cpu().topk(1, dim=1)\n",
    "            test_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "            test_loss.update(loss.item(), batch_size)\n",
    "    \n",
    "\n",
    "    return test_loss.avg, test_error.avg\n",
    "\n",
    "\n",
    "def epoch_loop(student, teacher, train_set, test_set, args):\n",
    "    # data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=args.workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=args.workers)\n",
    "\n",
    "    # student\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # student = nn.DataParallel(student, device_ids=args.gpus)\n",
    "    student = nn.DataParallel(student)\n",
    "    student.to(device)\n",
    "    # teacher = nn.DataParallel(teacher, device_ids=args.gpus)\n",
    "    teacher = nn.DataParallel(teacher)\n",
    "    teacher.to(device)\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    kd_loss = KDLoss(kl_loss_factor=args.kd_loss_factor, T=args.t).to(device)\n",
    "    nd_loss = DirectNormLoss(num_class=100, nd_loss_factor=args.nd_loss_factor).to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.SGD(params=student.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)\n",
    "\n",
    "    # weights\n",
    "    save_dir = Path(args.save_dir)\n",
    "    weights = save_dir / 'weights'\n",
    "    weights.mkdir(parents=True, exist_ok=True)\n",
    "    last = weights / 'last'\n",
    "    best = weights / 'best'\n",
    "\n",
    "    # acc,loss\n",
    "    acc_loss = save_dir / 'acc_loss'\n",
    "    acc_loss.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_acc_savepath = acc_loss / 'train_acc.npy'\n",
    "    train_loss_savepath = acc_loss / 'train_loss.npy'\n",
    "    val_acc_savepath = acc_loss / 'val_acc.npy'\n",
    "    val_loss_savepath = acc_loss / 'val_loss.npy'\n",
    "\n",
    "    # tensorboard\n",
    "    logdir = save_dir / 'logs'\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "    summary_writer = SummaryWriter(logdir, flush_secs=120)\n",
    "\n",
    "    # resume\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        student.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_error = checkpoint['best_error']\n",
    "        train_acc = checkpoint['train_acc']\n",
    "        train_loss = checkpoint['train_loss']\n",
    "        test_acc = checkpoint['test_acc']\n",
    "        test_loss = checkpoint['test_loss']\n",
    "        logger.info(colorstr('green', 'Resuming training from {} epoch'.format(start_epoch)))\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_error = 0\n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        test_acc = []\n",
    "        test_loss = []\n",
    "        test_precision = []\n",
    "        test_recall = []\n",
    "        test_f1 = []\n",
    "\n",
    "    # Train student\n",
    "    best_error = 1\n",
    "    ##\n",
    "    patience = args.patience\n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = float('inf')\n",
    "    epoch_val_losses = []\n",
    "    epoch_val_accuracies = []\n",
    "    ##\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        if epoch in [150, 180, 210]:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, args.epochs))\n",
    "        norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error = train(student=student,\n",
    "                                                                                 teacher=teacher,\n",
    "                                                                                 T_EMB=T_EMB,\n",
    "                                                                                 train_dataloader=train_loader,\n",
    "                                                                                 optimizer=optimizer,\n",
    "                                                                                 criterion=criterion,\n",
    "                                                                                 kd_loss=kd_loss,\n",
    "                                                                                 nd_loss=nd_loss,\n",
    "                                                                                 args=args,\n",
    "                                                                                 epoch=epoch)\n",
    "        test_epoch_loss, test_error = test(student=student,\n",
    "                                        test_dataloader=test_loader,\n",
    "                                        criterion=criterion)\n",
    "\n",
    "        s = \"Train Loss: {:.3f}, Train Acc: {:.3f}, Test Loss: {:.3f}, Test Acc: {:.3f}, lr: {:.5f}\".format(\n",
    "            train_epoch_loss, 1-train_error, test_epoch_loss, 1-test_error, optimizer.param_groups[0]['lr'])\n",
    "        logger.info(colorstr('green', s))\n",
    "\n",
    "        # save acc,loss\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        train_acc.append(1-train_error)\n",
    "        test_loss.append(test_epoch_loss)\n",
    "        test_acc.append(1-test_error)\n",
    "\n",
    "        epoch_val_accuracies.append(1-test_error)\n",
    "        epoch_val_losses.append(test_epoch_loss)\n",
    "\n",
    "        # save student\n",
    "        is_best = test_error < best_error\n",
    "        best_error = min(best_error, test_error)\n",
    "        state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': student.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_error': best_error,\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'test_acc': test_acc,\n",
    "                'test_loss': test_loss,\n",
    "            }\n",
    "\n",
    "        last_path = last / 'epoch_{}_loss_{:.3f}_acc_{:.3f}'.format(\n",
    "            epoch + 1, test_epoch_loss, 1-test_error)\n",
    "        best_path = best / 'epoch_{}_acc_{:.3f}'.format(\n",
    "                epoch + 1, 1-best_error)\n",
    "\n",
    "        Save_Checkpoint(state, last, last_path, best, best_path, is_best)\n",
    "\n",
    "        # tensorboard\n",
    "        if epoch == 1:\n",
    "            images, labels = next(iter(train_loader))\n",
    "            img_grid = torchvision.utils.make_grid(images)\n",
    "            summary_writer.add_image('Cifar Image', img_grid)\n",
    "        summary_writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "        summary_writer.add_scalar('train_loss', train_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('train_error', train_error, epoch)\n",
    "        summary_writer.add_scalar('val_loss', test_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('val_error', test_error, epoch)\n",
    "\n",
    "        summary_writer.add_scalar('nd_loss', norm_dir_loss, epoch)\n",
    "        summary_writer.add_scalar('kd_loss', div_loss, epoch)\n",
    "        summary_writer.add_scalar('cls_loss', cls_loss, epoch)\n",
    "\n",
    "        ### \n",
    "        # Check if current validation combined loss is lower than the best combined loss\n",
    "        if test_epoch_loss < best_val_loss:\n",
    "            best_val_loss = test_epoch_loss\n",
    "            best_val_accuracy = 1-test_error\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            # print results\n",
    "            best_checkpoint = torch.load(os.path.join(best_path, 'ckpt.pth'))['model_state_dict']\n",
    "            student.load_state_dict(best_checkpoint)\n",
    "            metrics = compare_performance_metrics(teacher, student, test_loader)\n",
    "            teacher_time, student_time = compare_inference_time(teacher, student, test_loader)\n",
    "            teacher_size, student_size = compare_model_size(teacher, student)\n",
    "\n",
    "            final_report_banner = '- - - - - METRICS REPORT - - - - -'\n",
    "            teacher_metrics = \"TEACHER: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, teacher_inf: {:.3f}, teacher_size: {:.3f}\".format(\n",
    "                metrics['accuracy'][0], metrics['precision'][0], metrics['recall'][0], metrics['f1'][0], \n",
    "                teacher_time, teacher_size,)\n",
    "            student_metrics = \"STUDENT: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, student_inf: {:.3f}, student_size: {:.3f}\".format(\n",
    "                metrics['accuracy'][1], metrics['precision'][1], metrics['recall'][1], metrics['f1'][1], \n",
    "                 student_time, student_size)\n",
    "            logger.info(colorstr('green', final_report_banner))\n",
    "            logger.info(colorstr('green', teacher_metrics))\n",
    "            logger.info(colorstr('green', student_metrics))\n",
    "            break\n",
    "\n",
    "        if epoch == (args.epochs - 1):\n",
    "            best_checkpoint = torch.load(os.path.join(best_path, 'ckpt.pth'))['model_state_dict']\n",
    "            student.load_state_dict(best_checkpoint)\n",
    "            metrics = compare_performance_metrics(teacher, student, test_loader)\n",
    "            teacher_time, student_time = compare_inference_time(teacher, student, test_loader)\n",
    "            teacher_size, student_size = compare_model_size(teacher, student)\n",
    "\n",
    "            final_report_banner = '- - - - - METRICS REPORT - - - - -'\n",
    "            teacher_metrics = \"TEACHER: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, teacher_inf: {:.3f}, teacher_size: {:.3f}\".format(\n",
    "                100*metrics['accuracy'][0], 100*metrics['precision'][0], 100*metrics['recall'][0], 100*metrics['f1'][0], \n",
    "                teacher_time, teacher_size,)\n",
    "            student_metrics = \"STUDENT: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, student_inf: {:.3f}, student_size: {:.3f}\".format(\n",
    "                100*metrics['accuracy'][1], 100*metrics['precision'][1], 100*metrics['recall'][1], 100*metrics['f1'][1], \n",
    "                 student_time, student_size)\n",
    "            logger.info(colorstr('green', final_report_banner))\n",
    "            logger.info(colorstr('green', teacher_metrics))\n",
    "            logger.info(colorstr('green', student_metrics))\n",
    "            \n",
    "        \n",
    "        ###\n",
    "\n",
    "    summary_writer.close()\n",
    "    if not os.path.exists(train_acc_savepath) or not os.path.exists(train_loss_savepath):\n",
    "        np.save(train_acc_savepath, train_acc)\n",
    "        np.save(train_loss_savepath, train_loss)\n",
    "        np.save(val_acc_savepath, test_acc)\n",
    "        np.save(val_loss_savepath, test_loss)\n",
    "    \n",
    "    return student, teacher, test_loader\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    student_names = sorted(name for name in Models.__dict__\n",
    "                         if name.islower() and not name.startswith(\"__\")\n",
    "                         and callable(Models.__dict__[name]))\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Cifar Training')\n",
    "    parser.add_argument('-f') # added to make this run in collab\n",
    "    parser.add_argument(\"--student_name\", type=str, default=\"resnet20_cifar\", choices=student_names, help=\"student architecture\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default='cifar100')\n",
    "    parser.add_argument(\"--epochs\", type=int, default=240)\n",
    "    # parser.add_argument(\"--epochs\", type=int, default=4)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size per gpu\")\n",
    "    parser.add_argument('--workers', default=32, type=int, help='number of data loading workers')\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.1)\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum')\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=5e-4)\n",
    "\n",
    "    parser.add_argument(\"--teacher\", type=str, default=\"resnet56_cifar\", help=\"teacher architecture\")\n",
    "    parser.add_argument(\"--teacher_weights\", type=str, default=\"./ckpt/cifar_teachers/resnet56_vanilla/ckpt_epoch_240.pth\", help=\"teacher weights path\")\n",
    "    parser.add_argument(\"--cls_loss_factor\", type=float, default=1.0, help=\"cls loss weight factor\")\n",
    "    parser.add_argument(\"--kd_loss_factor\", type=float, default=1.0, help=\"KD loss weight factor\")\n",
    "    parser.add_argument(\"--t\", type=float, default=4.0, help=\"temperature\")\n",
    "    parser.add_argument(\"--nd_loss_factor\", type=float, default=1.0, help=\"ND loss weight factor\")\n",
    "    parser.add_argument(\"--warm_up\", type=float, default=20.0, help='loss weight warm up epochs')\n",
    "    parser.add_argument(\"--patience\", type=int, default=5, help='loss weight warm up epochs')\n",
    "\n",
    "\n",
    "    # parser.add_argument(\"--gpus\", type=list, default=[0, 1])\n",
    "    \n",
    "    parser.add_argument('--seed', default=None, type=int, help='seed for initializing training.')\n",
    "    parser.add_argument(\"--resume\", type=str, help=\"best ckpt's path to resume most recent training\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"./run/KD++\", help=\"save path, eg, acc_loss, weights, tensorboard, and so on\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        cudnn.benchmark = False\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s [line:%(lineno)d] %(message)s', \n",
    "                        datefmt='%d %b %Y %H:%M:%S')\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # args.batch_size = args.batch_size * len(args.gpus)\n",
    "    args.batch_size = args.batch_size * 1\n",
    "\n",
    "    # logger.info(colorstr('green', \"Distribute train, gpus:{}, total batch size:{}, epoch:{}\".format(args.gpus, args.batch_size, args.epochs)))\n",
    "    logger.info(colorstr('green', \"Distribute train, total batch size:{}, epoch:{}\".format(args.batch_size, args.epochs)))\n",
    "\n",
    "\n",
    "    train_set, test_set, num_class = CIFAR(name=args.dataset)\n",
    "    student = Models.__dict__[args.student_name](num_class=num_class)\n",
    "\n",
    "    if args.student_name in ['wrn40_1_cifar', 'mobilenetv2', 'shufflev1_cifar', 'shufflev2_cifar']:\n",
    "        student = EmbTrans(student=student, student_name=args.student_name)\n",
    "\n",
    "    teacher = Models.__dict__[args.teacher](num_class=num_class)\n",
    "\n",
    "    if args.teacher_weights:\n",
    "        print('Load Teacher Weights')\n",
    "        teacher_ckpt = torch.load(args.teacher_weights)['model']\n",
    "        teacher.load_state_dict(teacher_ckpt)\n",
    "\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # res56    ./ckpt/teacher/resnet56/center_emb_train.json\n",
    "    # res32x4  ./ckpt/teacher/resnet32x4/center_emb_train.json\n",
    "    # wrn40_2  ./ckpt/teacher/wrn_40_2/center_emb_train.json\n",
    "    # res50    ./ckpt/teacher/resnet50/center_emb_train.json\n",
    "    # class-mean\n",
    "    with open(\"./ckpt/teacher/resnet56/center_emb_train.json\", 'r') as f:\n",
    "        T_EMB = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    logger.info(colorstr('green', 'Use ' + args.teacher + ' Training ' + args.student_name + ' ...'))\n",
    "    # Train the student\n",
    "    student, teacher, test_loader = epoch_loop(student=student, teacher=teacher, train_set=train_set, test_set=test_set, args=args)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747c3741-e38e-4e7e-a093-02ef9d889ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
