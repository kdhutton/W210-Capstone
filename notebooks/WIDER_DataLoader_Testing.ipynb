{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e057459-072c-4cc9-9547-5fb77dfbf36b",
   "metadata": {},
   "source": [
    "## WIDER Dataset - DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9fd4796-6aec-42af-9e21-413c8841c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66acd606-5ac6-4a16-92c4-59d34d57861c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "import boto3\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms, RandAugment\n",
    "import torch\n",
    "import tarfile\n",
    "import os\n",
    "import getpass\n",
    "import s3fs\n",
    "import json\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a55be-ed9a-4a8a-a16e-8725a011a310",
   "metadata": {},
   "source": [
    "### Data Download - Do not need to repeat this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e679eef-7c26-4ac3-9b73-a8310d9d94bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "# # !aws s3 cp s3://210bucket/wider_attribute_image.tgz ./wider_attribute_image.tgz\n",
    "# Option 2\n",
    "# s3 = boto3.client('s3')\n",
    "# s3.download_file('210bucket', 'wider_attribute_image.tgz', 'wider_attribute_image.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b7f2e4c-61bc-4c4c-8aae-596d37c60768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the .tgz file\n",
    "# with tarfile.open('wider_attribute_image.tgz', 'r:gz') as tar:\n",
    "#     tar.extractall(path=\"local_wider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09d2d531-4065-4e23-b186-ab9248633d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upload the image files to s3 Bukcet\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# def upload_dir_to_s3(bucket_name, s3_folder, local_dir):\n",
    "#     for root, dirs, files in os.walk(local_dir):\n",
    "#         for file in files:\n",
    "#             local_path = os.path.join(root, file)\n",
    "#             relative_path = os.path.relpath(local_path, local_dir)\n",
    "#             s3_path = os.path.join(s3_folder, relative_path)\n",
    "            \n",
    "#             s3.upload_file(local_path, bucket_name, s3_path)\n",
    "\n",
    "# upload_dir_to_s3('210bucket', 'WIDER/', 'WIDER/Image/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3239f8ac-8c25-46b1-b7dd-f602145f1b91",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea1301a1-88a0-4aab-bcb8-83b34be3161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, ann_files, augs, img_size, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.ann_files = ann_files\n",
    "        self.augment = self.augs_function(augs, img_size)\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "            ]\n",
    "        )\n",
    "        self.anns = []\n",
    "        self.s3_client = boto3.client('s3') \n",
    "        self.load_anns()\n",
    "        print(self.augment)\n",
    "\n",
    "        if self.dataset == \"wider\":\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def extract_label_from_filename(self, file_name):\n",
    "        # Split the path and extract the part with '--'\n",
    "        parts = file_name.split('/')\n",
    "        for part in parts:\n",
    "            if '--' in part:\n",
    "                # Extract the numeric part before '--'\n",
    "                label = part.split('--')[0]\n",
    "                if label.isdigit():\n",
    "                    return int(label)  # Return as an integer\n",
    "        raise ValueError(f\"Label not found in file name: {file_name}\")\n",
    "\n",
    "        \n",
    "\n",
    "    def augs_function(self, augs, img_size):            \n",
    "        t = []\n",
    "        if 'randomflip' in augs:\n",
    "            t.append(transforms.RandomHorizontalFlip())\n",
    "        if 'ColorJitter' in augs:\n",
    "            t.append(transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0))\n",
    "        if 'resizedcrop' in augs:\n",
    "            t.append(transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0)))\n",
    "        if 'RandAugment' in augs: # need to review RandAugment()\n",
    "            t.append(RandAugment())\n",
    "            # t.append(transforms.RandomApply([\n",
    "            #     transforms.RandomRotation(degrees=10),\n",
    "            #     transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "            #     transforms.RandomPerspective(distortion_scale=0.05)\n",
    "            # ], p=0.5))\n",
    "\n",
    "        t.append(transforms.Resize((img_size, img_size)))\n",
    "    \n",
    "        return transforms.Compose(t)\n",
    "\n",
    "    def load_anns(self):\n",
    "        s3_client = boto3.client('s3')\n",
    "        self.anns = []\n",
    "        for ann_file in self.ann_files:\n",
    "            bucket, key = self.parse_s3_path(ann_file)\n",
    "            response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "            json_data = json.loads(response['Body'].read())\n",
    "            for image in json_data['images']:\n",
    "                file_name = image['file_name']\n",
    "                label = self.extract_label_from_filename(file_name)  # Use the new method\n",
    "                for target in image['targets']:\n",
    "                    ann = {\n",
    "                        'img_path': f's3://210bucket/WIDER/{file_name}',\n",
    "                        'bbox': target['bbox'],\n",
    "                        'label': label,  \n",
    "                        'target': target['attribute']\n",
    "                    }\n",
    "                    self.anns.append(ann)\n",
    "        print(f\"Loaded annotations: {len(self.anns)}\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self)\n",
    "        ann = self.anns[idx]\n",
    "    \n",
    "        if not isinstance(ann, dict) or \"img_path\" not in ann:\n",
    "            raise ValueError(f\"Annotation at index {idx} is not a dictionary with an 'img_path' key: {ann}\")\n",
    "    \n",
    "        bucket, key = self.parse_s3_path(ann[\"img_path\"])\n",
    "        try:\n",
    "            response = self.s3_client.get_object(Bucket=bucket, Key=key)\n",
    "            img = Image.open(io.BytesIO(response['Body'].read())).convert(\"RGB\")\n",
    "        except self.s3_client.exceptions.NoSuchKey:\n",
    "            print(f\"File not found: s3://{bucket}/{key}\")\n",
    "            img = self.get_placeholder_image()\n",
    "            return None \n",
    "    \n",
    "        x, y, w, h = ann['bbox']\n",
    "        img = img.crop((x, y, x + w, y + h))\n",
    "        img = self.augment(img)\n",
    "        img = self.transform(img)\n",
    "    \n",
    "        label = torch.tensor(ann['label'], dtype=torch.long)    \n",
    "        target = torch.tensor(ann['target'], dtype=torch.float32)\n",
    "    \n",
    "        message = {\n",
    "            \"label\": label,  \n",
    "            \"target\": target,  \n",
    "            \"img\": img \n",
    "        }\n",
    "    \n",
    "        return message\n",
    "\n",
    "\n",
    "    def get_placeholder_image(self):\n",
    "        return Image.new('RGB', (256, 256), color = 'gray')\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_s3_path(s3_path):\n",
    "        if not s3_path.startswith(\"s3://\"):\n",
    "            raise ValueError(f\"Invalid S3 path: {s3_path}\")\n",
    "        s3_path = s3_path[5:]\n",
    "        bucket, key = s3_path.split('/', 1)\n",
    "        return bucket, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f9594ae-71a7-40a7-8c7a-bd24aaac4bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_wider(batch_size=64, subset_size=None):\n",
    "\n",
    "    train_augs = ['randomflip', 'ColorJitter', 'resizedcrop', 'RandAugment']\n",
    "    test_augs = []  \n",
    "    img_size = 256 \n",
    "    \n",
    "    train_dataset = DataSet(\n",
    "        ann_files=['s3://210bucket/wider_attribute_annotation/wider_attribute_trainval.json'],  \n",
    "        augs=train_augs,\n",
    "        img_size=img_size,\n",
    "        dataset='wider'\n",
    "    )\n",
    "    \n",
    "    test_dataset = DataSet(\n",
    "        ann_files=['s3://210bucket/wider_attribute_annotation/wider_attribute_test.json'], \n",
    "        augs=test_augs,\n",
    "        img_size=img_size,\n",
    "        dataset='wider'\n",
    "    )\n",
    "    \n",
    "    if subset_size is not None:\n",
    "        train_indices = np.random.choice(len(train_dataset), subset_size, replace=False)\n",
    "        test_indices = np.random.choice(len(test_dataset), subset_size, replace=False)\n",
    "        \n",
    "        train_subset = Subset(train_dataset, train_indices)\n",
    "        test_subset = Subset(test_dataset, test_indices)\n",
    "        \n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "708ee75d-2c9f-4f41-bb28-101fb565f3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded annotations: 28345\n",
      "Compose(\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    ColorJitter(brightness=(0.5, 1.5), contrast=(0.5, 1.5), saturation=(0.5, 1.5), hue=None)\n",
      "    RandomResizedCrop(size=(256, 256), scale=(0.7, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=warn)\n",
      "    RandAugment(num_ops=2, magnitude=9, num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
      "    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n",
      "Loaded annotations: 29179\n",
      "Compose(\n",
      "    Resize(size=(256, 256), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader  = load_wider(batch_size=64, subset_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6806e36-5b87-4b34-a680-1d1d020d3efe",
   "metadata": {},
   "source": [
    "## Testing - RKD Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a469db80-e536-4082-a9df-95786efe286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import boto3\n",
    "import io\n",
    "\n",
    "# new libraries\n",
    "from models_package.models import Teacher, Student, CustomResNet18\n",
    "from torchvision import datasets, transforms, models\n",
    "from data.data_loader import load_cifar10, load_cifar100, load_imagenet, load_prof\n",
    "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n",
    "from torchvision.models.resnet import ResNet18_Weights, ResNet34_Weights\n",
    "from utils.loss_functions import tkd_kdloss, DD_loss, AD_loss, RKDDistanceLoss, RKDAngleLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f0a6299-b393-4ed2-b24d-f2e8cfb22cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    inputs, _ = next(iter(dataloader))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher(inputs)\n",
    "    teacher_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        student_outputs = student(inputs)\n",
    "    student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n",
    "\n",
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs.to(device))\n",
    "            student_outputs = student(inputs.to(device))\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(torch.argmax(teacher_outputs, dim=1).cpu().numpy())\n",
    "        all_student_preds.append(torch.argmax(student_outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),  # Updated line\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def plot_comparison(labels, teacher_values, student_values, title, ylabel):\n",
    "    # Convert parameter count to millions\n",
    "    if 'Parameter Count' in title or 'Parameter Count' in ylabel:\n",
    "        teacher_values = [value / 1e6 for value in teacher_values]\n",
    "        student_values = [value / 1e6 for value in student_values]\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, teacher_values, width, label='Teacher')\n",
    "    rects2 = ax.bar(x + width/2, student_values, width, label='Student')\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5115025-c369-400c-bbad-2cbdad37fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.04 \n",
    "num_epochs = 1 \n",
    "num_workers = 2\n",
    "batch_size = 32\n",
    "temperature = 4.0\n",
    "alpha = 0.9\n",
    "momentum = 0.9\n",
    "num_classes = 10\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "\n",
    "# new parameters\n",
    "# lr_input = 0.1\n",
    "# momentum_input = 0.9\n",
    "weight_decay_input = 5e-4\n",
    "# epochs = 20\n",
    "# T = 4.0 # temperatureture\n",
    "# alpha = 0.9\n",
    "patience = 5  # for early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d614d52b-41f1-42ae-8379-37fdff8bfa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Testing 1 ######################\n",
    "# Create instances of your models\n",
    "teacher_model = models.resnet50(pretrained=True)  # Keep ResNet50 as it is\n",
    "teacher_model.eval()  # Set teacher model to evaluation mode\n",
    "student_model = CustomResNet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d14240f-db55-4b5c-8ed7-222209c988f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the final classification layer is added to the model\n",
    "teacher_model.fc = nn.Linear(teacher_model.fc.in_features, 10)\n",
    "student_model.fc = nn.Linear(student_model.output_size, 10)\n",
    "\n",
    "# Optimizer and scheduler for the student model\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Optimizer and scheduler for the teacher model\n",
    "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "teacher_scheduler = torch.optim.lr_scheduler.StepLR(teacher_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Assuming the device is a CUDA device if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# teacher_model.to(device)\n",
    "# student_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c1795a2-dd2c-4129-869b-59d0d04dffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the teacher model\n",
    "def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, num_epochs=5, patience=5):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    best_train_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        for batch in tqdm(trainloader):\n",
    "            inputs = batch['img'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            print(inputs.shape)  # Should show torch.Size([64, C, H, W])\n",
    "            print(labels.shape)  # Should show torch.Size([64]) or torch.Size([64, 1])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        epoch_loss /= num_batches  \n",
    "        \n",
    "        # Check for early stopping\n",
    "        if epoch_loss < best_train_loss:\n",
    "            best_train_loss = epoch_loss\n",
    "            patience_counter = 0 \n",
    "            # checkpoint\n",
    "            torch.save(model.state_dict(), f'teacher_model_weights_rkd_prof_checkpoint.pth')\n",
    "            torch.save(model, f'teacher_model_rkd_prof_checkpoint.pth')\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Finished Training Teacher\")\n",
    "\n",
    "\n",
    "# Function to train the student model with knowledge distillation\n",
    "def train_student_with_distillation(student, teacher, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs, patience=5):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    student.to(device)\n",
    "    teacher.to(device)\n",
    "    best_train_loss = float('inf')  \n",
    "    patience_counter = 0 \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        for i, (inputs, labels) in enumerate(tqdm(trainloader)):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student(inputs)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "            # ce_loss = criterion(student_outputs, labels)\n",
    "            # kd_loss = tkd_kdloss(student_outputs, teacher_outputs, temperature=temperature)  # from utils.loss_functions\n",
    "            # loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "\n",
    "            distance_loss = RKDDistanceLoss()(student_outputs, teacher_outputs)\n",
    "            angle_loss = RKDAngleLoss()(student_outputs, teacher_outputs)\n",
    "            loss = criterion(student_outputs, labels) + 0.1 * (distance_loss + angle_loss)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            if i % 100 == 99:  \n",
    "                print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        epoch_loss /= num_batches  \n",
    "\n",
    "        # Check for early stopping\n",
    "        if epoch_loss < best_train_loss:\n",
    "            best_train_loss = epoch_loss\n",
    "            patience_counter = 0 \n",
    "            torch.save(student.state_dict(), f'student_model_weights_rkd_prof_checkpoint.pth')\n",
    "            torch.save(student, f'student_model_rkd_prof_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break  \n",
    "\n",
    "        scheduler.step() \n",
    "\n",
    "    print(\"Finished Training Student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be2a3dd2-9ced-490b-b518-4ca2374706f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Call the function to train the teacher model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_teacher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Call the function to train the student model with knowledge distillation\u001b[39;00m\n\u001b[1;32m      8\u001b[0m train_student_with_distillation(student_model, teacher_model, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs)\n",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m, in \u001b[0;36mtrain_teacher\u001b[0;34m(model, trainloader, criterion, optimizer, scheduler, device, num_epochs, patience)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_teacher\u001b[39m(model, trainloader, criterion, optimizer, scheduler, device, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     best_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     patience_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1158\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Assuming the device is a CUDA device if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Call the function to train the teacher model\n",
    "train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs=num_epochs)\n",
    "\n",
    "# Call the function to train the student model with knowledge distillation\n",
    "train_student_with_distillation(student_model, teacher_model, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93b302cd-5cad-44ed-bc09-142a2a38c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86792245-fb8f-4afa-9120-96ac05de041b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
