{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed182ac-bc78-4d44-b703-561a052cfc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "import torchvision\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances_argmin_min, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "from torchvision.models import EfficientNet\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7437867-ecd5-4643-9fb8-a2fc1d1f15f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0005 # 0.096779\n",
    "epochs = 3\n",
    "epochs_pretrain = 3\n",
    "epochs_optimal_lr = 3\n",
    "patience = 6\n",
    "momentum = 0.9\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "\n",
    "# list of lambda values to loop through for grid search\n",
    "lmda_list = [5,0]\n",
    "\n",
    "# labels used including for plotting\n",
    "class_labels = [0, 1, 3, 4, 6, 7, 11, 15, 17, 18, 19, 20, 22, 25, 27, 28, 30, 31, 33, 35, 36, 37, 39, 43, 44, 50, 51, 54, 57, 58]\n",
    "class_labels_new = torch.tensor([i for i in range(len(class_labels))])\n",
    "num_classes = 16\n",
    "class_names_new = [f\"Class {label}\" for label in range(num_classes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11b671f-0b71-438a-bf75-7ceb293b72ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# set device to cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Count the number of GPUs available\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "print(\"CUDA Available:\", cuda_available)\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eabaae7-724f-4ee9-9df8-9463d205b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your JSON file is named 'your_file.json'\n",
    "file_path = './WIDER/Annotations/wider_attribute_trainval.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Load the JSON data from the file\n",
    "    data = json.load(file)\n",
    "\n",
    "class_idx = data['scene_id_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c142a7d7-9627-48b6-a5aa-86527150ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_mapping = {\n",
    "    0: \"Team_Sports\",\n",
    "    1: \"Celebration\",\n",
    "    2: \"Parade\",\n",
    "    3: \"Waiter_Or_Waitress\",\n",
    "    4: \"Individual_Sports\",\n",
    "    5: \"Surgeons\",\n",
    "    6: \"Spa\",\n",
    "    7: \"Law_Enforcement\",\n",
    "    8: \"Business\",\n",
    "    9: \"Dresses\",\n",
    "    10: \"Water_Activities\",\n",
    "    11: \"Picnic\",\n",
    "    12: \"Rescue\",\n",
    "    13: \"Cheering\",\n",
    "    14: \"Performance_And_Entertainment\",\n",
    "    15: \"Family\"\n",
    "}\n",
    "\n",
    "# Ensure that all 16 new classes are covered\n",
    "# If some classes are not explicitly mentioned in new_label_mapping, add them\n",
    "for i in range(num_classes):\n",
    "    if i not in new_label_mapping:\n",
    "        new_label_mapping[i] = \"Additional Category {}\".format(i)\n",
    "\n",
    "class_idx = new_label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cab11dc9-d6c6-4c45-8452-cff149af8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, ann_files, augs, img_size, dataset):\n",
    "\n",
    "        # Create a mapping from old labels to new labels\n",
    "        self.label_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted(class_labels))}\n",
    "\n",
    "        self.new_label_mapping = {\n",
    "            0: 2,  # Parade\n",
    "            1: 8,  # Business\n",
    "            2: 7,  # Law Enforcement\n",
    "            3: 14,  # Performance and Entertainment\n",
    "            4: 1,  # Celebration\n",
    "            5: 13,  # Cheering\n",
    "            6: 8,  # Business\n",
    "            7: 8,  # Business\n",
    "            8: 1,  # Celebration\n",
    "            9: 14,  # Performance and Entertainment\n",
    "            10: 15, # Family\n",
    "            11: 15, # Family\n",
    "            12: 11, # Picnic\n",
    "            13: 7, # Law Enforcement\n",
    "            14: 6, # Spa\n",
    "            15: 13, # Cheering\n",
    "            16: 5, # Surgeons\n",
    "            17: 3, # Waiter or Waitress\n",
    "            18: 4, # Individual Sports\n",
    "            19: 0, # Team Sports\n",
    "            20: 0, # Team Sports\n",
    "            21: 0, # Team Sports\n",
    "            22: 4, # Individual Sports\n",
    "            23: 10, # Water Activities\n",
    "            24: 4, # Individual Sports\n",
    "            25: 1, # Celebration\n",
    "            26: 9, # Dresses\n",
    "            27: 12, # Rescue\n",
    "            28: 10,# Water Activities\n",
    "            29: 0  # Team Sports\n",
    "        }\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.ann_files = ann_files\n",
    "        self.augment = self.augs_function(augs, img_size)\n",
    "        # Initialize transformations directly\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "            ] \n",
    "        )\n",
    "        if self.dataset == \"wider\":\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                ] \n",
    "            )        \n",
    "\n",
    "        self.anns = []\n",
    "        self.load_anns()\n",
    "        print(self.augment)\n",
    "\n",
    "    def augs_function(self, augs, img_size):            \n",
    "        t = []\n",
    "        if 'randomflip' in augs:\n",
    "            t.append(transforms.RandomHorizontalFlip())\n",
    "        if 'ColorJitter' in augs:\n",
    "            t.append(transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0))\n",
    "        if 'resizedcrop' in augs:\n",
    "            t.append(transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0)))\n",
    "        if 'RandAugment' in augs:\n",
    "            t.append(transforms.RandAugment())\n",
    "\n",
    "        t.append(transforms.Resize((img_size, img_size)))\n",
    "\n",
    "        return transforms.Compose(t)\n",
    "    \n",
    "    def load_anns(self):\n",
    "        self.anns = []\n",
    "        for ann_file in self.ann_files:\n",
    "            json_data = json.load(open(ann_file, \"r\"))\n",
    "            self.anns += json_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Make sure the index is within bounds\n",
    "        idx = idx % len(self)\n",
    "        ann = self.anns[idx]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to open the image file\n",
    "            img = Image.open(f'WIDER/Image/{ann[\"file_name\"]}').convert(\"RGB\")\n",
    "\n",
    "            # If this is the wider dataset, proceed with specific processing\n",
    "            # x, y, w, h = ann['bbox']\n",
    "            # img_area = img.crop([x, y, x+w, y+h])\n",
    "            img_area = self.augment(img)\n",
    "            img_area = self.transform(img_area)\n",
    "            attributes_list = [target['attribute'] for target in ann['targets']]\n",
    "            num_people = len(attributes_list)\n",
    "            attributes_distribution = [max(sum(attribute), 0)/num_people for attribute in zip(*attributes_list)]\n",
    "            # Extract label from image path\n",
    "            img_path = f'WIDER/Image/{ann[\"file_name\"]}'\n",
    "            label = self.extract_label(img_path)  # You might need to implement this method\n",
    "            \n",
    "            return {\n",
    "                \"label\": label,\n",
    "                \"target\": torch.tensor([attributes_distribution[0]], dtype=torch.float32),\n",
    "                \"img\": img_area\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If any error occurs during the processing of an image, log the error and the index\n",
    "            print(f\"Error processing image at index {idx}: {e}\")\n",
    "            # Instead of returning None, raise the exception\n",
    "            raise\n",
    "\n",
    "    def extract_label(self, img_path):\n",
    "        original_label = None\n",
    "    \n",
    "        if \"WIDER/Image/train\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/train/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/test\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/test/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/val\" in img_path:  # Handle validation images\n",
    "            label_str = img_path.split(\"WIDER/Image/val/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "    \n",
    "        if original_label is not None:\n",
    "            remapped_label = self.label_mapping[original_label]\n",
    "            new_label_mapping = self.new_label_mapping[remapped_label]\n",
    "            return new_label_mapping\n",
    "        else:\n",
    "            raise ValueError(f\"Label could not be extracted from path: {img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742c75bb-56ba-497e-8160-745d070907fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = ['data/wider/trainval_wider.json']\n",
    "test_file = ['data/wider/test_wider.json']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21c07e8c-9d6a-4f00-9b72-dbb9f35f6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # Filter out any None items in the batch\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    # If after filtering the batch is empty, handle this case by either returning an empty tensor or raising an exception\n",
    "    if len(batch) == 0:\n",
    "        raise ValueError(\"Batch is empty after filtering out None items.\")\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e188300a-7896-4ed7-ba20-549a3057fbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = DataSet(train_file, augs = ['RandAugment'], img_size = 226, dataset = 'wider')\n",
    "train_dataset = DataSet(train_file, augs = [], img_size = 226, dataset = 'wider')\n",
    "test_dataset = DataSet(test_file, augs = [], img_size = 226, dataset = 'wider')\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                         num_workers=num_workers, collate_fn=custom_collate)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c464baa9-1e78-4409-8133-cbbc815ace9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "def calculate_recall_multiclass(conf_matrix):\n",
    "    if conf_matrix.size == 0:\n",
    "        return np.array([0])  # Return an array with a single zero if the confusion matrix is empty\n",
    "\n",
    "    recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "    recalls[np.isnan(recalls)] = 0  # Replace NaN with 0\n",
    "    return recalls  # This will always be an array\n",
    "\n",
    "def evaluate_model_with_gender_multiclass(pred, label, gender, num_classes):\n",
    "    predictions = pred.cpu().numpy()\n",
    "    true_labels = label.cpu().numpy()\n",
    "    gender = gender.cpu().numpy()\n",
    "\n",
    "    valid_disparities = []\n",
    "\n",
    "    for cls in range(num_classes):\n",
    "        # Calculate indices for male and female instances of the current class\n",
    "        male_indices = np.where((gender >= 0.5) & (true_labels == cls))[0]\n",
    "        female_indices = np.where((gender < 0.5) & (true_labels == cls))[0]\n",
    "\n",
    "        if len(male_indices) > 0 and len(female_indices) > 0:\n",
    "            male_pred_cls = predictions[male_indices]\n",
    "            female_pred_cls = predictions[female_indices]\n",
    "            male_true_cls = true_labels[male_indices]\n",
    "            female_true_cls = true_labels[female_indices]\n",
    "\n",
    "            # Ensure that both male and female true labels contain the class 'cls'\n",
    "            if cls in male_true_cls and cls in female_true_cls:\n",
    "                male_conf_matrix = confusion_matrix(male_true_cls, male_pred_cls, labels=[cls])\n",
    "                female_conf_matrix = confusion_matrix(female_true_cls, female_pred_cls, labels=[cls])\n",
    "\n",
    "                if male_conf_matrix.size > 0 and female_conf_matrix.size > 0:\n",
    "                    male_recall_cls = calculate_recall_multiclass(male_conf_matrix)[0]  # Take first element\n",
    "                    female_recall_cls = calculate_recall_multiclass(female_conf_matrix)[0]  # Take first element\n",
    "\n",
    "                    disparity = np.abs(male_recall_cls - female_recall_cls)\n",
    "                    valid_disparities.append(disparity)\n",
    "\n",
    "    return np.mean(valid_disparities) if valid_disparities else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3df5bc74-4a24-42a0-9ca4-a4e04861d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EfficientNet B0 model\n",
    "model = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Determine the number of output features from the feature extractor part of EfficientNet B0\n",
    "num_ftrs = model.classifier[1].in_features  # This is the correct number of input features for your adversarial classifier\n",
    "\n",
    "# Modify the classifier layer of the EfficientNet model to match your number of classes\n",
    "model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Move the EfficientNet model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # Outputting a single value for bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Assuming bias is a probability-like value\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the Critic model\n",
    "critic = Critic(input_size=16).to(device)  # Adjust the input size based on your model's output\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate)\n",
    "critic_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Redefine your main model optimizer if needed\n",
    "actor_optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "actor_loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777235b-0ae2-47e8-a1a8-562402565000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Critic Training:  60%|██████████████████████████████████████▋                         | 98/162 [00:30<00:19,  3.21it/s, Critic Loss=0.00894]"
     ]
    }
   ],
   "source": [
    "lambda_factor = 3\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    # Adversarial Training Loop\n",
    "    model.eval()  # Actor in evaluation mode\n",
    "    critic.train()\n",
    "\n",
    "    trainloader_tqdm = tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}, Critic Training')\n",
    "    for batch_data in trainloader_tqdm:\n",
    "        images = batch_data[\"img\"].to(device)\n",
    "        labels = batch_data[\"label\"].to(device)\n",
    "        gender_scores = batch_data[\"target\"].to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            actor_output = model(images)\n",
    "            class_predictions = torch.argmax(actor_output, dim=1)\n",
    "    \n",
    "        # Compute the bias\n",
    "        bias = evaluate_model_with_gender_multiclass(class_predictions, labels, gender_scores, num_classes)\n",
    "        \n",
    "        bias_mean = torch.tensor([bias], device=device, dtype=torch.float32)\n",
    "    \n",
    "        critic_output = critic(actor_output.detach())\n",
    "        critic_loss = critic_loss_fn(critic_output, bias_mean)\n",
    "        critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optimizer.step()\n",
    "        trainloader_tqdm.set_postfix({'Critic Loss': critic_loss.item()})\n",
    "\n",
    "    # Main Model Training Loop\n",
    "    model.train()\n",
    "    critic.eval()\n",
    "\n",
    "    trainloader_tqdm = tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}, Actor Training')\n",
    "    for batch_data in trainloader_tqdm:\n",
    "        images = batch_data[\"img\"].to(device)\n",
    "        labels = batch_data[\"label\"].to(device)\n",
    "        gender_scores = batch_data[\"target\"].to(device)\n",
    "\n",
    "        actor_output = model(images)\n",
    "        class_predictions = torch.argmax(actor_output, dim=1)\n",
    "\n",
    "        accuracy = (class_predictions == labels).float().mean()\n",
    "        batch_bias = evaluate_model_with_gender_multiclass(class_predictions, labels, gender_scores, num_classes)\n",
    "\n",
    "        main_loss = actor_loss_fn(actor_output, labels)\n",
    "        combined_loss = main_loss + lambda_factor * torch.tensor(batch_bias, device=device)\n",
    "\n",
    "        actor_optimizer.zero_grad()\n",
    "        combined_loss.backward()\n",
    "        actor_optimizer.step()\n",
    "\n",
    "        trainloader_tqdm.set_postfix({'Actor Loss': combined_loss.item(), 'Accuracy': accuracy.item(), 'Bias': batch_bias})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f8d2be-db17-4f32-ac35-594f77f550ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adversarial Intraprocessing Algorithm.\n",
    "\"\"\"\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from models import load_model, Critic\n",
    "from utils import get_best_thresh, get_test_objective, get_valid_objective, compute_bias\n",
    "\n",
    "logger = logging.getLogger(\"Debiasing\")\n",
    "\n",
    "\n",
    "def adversarial_debiasing(model_state_dict, data, config, device):\n",
    "    logger.info('Training Adversarial model.')\n",
    "    actor = load_model(data.num_features, config.get('hyperparameters', {}))\n",
    "    actor.load_state_dict(model_state_dict)\n",
    "    actor.to(device)\n",
    "    hid = config['hyperparameters']['hid'] if 'hyperparameters' in config else 32\n",
    "    critic = Critic(hid * config['adversarial']['batch_size'], num_deep=config['adversarial']['num_deep'], hid=hid)\n",
    "    critic.to(device)\n",
    "    critic_optimizer = optim.Adam(critic.parameters())\n",
    "    critic_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=config['adversarial']['lr'])\n",
    "    actor_loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "    for epoch in range(config['adversarial']['epochs']):\n",
    "        for param in critic.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in actor.parameters():\n",
    "            param.requires_grad = False\n",
    "        actor.eval()\n",
    "        critic.train()\n",
    "        for step in range(config['adversarial']['critic_steps']):\n",
    "            critic_optimizer.zero_grad()\n",
    "            indices = torch.randint(0, data.X_valid.size(0), (config['adversarial']['batch_size'],))\n",
    "            cX_valid = data.X_valid_gpu[indices]\n",
    "            cy_valid = data.y_valid[indices]\n",
    "            cp_valid = data.p_valid[indices]\n",
    "            with torch.no_grad():\n",
    "                scores = actor(cX_valid)[:, 0].reshape(-1).cpu().numpy()\n",
    "\n",
    "            bias = compute_bias(scores, cy_valid.numpy(), cp_valid, config['metric'])\n",
    "\n",
    "            res = critic(actor.trunc_forward(cX_valid))\n",
    "            loss = critic_loss_fn(torch.tensor([bias], device=device), res[0])\n",
    "            loss.backward()\n",
    "            train_loss = loss.item()\n",
    "            critic_optimizer.step()\n",
    "            if (epoch % 10 == 0) and (step % 100 == 0):\n",
    "                logger.info(f'=======> Critic Epoch: {(epoch, step)} loss: {train_loss}')\n",
    "\n",
    "        for param in critic.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in actor.parameters():\n",
    "            param.requires_grad = True\n",
    "        actor.train()\n",
    "        critic.eval()\n",
    "        for step in range(config['adversarial']['actor_steps']):\n",
    "            actor_optimizer.zero_grad()\n",
    "            indices = torch.randint(0, data.X_valid.size(0), (config['adversarial']['batch_size'],))\n",
    "            cy_valid = data.y_valid_gpu[indices]\n",
    "            cX_valid = data.X_valid_gpu[indices]\n",
    "\n",
    "            pred_bias = critic(actor.trunc_forward(cX_valid)) # outputs of adversary model\n",
    "            bceloss = actor_loss_fn(actor(cX_valid)[:, 0], cy_valid)\n",
    "\n",
    "            # loss = lam*abs(pred_bias) + (1-lam)*loss   \n",
    "\n",
    "            # max(1, lambda * abs(outputs of adversary model) - epsilon (hypderparam) + margin + 1)\n",
    "            # epsilon = target bias (0.09?)\n",
    "            # margin = margin of error\n",
    "            objloss = max(1, config['adversarial']['lambda']*(abs(pred_bias[0][0])-config['objective']['epsilon']+config['adversarial']['margin'])+1) * bceloss\n",
    "            # ce-loss instead of BCE\n",
    "            # dont need to binarize\n",
    "            \n",
    "            objloss.backward()\n",
    "            train_loss = objloss.item()\n",
    "            actor_optimizer.step()\n",
    "            if (epoch % 10 == 0) and (step % 100 == 0):\n",
    "                logger.info(f'=======> Actor Epoch: {(epoch, step)} loss: {train_loss}')\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                scores = actor(data.X_valid_gpu)[:, 0].reshape(-1, 1).cpu().numpy()\n",
    "                _, best_adv_obj = get_best_thresh(scores, np.linspace(0, 1, 1001), data, config, valid=False, margin=config['adversarial']['margin'])\n",
    "                logger.info(f'Objective: {best_adv_obj}')\n",
    "\n",
    "    logger.info('Finding optimal threshold for Adversarial model.')\n",
    "    with torch.no_grad():\n",
    "        scores = actor(data.X_valid_gpu)[:, 0].reshape(-1, 1).cpu().numpy()\n",
    "\n",
    "    best_adv_thresh, _ = get_best_thresh(scores, np.linspace(0, 1, 1001), data, config, valid=False, margin=config['adversarial']['margin'])\n",
    "\n",
    "    logger.info('Evaluating Adversarial model on best threshold.')\n",
    "    with torch.no_grad():\n",
    "        labels = (actor(data.X_valid_gpu)[:, 0] > best_adv_thresh).reshape(-1, 1).cpu().numpy()\n",
    "    results_valid = get_valid_objective(labels, data, config)\n",
    "    logger.info(f'Results: {results_valid}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        labels = (actor(data.X_test_gpu)[:, 0] > best_adv_thresh).reshape(-1, 1).cpu().numpy()\n",
    "    results_test = get_test_objective(labels, data, config)\n",
    "\n",
    "    return results_valid, results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836d70b-6686-4e3a-a9e6-e5266f7227ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
