{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f79206-feac-4302-abb1-d699b6354a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "import torchvision\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances_argmin_min, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "from torchvision.models import EfficientNet\n",
    "from torchvision.datasets import ImageFolder\n",
    "from utils.loss_functions import tkd_kdloss\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55b8b44c-02d4-41a9-b260-c921a5d99c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_available_dir(root_dir):\n",
    "    \"\"\"\n",
    "    If a directory at 'root_dir' exists, append a number to create a new path.\n",
    "    For example, if 'RUN' exists, this returns 'RUN_01', and so on.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(root_dir):\n",
    "        return root_dir\n",
    "\n",
    "    i = 1\n",
    "    new_dir = f\"{root_dir}_{str(i).zfill(2)}\"\n",
    "\n",
    "    while os.path.exists(new_dir):\n",
    "        i += 1\n",
    "        new_dir = f\"{root_dir}_{str(i).zfill(2)}\"\n",
    "\n",
    "    return new_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc3c0ec8-98dc-4933-b9a3-e96fce39c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0005 # 0.096779\n",
    "epochs = 100\n",
    "epochs_pretrain = 3\n",
    "epochs_optimal_lr = 3\n",
    "patience_teacher = 6\n",
    "patience_student = 6\n",
    "temperature = 4.0\n",
    "alpha = 0.9\n",
    "momentum = 0.9\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "\n",
    "# set to true to use stratified sampling\n",
    "stratified_sampling_flag = False\n",
    "\n",
    "# list of lambda values to loop through for grid search\n",
    "lmda_list_teacher = [0]\n",
    "lmda_list_student = [0]\n",
    "\n",
    "# labels used including for plotting\n",
    "class_labels = [0, 1, 3, 4, 6, 7, 11, 15, 17, 18, 19, 20, 22, 25, 27, 28, 30, 31, 33, 35, 36, 37, 39, 43, 44, 50, 51, 54, 57, 58]\n",
    "class_labels_new = torch.tensor([i for i in range(len(class_labels))])\n",
    "num_classes = 16\n",
    "class_names_new = [f\"Class {label}\" for label in range(num_classes)]\n",
    "\n",
    "# SPECIFY OUT DIR NAME\n",
    "# Create directory and file path to save all outputs\n",
    "directory_name = 'Smaller_Student_Models'\n",
    "# This will append version to end of directory name and create new directory if already exists\n",
    "output_dir = get_next_available_dir(directory_name)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b696c15-7e2a-4c36-ba16-c174975be92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for subdirectories inside output_dir\n",
    "models_dir = os.path.join(output_dir, 'models')\n",
    "weights_dir = os.path.join(output_dir, 'weights')\n",
    "\n",
    "# Create the subdirectories if they don't exist\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(weights_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765cd0f9-e661-4888-850b-35ddfb47e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Initialize EfficientNet B2\n",
    "efficientnet_b2 = models.efficientnet_b2(pretrained=True)\n",
    "efficientnet_b2.classifier[1] = nn.Linear(efficientnet_b2.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Initialize EfficientNet B1\n",
    "efficientnet_b1 = models.efficientnet_b1(pretrained=True)\n",
    "efficientnet_b1.classifier[1] = nn.Linear(efficientnet_b1.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Initialize EfficientNet B0\n",
    "efficientnet_b0 = models.efficientnet_b0(pretrained=True)\n",
    "efficientnet_b0.classifier[1] = nn.Linear(efficientnet_b0.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Initialize MobileNetV2\n",
    "student_model_mobilenetv2 = models.mobilenet_v2(pretrained=True)\n",
    "student_model_mobilenetv2.classifier[1] = nn.Linear(student_model_mobilenetv2.classifier[1].in_features, num_classes)\n",
    "\n",
    "# Initialize ShuffleNet\n",
    "student_model_shufflenet = models.shufflenet_v2_x1_0(pretrained=True)\n",
    "num_features = student_model_shufflenet.fc.in_features\n",
    "student_model_shufflenet.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "# Initialize SqueezeNet\n",
    "student_model_squeezenet = models.squeezenet1_0(pretrained=True)\n",
    "student_model_squeezenet.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "student_model_squeezenet.num_classes = num_classes\n",
    "\n",
    "class CustomSmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes=16):\n",
    "        super(CustomSmallCNN, self).__init__()\n",
    "        # Convolutional layer (input channels, output channels, kernel size)\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\n",
    "        # Pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Adaptive pooling to make it size-agnostic\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((5, 5))\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 5 * 5, 120)  # 5*5 is derived from the adaptive pooling size\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolutions, followed by max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Adaptive pooling and flattening\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize Custom Ultra Tiny CNN\n",
    "student_model_custom_small_cnn = CustomSmallCNN(num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb393bfb-07a6-4151-95e2-d33f54700f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EfficientNet B2 Total Parameters: 7723538\n",
      "EfficientNet B1 Total Parameters: 6533680\n",
      "EfficientNet B0 Total Parameters: 4028044\n",
      "MobileNetV2 Total Parameters: 2244368\n",
      "ShuffleNet Total Parameters: 1270004\n",
      "SqueezeNet Total Parameters: 743632\n",
      "Custom Small CNN Total Parameters: 112732\n"
     ]
    }
   ],
   "source": [
    "# Count parameters\n",
    "total_params_efficientnet_b2 = count_parameters(efficientnet_b2)\n",
    "total_params_efficientnet_b1 = count_parameters(efficientnet_b1)\n",
    "total_params_efficientnet_b0 = count_parameters(efficientnet_b0)\n",
    "total_params_mobilenetv2 = count_parameters(student_model_mobilenetv2)\n",
    "total_params_shufflenet = count_parameters(student_model_shufflenet)\n",
    "total_params_squeezenet = count_parameters(student_model_squeezenet)\n",
    "total_params_custom_small_cnn = count_parameters(student_model_custom_small_cnn)\n",
    "\n",
    "print(f\"EfficientNet B2 Total Parameters: {total_params_efficientnet_b2}\")\n",
    "print(f\"EfficientNet B1 Total Parameters: {total_params_efficientnet_b1}\")\n",
    "print(f\"EfficientNet B0 Total Parameters: {total_params_efficientnet_b0}\")\n",
    "print(f\"MobileNetV2 Total Parameters: {total_params_mobilenetv2}\")\n",
    "print(f\"ShuffleNet Total Parameters: {total_params_shufflenet}\")\n",
    "print(f\"SqueezeNet Total Parameters: {total_params_squeezenet}\")\n",
    "print(f\"Custom Small CNN Total Parameters: {total_params_custom_small_cnn}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b66c2-f45b-41b1-a280-739775145f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models\n",
    "student_models = [\n",
    "    efficientnet_b2, \n",
    "    efficientnet_b1, \n",
    "    efficientnet_b0, \n",
    "    student_model_mobilenetv2, \n",
    "    student_model_shufflenet, \n",
    "    student_model_squeezenet,\n",
    "    student_model_custom_small_cnn\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2770eac-10b7-47de-ba06-9aa63cd7ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# set device to cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Count the number of GPUs available\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "print(\"CUDA Available:\", cuda_available)\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46265942-58a1-4c1a-a7ea-82d919a7e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your JSON file is named 'your_file.json'\n",
    "file_path = './WIDER/Annotations/wider_attribute_trainval.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Load the JSON data from the file\n",
    "    data = json.load(file)\n",
    "\n",
    "class_idx = data['scene_id_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "312fa312-b347-4434-862b-d25e32a79108",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_mapping = {\n",
    "    0: \"Team_Sports\",\n",
    "    1: \"Celebration\",\n",
    "    2: \"Parade\",\n",
    "    3: \"Waiter_Or_Waitress\",\n",
    "    4: \"Individual_Sports\",\n",
    "    5: \"Surgeons\",\n",
    "    6: \"Spa\",\n",
    "    7: \"Law_Enforcement\",\n",
    "    8: \"Business\",\n",
    "    9: \"Dresses\",\n",
    "    10: \"Water_Activities\",\n",
    "    11: \"Picnic\",\n",
    "    12: \"Rescue\",\n",
    "    13: \"Cheering\",\n",
    "    14: \"Performance_And_Entertainment\",\n",
    "    15: \"Family\"\n",
    "}\n",
    "\n",
    "# Ensure that all 16 new classes are covered\n",
    "# If some classes are not explicitly mentioned in new_label_mapping, add them\n",
    "for i in range(num_classes):\n",
    "    if i not in new_label_mapping:\n",
    "        new_label_mapping[i] = \"Additional Category {}\".format(i)\n",
    "\n",
    "class_idx = new_label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c922f94-bf69-40d5-bc88-1e6fab4b4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedBatchSampler:\n",
    "    \"\"\"Stratified batch sampling\n",
    "    Provides equal representation of target classes in each batch\n",
    "    \"\"\"\n",
    "    def __init__(self, y, batch_size, shuffle=True):\n",
    "        if torch.is_tensor(y):\n",
    "            y = y.numpy()\n",
    "        assert len(y.shape) == 1, 'label array must be 1D'\n",
    "        n_batches = int(len(y) / batch_size)\n",
    "        self.skf = StratifiedKFold(n_splits=n_batches, shuffle=shuffle)\n",
    "        self.X = torch.randn(len(y),1).numpy()\n",
    "        self.y = y\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.skf.random_state = torch.randint(0,int(1e8),size=()).item()\n",
    "        for train_idx, test_idx in self.skf.split(self.X, self.y):\n",
    "            yield test_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1c84514-66c9-4543-8448-cafa751730ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, ann_files, augs, img_size, dataset):\n",
    "\n",
    "        # Create a mapping from old labels to new labels\n",
    "        self.label_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted(class_labels))}\n",
    "\n",
    "        self.new_label_mapping = {\n",
    "            0: 2,  # Parade\n",
    "            1: 8,  # Business\n",
    "            2: 7,  # Law Enforcement\n",
    "            3: 14,  # Performance and Entertainment\n",
    "            4: 1,  # Celebration\n",
    "            5: 13,  # Cheering\n",
    "            6: 8,  # Business\n",
    "            7: 8,  # Business\n",
    "            8: 1,  # Celebration\n",
    "            9: 14,  # Performance and Entertainment\n",
    "            10: 15, # Family\n",
    "            11: 15, # Family\n",
    "            12: 11, # Picnic\n",
    "            13: 7, # Law Enforcement\n",
    "            14: 6, # Spa\n",
    "            15: 13, # Cheering\n",
    "            16: 5, # Surgeons\n",
    "            17: 3, # Waiter or Waitress\n",
    "            18: 4, # Individual Sports\n",
    "            19: 0, # Team Sports\n",
    "            20: 0, # Team Sports\n",
    "            21: 0, # Team Sports\n",
    "            22: 4, # Individual Sports\n",
    "            23: 10, # Water Activities\n",
    "            24: 4, # Individual Sports\n",
    "            25: 1, # Celebration\n",
    "            26: 9, # Dresses\n",
    "            27: 12, # Rescue\n",
    "            28: 10,# Water Activities\n",
    "            29: 0  # Team Sports\n",
    "        }\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.ann_files = ann_files\n",
    "        self.augment = self.augs_function(augs, img_size)\n",
    "        # Initialize transformations directly\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "            ] \n",
    "        )\n",
    "        if self.dataset == \"wider\":\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                ] \n",
    "            )        \n",
    "\n",
    "        self.anns = []\n",
    "        self.load_anns()\n",
    "        print(self.augment)\n",
    "\n",
    "    def augs_function(self, augs, img_size):            \n",
    "        t = []\n",
    "        if 'randomflip' in augs:\n",
    "            t.append(transforms.RandomHorizontalFlip())\n",
    "        if 'ColorJitter' in augs:\n",
    "            t.append(transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0))\n",
    "        if 'resizedcrop' in augs:\n",
    "            t.append(transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0)))\n",
    "        if 'RandAugment' in augs:\n",
    "            t.append(transforms.RandAugment())\n",
    "\n",
    "        t.append(transforms.Resize((img_size, img_size)))\n",
    "\n",
    "        return transforms.Compose(t)\n",
    "    \n",
    "    def load_anns(self):\n",
    "        self.anns = []\n",
    "        for ann_file in self.ann_files:\n",
    "            json_data = json.load(open(ann_file, \"r\"))\n",
    "            self.anns += json_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Make sure the index is within bounds\n",
    "        idx = idx % len(self)\n",
    "        ann = self.anns[idx]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to open the image file\n",
    "            img = Image.open(f'WIDER/Image/{ann[\"file_name\"]}').convert(\"RGB\")\n",
    "\n",
    "            # If this is the wider dataset, proceed with specific processing\n",
    "            # x, y, w, h = ann['bbox']\n",
    "            # img_area = img.crop([x, y, x+w, y+h])\n",
    "            img_area = self.augment(img)\n",
    "            img_area = self.transform(img_area)\n",
    "            attributes_list = [target['attribute'] for target in ann['targets']]\n",
    "            num_people = len(attributes_list)\n",
    "            attributes_distribution = [max(sum(attribute), 0)/num_people for attribute in zip(*attributes_list)]\n",
    "            # Extract label from image path\n",
    "            img_path = f'WIDER/Image/{ann[\"file_name\"]}'\n",
    "            label = self.extract_label(img_path)  # You might need to implement this method\n",
    "            \n",
    "            return {\n",
    "                \"label\": label,\n",
    "                \"target\": torch.tensor([attributes_distribution[0]], dtype=torch.float32),\n",
    "                \"img\": img_area\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If any error occurs during the processing of an image, log the error and the index\n",
    "            print(f\"Error processing image at index {idx}: {e}\")\n",
    "            # Instead of returning None, raise the exception\n",
    "            raise\n",
    "\n",
    "    def extract_label(self, img_path):\n",
    "        original_label = None\n",
    "    \n",
    "        if \"WIDER/Image/train\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/train/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/test\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/test/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/val\" in img_path:  # Handle validation images\n",
    "            label_str = img_path.split(\"WIDER/Image/val/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "    \n",
    "        if original_label is not None:\n",
    "            remapped_label = self.label_mapping[original_label]\n",
    "            new_label_mapping = self.new_label_mapping[remapped_label]\n",
    "            return new_label_mapping\n",
    "        else:\n",
    "            raise ValueError(f\"Label could not be extracted from path: {img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea2490c2-046a-4a19-9717-642adbabb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = ['data/wider/trainval_wider.json']\n",
    "test_file = ['data/wider/test_wider.json']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "012d6bd8-61d5-4a20-845b-689234718508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # Filter out any None items in the batch\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    # If after filtering the batch is empty, handle this case by either returning an empty tensor or raising an exception\n",
    "    if len(batch) == 0:\n",
    "        raise ValueError(\"Batch is empty after filtering out None items.\")\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fe2adb5-687d-4055-a108-a9c041836b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = DataSet(train_file, augs = ['RandAugment'], img_size = 226, dataset = 'wider')\n",
    "train_dataset = DataSet(train_file, augs = [], img_size = 226, dataset = 'wider')\n",
    "test_dataset = DataSet(test_file, augs = [], img_size = 226, dataset = 'wider')\n",
    "\n",
    "if stratified_sampling_flag:\n",
    "    trainloader = DataLoader(train_dataset, \n",
    "                             batch_sampler=StratifiedBatchSampler(torch.tensor([train_dataset[i]['label'] for i in range(len(train_dataset))]), \n",
    "                             batch_size=batch_size), num_workers=num_workers, collate_fn=custom_collate)\n",
    "    testloader = DataLoader(test_dataset, batch_sampler=StratifiedBatchSampler(torch.tensor([test_dataset[i]['label'] for i in range(len(test_dataset))]), \n",
    "                             batch_size=batch_size), num_workers=num_workers, collate_fn=custom_collate)\n",
    "else:\n",
    "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                             num_workers=num_workers, collate_fn=custom_collate)\n",
    "    testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cff90b9a-9e51-4fb9-a343-da49cdc713da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10324"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f155c74-dfca-45a3-baa6-42a26437d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_batch_class_counts(data_loader, label_mapping, num_batches=5):\n",
    "#     for i, batch in enumerate(data_loader):\n",
    "#         if i >= num_batches:\n",
    "#             break\n",
    "\n",
    "#         # Extract labels from the batch\n",
    "#         labels = batch['label']\n",
    "\n",
    "#         # Count occurrences of each class\n",
    "#         class_counts = torch.bincount(labels)\n",
    "\n",
    "#         # Map class counts to class names\n",
    "#         class_counts_with_names = {label_mapping.get(j, f\"Unknown Class {j}\"): class_counts[j].item() for j in range(len(class_counts))}\n",
    "\n",
    "#         # Print class counts and total observations\n",
    "#         print(f\"Batch {i + 1}:\")\n",
    "#         for class_name, count in class_counts_with_names.items():\n",
    "#             print(f\"    {class_name}: {count}\")\n",
    "#         print(f\"Total Observations: {len(labels)}\\n\")\n",
    "\n",
    "# print_batch_class_counts(trainloader, new_label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d18d4b-a3c0-4c5b-ad75-e74b6b1b7296",
   "metadata": {},
   "source": [
    "# Start Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c37c867-c1d1-438c-ac22-755904ce121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "def calculate_recall_multiclass(conf_matrix):\n",
    "    recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "    recalls[np.isnan(recalls)] = 0  # Replace NaN with 0\n",
    "    return recalls\n",
    "\n",
    "def evaluate_model_with_gender_multiclass(pred, label, gender, num_classes):\n",
    "    predictions = pred.cpu()\n",
    "    true_labels = label.cpu()\n",
    "    gender = gender.cpu()\n",
    "\n",
    "    # Identify male and female indices based on the gender threshold\n",
    "    male_indices = np.where(gender >= 0.5)[0]\n",
    "    female_indices = np.where(gender < 0.5)[0]\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    one_hot_labels = one_hot_encode(true_labels, num_classes=num_classes)\n",
    "    one_hot_preds = one_hot_encode(predictions, num_classes=num_classes)\n",
    "    # Initialize recall arrays\n",
    "    male_recall = np.zeros(num_classes)\n",
    "    female_recall = np.zeros(num_classes)\n",
    "\n",
    "    # Extract predictions and labels for male and female indices\n",
    "    male_predictions = np.argmax(one_hot_preds[male_indices], axis=1)\n",
    "    female_predictions = np.argmax(one_hot_preds[female_indices], axis=1)\n",
    "    male_labels = np.argmax(one_hot_labels[male_indices], axis=1)\n",
    "    female_labels = np.argmax(one_hot_labels[female_indices], axis=1)\n",
    "\n",
    "    # Check if the class labels are within the expected range\n",
    "    assert (0 <= male_predictions.min() < num_classes) and (0 <= male_predictions.max() < num_classes), \"Invalid class indices in male predictions\"\n",
    "    assert (0 <= female_predictions.min() < num_classes) and (0 <= female_predictions.max() < num_classes), \"Invalid class indices in female predictions\"\n",
    "    assert (0 <= male_labels.min() < num_classes) and (0 <= male_labels.max() < num_classes), \"Invalid class indices in male labels\"\n",
    "    assert (0 <= female_labels.min() < num_classes) and (0 <= female_labels.max() < num_classes), \"Invalid class indices in female labels\"\n",
    "\n",
    "    # Calculate confusion matrices for each gender\n",
    "    male_conf_matrix = confusion_matrix(male_labels, male_predictions, labels=np.arange(num_classes))\n",
    "    female_conf_matrix = confusion_matrix(female_labels, female_predictions, labels=np.arange(num_classes))\n",
    "\n",
    "    # Calculate recall for each class and gender\n",
    "    male_recall[:len(male_conf_matrix)] = calculate_recall_multiclass(male_conf_matrix)\n",
    "    female_recall[:len(female_conf_matrix)] = calculate_recall_multiclass(female_conf_matrix)\n",
    "\n",
    "    return male_recall - female_recall, male_conf_matrix, female_conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c6b93cb-37c9-448f-a8d9-424d3c4d40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "###################### Testing 1 ######################\n",
    "# Create instances of your models\n",
    "teacher_model = torchvision.models.efficientnet_b3(weights='DEFAULT')\n",
    "teacher_model.classifier = nn.Linear(1536, num_classes)\n",
    "student_model = torchvision.models.efficientnet_b0(weights='DEFAULT')\n",
    "student_model.classifier = nn.Linear(1280, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdcd643-ac9d-4e22-821a-4c784c86e4a5",
   "metadata": {},
   "source": [
    "This is the initialization of the 2-layer Adversary Perceptron. It is initialized with the number of classes*2, which represents the predicted labels (y_hat) and the true labels (y). The output of the final layer is a regression output, which is intended to predict the strength of gender (continuous number where anything past 0.5 is more male).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "836f2993-0505-47c6-85ee-2ad2f45db946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adversary(nn.Module):\n",
    "    def __init__(self, input_size=num_classes):\n",
    "        super(Adversary, self).__init__()\n",
    "\n",
    "        self.a1 = nn.Linear(input_size, 16)\n",
    "        self.a2 = nn.Linear(16, 1)  # Output size 1 for regression\n",
    "        nn.init.xavier_normal_(self.a1.weight)\n",
    "        nn.init.kaiming_normal_(self.a2.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        adversary = F.relu(self.a1(input_ids))\n",
    "        adversary_output = F.sigmoid(self.a2(adversary))  # Linear activation for regression\n",
    "        return adversary_output\n",
    "\n",
    "# Instantiate the Adversary\n",
    "adv = Adversary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a761874d-cc14-482e-9c8c-6342adea3cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_student(student, teacher, trainloader, criterion, optimizer, device, alpha, temperature, epochs_pretrain, patience=patience_student):\n",
    "    teacher.eval()\n",
    "    teacher.to(device)\n",
    "    best_val_loss = float('inf')  \n",
    "    patience_counter = 0 \n",
    "    student_epoch_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs_pretrain):\n",
    "        student.train()\n",
    "        student.to(device)\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student(inputs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "\n",
    "            ce_loss = criterion(student_outputs, labels)\n",
    "            kd_loss = tkd_kdloss(student_outputs, teacher_outputs, temperature=temperature)  # Make sure this returns a scalar\n",
    "            \n",
    "            # If not scalar, sum up to make sure the loss is scalar\n",
    "            if kd_loss.ndim != 0:\n",
    "                kd_loss = kd_loss.sum()\n",
    "                \n",
    "            # Now combine the losses\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        print(f'*******Epoch {epoch}: loss - {epoch_loss}')\n",
    "        student_epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16f4630d-f0d5-4d83-b61f-8707df8800a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_teacher(teacher, trainloader, criterion, optimizer, device, epochs_pretrain, patience=patience_student):\n",
    "    teacher.to(device)\n",
    "    teacher.train()  # Set the model to training mode\n",
    "    best_val_loss = float('inf')  \n",
    "    patience_counter = 0 \n",
    "    teacher_epoch_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs_pretrain):\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            teacher_outputs = teacher(inputs)\n",
    "\n",
    "            ce_loss = criterion(teacher_outputs, labels)\n",
    "                \n",
    "            loss = ce_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        print(f'*******Epoch {epoch}: loss - {epoch_loss}')\n",
    "        teacher_epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "005d66e3-f169-4d59-bc29-b5c0e05816b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_adversary(adv, student, adversary_optimizer, trainloader, adv_criterion, device, epochs_pretrain):\n",
    "\n",
    "  for epoch in range(epochs_pretrain):\n",
    "    epoch_loss = 0\n",
    "    epoch_batches = 0\n",
    "    for i, data in enumerate(tqdm(trainloader)): # starting from the 0th batch\n",
    "        # get the inputs and labels\n",
    "        adv.train()\n",
    "        adv.to(device)\n",
    "        inputs = data['img'].to(device)\n",
    "        labels = data['label'].to(device)\n",
    "        targets = data['target'].to(device)\n",
    "        student = student.to(device)\n",
    "        adversary_optimizer.zero_grad()\n",
    "        student_output = student(inputs)\n",
    "        adversary_output = adv(student_output)\n",
    "        adversary_loss = adv_criterion(adversary_output, targets) # compute loss\n",
    "        adversary_loss.backward() # back prop\n",
    "        adversary_optimizer.step()\n",
    "        epoch_loss += adversary_loss.item()\n",
    "        epoch_batches += 1\n",
    "\n",
    "    print(\"Average Pretrain Adversary epoch loss: \", epoch_loss/epoch_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7b4731c-f105-4192-9650-6587c46f6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler for the student model\n",
    "student_optimizer = optim.Adam(student_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Optimizer and scheduler for the teacher model\n",
    "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "teacher_scheduler = torch.optim.lr_scheduler.StepLR(teacher_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "optimizer_adv = optim.Adam(adv.parameters(), lr=learning_rate)\n",
    "\n",
    "# Instantiate the model and the loss function\n",
    "criterion_clf = nn.CrossEntropyLoss()\n",
    "adv_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3b7719f-4d65-4601-8e1d-1015de2201cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### finding the optimal learning rate\n",
    "# def train_teacher_optimal_lr(model, trainloader, criterion, optimizer, scheduler, device, epochs_optimal_lr=5, lr_range=(1e-4, 1e-1), plot_loss=True):\n",
    "#     model.train()\n",
    "#     model.to(device)\n",
    "#     lr_values = np.logspace(np.log10(lr_range[0]), np.log10(lr_range[1]), epochs_optimal_lr * len(trainloader))  # Generate learning rates for each batch\n",
    "#     lr_iter = iter(lr_values)\n",
    "#     losses = []\n",
    "#     lrs = []\n",
    "    \n",
    "#     for epoch in range(epochs_optimal_lr):\n",
    "#         for i, batch in enumerate(tqdm(trainloader)):\n",
    "#             lr = next(lr_iter)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = lr  # Set new learning rate\n",
    "            \n",
    "#             inputs, labels = batch['img'].to(device), batch['label'].to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             losses.append(loss.item())\n",
    "#             lrs.append(lr)\n",
    "    \n",
    "#     # Calculate the derivative of the loss\n",
    "#     loss_derivative = np.gradient(losses)\n",
    "    \n",
    "#     # Find the learning rate corresponding to the minimum derivative (steepest decline)\n",
    "#     best_lr_index = np.argmin(loss_derivative)\n",
    "#     best_lr = lrs[best_lr_index]\n",
    "    \n",
    "#     if plot_loss:\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.figure()\n",
    "#         plt.plot(lrs, losses)\n",
    "#         plt.xscale('log')\n",
    "#         plt.xlabel('Learning Rate')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title('Learning Rate Range Test - Teacher')\n",
    "#         plt.axvline(x=best_lr, color='red', linestyle='--', label=f'Best LR: {best_lr}')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "    \n",
    "#     print(f'Best Learning Rate Teacher: {best_lr}')\n",
    "#     return best_lr\n",
    "\n",
    "# ############# input ############## \n",
    "# best_lr_teacher = train_teacher_optimal_lr(teacher_model, trainloader, criterion_clf, teacher_optimizer, teacher_scheduler, device, epochs_optimal_lr)  \n",
    "# print(best_lr_teacher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fc2fd75-c5a3-415d-8da7-32816addb845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### finding the optimal learning rate\n",
    "# def train_student_optimal_lr(model, trainloader, criterion, optimizer, device, epochs_optimal_lr=5, lr_range=(1e-4, 1e-1), plot_loss=True):\n",
    "#     model.train()\n",
    "#     model.to(device)\n",
    "#     lr_values = np.logspace(np.log10(lr_range[0]), np.log10(lr_range[1]), epochs_optimal_lr * len(trainloader))  # Generate learning rates for each batch\n",
    "#     lr_iter = iter(lr_values)\n",
    "#     losses = []\n",
    "#     lrs = []\n",
    "    \n",
    "#     for epoch in range(epochs_optimal_lr):\n",
    "#         for i, batch in enumerate(tqdm(trainloader)):\n",
    "#             lr = next(lr_iter)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = lr  # Set new learning rate\n",
    "            \n",
    "#             inputs, labels = batch['img'].to(device), batch['label'].to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             losses.append(loss.item())\n",
    "#             lrs.append(lr)\n",
    "    \n",
    "#     # Calculate the derivative of the loss\n",
    "#     loss_derivative = np.gradient(losses)\n",
    "    \n",
    "#     # Find the learning rate corresponding to the minimum derivative (steepest decline)\n",
    "#     best_lr_index = np.argmin(loss_derivative)\n",
    "#     best_lr = lrs[best_lr_index]\n",
    "    \n",
    "#     if plot_loss:\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.figure()\n",
    "#         plt.plot(lrs, losses)\n",
    "#         plt.xscale('log')\n",
    "#         plt.xlabel('Learning Rate')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title('Learning Rate Range Test - Student')\n",
    "#         plt.axvline(x=best_lr, color='red', linestyle='--', label=f'Best LR: {best_lr}')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "    \n",
    "#     print(f'Best Learning Rate Student: {best_lr}')\n",
    "#     return best_lr\n",
    "\n",
    "# ############# input ############## \n",
    "# best_lr_student = train_student_optimal_lr(student_model, trainloader, criterion_clf, student_optimizer, device, epochs_optimal_lr)  \n",
    "# print(best_lr_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2037b64-dba1-4c4b-a586-0a0ac3373d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lr_student = learning_rate\n",
    "best_lr_teacher = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "411c4f7a-d0b4-478f-9fa5-5e152e08ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(losses):\n",
    "    epochs = range(1, len(losses) + 1)\n",
    "    plt.plot(epochs, losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Val Loss Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "286386b2-d6e9-447e-95b5-b3dd8f18010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_teacher(model, adv, trainloader, criterion, adv_criterion, optimizer, optimizer_adv, device, \n",
    "                  epochs, lmda, patience=patience_teacher):\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    epoch_losses = [] \n",
    "    val_losses = []\n",
    "    val_disparities = []\n",
    "    val_accuracies = []\n",
    "    best_total_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        adv.train()\n",
    "        model.to(device)\n",
    "        adv.to(device)\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            targets = data['target'].to(device)\n",
    "        \n",
    "            # Forward pass for teacher model\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            classification_loss = criterion(outputs, labels)\n",
    "        \n",
    "            # Forward pass for adversary model\n",
    "            optimizer_adv.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs_detached = outputs.detach()\n",
    "            adversary_output = adv(teacher_outputs_detached)\n",
    "            adversary_loss = adv_criterion(adversary_output, targets)\n",
    "        \n",
    "            # Calculate the total loss by combining classification and adversary loss\n",
    "            if lmda != 0:\n",
    "                total_loss = classification_loss + classification_loss/adversary_loss - lmda * adversary_loss\n",
    "            else:\n",
    "                total_loss = classification_loss\n",
    "                \n",
    "            total_loss.backward()\n",
    "        \n",
    "            optimizer.step()\n",
    "            optimizer_adv.step()\n",
    "        \n",
    "            running_loss += total_loss.item()\n",
    "            epoch_loss += total_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        epoch_loss /= num_batches  \n",
    "        epoch_losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_val_loss = 0.0\n",
    "        num_batches = 0\n",
    "        confusion_male = np.zeros((num_classes, num_classes))\n",
    "        confusion_female = np.zeros((num_classes, num_classes))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_data in tqdm(testloader):\n",
    "                val_inputs = val_data['img'].to(device)\n",
    "                val_labels = val_data['label'].to(device)\n",
    "                val_targets = val_data['target'].to(device)\n",
    "                \n",
    "                val_outputs = model(val_inputs)\n",
    "                with torch.no_grad():\n",
    "                    teacher_outputs_detached_val = val_outputs.detach()                \n",
    "                adversary_output_val = adv(teacher_outputs_detached_val)\n",
    "                adversary_loss_val = adv_criterion(adversary_output_val, val_targets)\n",
    "                \n",
    "                # Compute validation loss\n",
    "                val_ce_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                if lmda !=0:\n",
    "                    val_loss = val_ce_loss + val_ce_loss/adversary_loss_val - lmda * adversary_loss_val\n",
    "                else:\n",
    "                    val_loss = val_ce_loss\n",
    "                    \n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                # Compute the validation accuracy\n",
    "                _, predicted = torch.max(val_outputs, 1)\n",
    "                total_samples += val_labels.size(0)\n",
    "                total_correct += (predicted == val_labels).sum().item()\n",
    "                num_batches += 1\n",
    "\n",
    "                # Compute recall differences for gender\n",
    "                recall_diff = evaluate_model_with_gender_multiclass(predicted, val_labels, val_targets, num_classes=num_classes)\n",
    "                confusion_male += recall_diff[1]\n",
    "                confusion_female += recall_diff[2]\n",
    "\n",
    "            total_val_loss /= num_batches\n",
    "            confusion_male /= num_batches\n",
    "            confusion_female /= num_batches\n",
    "            \n",
    "            epoch_disparity = calculate_recall_multiclass(confusion_male) - calculate_recall_multiclass(confusion_female)\n",
    "            val_losses.append(total_val_loss)\n",
    "            non_zero_abs_values = np.abs(epoch_disparity[epoch_disparity != 0])\n",
    "            mean_non_zero_abs_disparity = np.mean(non_zero_abs_values)\n",
    "            val_disparities.append(mean_non_zero_abs_disparity)\n",
    "            accuracy = total_correct / total_samples\n",
    "            val_accuracies.append(accuracy)\n",
    "            class_recall_mapping = {class_name: epoch_disparity[int(class_label)] for class_label, class_name in class_idx.items()}\n",
    "\n",
    "            print(f'*****Epoch {epoch + 1}/{epochs}*****\\n' \n",
    "            f'*****Train Loss: {epoch_loss: .6f} Val Loss: {total_val_loss: .6f}*****\\n'\n",
    "            f'*****Validation Accuracy: {accuracy * 100:.2f}%*****\\n'\n",
    "            f'*****Total Avg Disparity: {mean_non_zero_abs_disparity}*****\\n')\n",
    "            \n",
    "            # Print disparities by class label\n",
    "            for class_label, recall_diff in class_recall_mapping.items():\n",
    "                print(f\"Class {class_label}: Recall Difference = {recall_diff}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if abs(total_val_loss) < abs(best_total_val_loss):\n",
    "            best_total_val_loss = total_val_loss\n",
    "            patience_counter = 0 \n",
    "            best_epoch_mean_abs_disparity = mean_non_zero_abs_disparity\n",
    "\n",
    "            # Save the teacher model and its state to the 'output_dir'\n",
    "            teacher_model_weights_path = os.path.join(output_dir, f'teacher_model_ckd_wider_lambda{i}.pth')\n",
    "            torch.save(teacher_model.state_dict(), teacher_model_weights_path)\n",
    "            \n",
    "            teacher_model_path = os.path.join(output_dir, f'teacher_model_ckd_wider_checkpoint{i}.pth')\n",
    "            torch.save(teacher_model, teacher_model_path)\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break  \n",
    "\n",
    "        # Writing epoch data to file\n",
    "        file_path = os.path.join(output_dir, f'teacher_validation_{lmda}.txt')\n",
    "        with open(file_path, 'a') as file:\n",
    "            file.write(f'*****Epoch {epoch + 1}/{epochs}*****\\n')\n",
    "            file.write(f'*****Train Loss: {epoch_loss:.6f} Val Loss: {total_val_loss:.6f}*****\\n')\n",
    "            file.write(f'*****Validation Accuracy: {accuracy * 100:.2f}%*****\\n')\n",
    "            file.write(f'*****Total Avg Disparity: {mean_non_zero_abs_disparity}*****\\n')\n",
    "    \n",
    "            # Writing disparities by class label\n",
    "            for class_label, recall_diff in class_recall_mapping.items():\n",
    "                file.write(f\"Class {class_label}: Recall Difference = {recall_diff}\\n\")\n",
    "\n",
    "        print(f\"Data has been appended to {file_path}\")\n",
    "        \n",
    "    plot_loss_curve(val_losses)\n",
    "    print(f\"Finished Training Teacher for lambda val of {lmda}\")\n",
    "    return val_disparities\n",
    "\n",
    "\n",
    "\n",
    "def train_student_with_distillation_disparity(student, teacher, adv, trainloader, testloader, criterion, adv_criterion, optimizers, optimizera, \n",
    "                                              device, alpha, temperature, epochs, lmda, patience=patience_student):\n",
    "    teacher.eval()\n",
    "    teacher.to(device)\n",
    "    best_val_accuracy = 0\n",
    "    best_total_val_loss = float('inf')\n",
    "    best_epoch_accuracy = 0.0\n",
    "    best_epoch_disparity = 0.0\n",
    "    patience_counter = 0 \n",
    "    student_epoch_losses = []\n",
    "    val_losses = []\n",
    "    val_disparities = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        student.to(device)\n",
    "        if lmda != 0:\n",
    "            adv.train()\n",
    "            adv.to(device)\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0 \n",
    "        confusion_male = np.zeros((num_classes, num_classes))\n",
    "        confusion_female = np.zeros((num_classes, num_classes))\n",
    "\n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            targets = data['target'].to(device)\n",
    "            optimizers.zero_grad()\n",
    "            student_outputs = student(inputs)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "\n",
    "            # Run the adversarial model on concatenated true labels, and predicted labels\n",
    "            if lmda != 0:\n",
    "                optimizera.zero_grad()\n",
    "                studentached = student_outputs.detach()\n",
    "                with torch.no_grad():\n",
    "                    adversary_output = adv(studentached)\n",
    "                adversary_loss = adv_criterion(adversary_output, targets)\n",
    "                \n",
    "            ce_loss = criterion(student_outputs, labels)\n",
    "            kd_loss = tkd_kdloss(student_outputs, teacher_outputs, temperature=temperature)  # Make sure this returns a scalar\n",
    "            \n",
    "            if kd_loss.ndim != 0:\n",
    "                kd_loss = kd_loss.sum()\n",
    "\n",
    "            # Now combine the losses, subtract weighted adversary loss because we need to maximize that loss \n",
    "            # goal of the model is to have the adversary not predict gender. \n",
    "            if lmda != 0:\n",
    "                loss = (alpha * kd_loss + (1 - alpha) * ce_loss) + (alpha * kd_loss + (1 - alpha) * ce_loss)/adversary_loss - lmda * adversary_loss\n",
    "            else:\n",
    "                loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "                \n",
    "            loss.backward()\n",
    "\n",
    "            optimizers.step()\n",
    "            if lmda != 0:\n",
    "                optimizera.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        epoch_loss /= num_batches\n",
    "        student_epoch_losses.append(epoch_loss)\n",
    "\n",
    "        student.eval()\n",
    "        adv.eval()\n",
    "        adv.to(device)\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_val_loss = 0.0\n",
    "        num_batches = 0\n",
    "        # Validation after each epoch\n",
    "        with torch.no_grad():\n",
    "            for val_data in tqdm(testloader):\n",
    "                val_inputs = val_data['img'].to(device)\n",
    "                val_labels = val_data['label'].to(device)\n",
    "                val_targets = val_data['target'].to(device)\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                val_student_outputs = student(val_inputs)\n",
    "                val_teacher_outputs = teacher(val_inputs)\n",
    "                if lmda != 0:\n",
    "                    val_studentached = val_student_outputs.detach()   \n",
    "\n",
    "                    val_adversary_output = adv(val_studentached)\n",
    "                    val_adversary_loss = adv_criterion(val_adversary_output, val_targets)\n",
    "                    \n",
    "                val_ce_loss = criterion(val_student_outputs, val_labels)\n",
    "                val_kd_loss = tkd_kdloss(val_student_outputs, val_teacher_outputs, temperature=temperature)  # Make sure this returns a scalar\n",
    "                \n",
    "                if val_kd_loss.ndim != 0:\n",
    "                    val_kd_loss = val_kd_loss.sum()\n",
    "                if lmda != 0:\n",
    "                    val_loss = (alpha * val_kd_loss + (1 - alpha) * val_ce_loss) + (alpha * val_kd_loss + (1 - alpha) * val_ce_loss)/val_adversary_loss - lmda * val_adversary_loss\n",
    "                else:\n",
    "                    val_loss = alpha * val_kd_loss + (1 - alpha) * val_ce_loss\n",
    "                    \n",
    "                total_val_loss += val_loss.item()\n",
    "    \n",
    "                # Compute the validation accuracy\n",
    "                _, predicted = torch.max(val_student_outputs, 1)\n",
    "                total_samples += val_labels.size(0)\n",
    "                total_correct += (predicted == val_labels).sum().item()\n",
    "                num_batches += 1\n",
    "                recall_diff = evaluate_model_with_gender_multiclass(predicted, val_labels, val_targets, num_classes=num_classes)\n",
    "                confusion_male += recall_diff[1]\n",
    "                confusion_female += recall_diff[2]\n",
    "    \n",
    "            total_val_loss /= num_batches\n",
    "            confusion_male /= num_batches\n",
    "            confusion_female /= num_batches\n",
    "\n",
    "            epoch_disparity = calculate_recall_multiclass(confusion_male) - calculate_recall_multiclass(confusion_female)\n",
    "            val_losses.append(total_val_loss)\n",
    "            non_zero_abs_values = np.abs(epoch_disparity[epoch_disparity != 0])\n",
    "            mean_non_zero_abs_disparity = np.mean(non_zero_abs_values)\n",
    "            val_disparities.append(mean_non_zero_abs_disparity)\n",
    "            accuracy = total_correct / total_samples\n",
    "            val_accuracies.append(accuracy)\n",
    "            class_recall_mapping = {class_name: epoch_disparity[int(class_label)] for class_label, class_name in class_idx.items()}\n",
    "            print(f'*****Epoch {epoch + 1}/{epochs}*****\\n' \n",
    "            f'*****Train Loss: {epoch_loss: .6f} Val Loss: {total_val_loss: .6f}*****\\n'\n",
    "            f'*****Validation Accuracy: {accuracy * 100:.2f}%*****\\n'\n",
    "            f'*****Total Avg Disparity: {mean_non_zero_abs_disparity}*****\\n')\n",
    "        \n",
    "            # Print disparities by class label\n",
    "            for class_label, recall_diff in class_recall_mapping.items():\n",
    "                print(f\"Class {class_label}: Recall Difference = {recall_diff}\")\n",
    "\n",
    "        # Check for early stopping\n",
    "        if abs(total_val_loss) < abs(best_total_val_loss):\n",
    "            best_total_val_loss = total_val_loss\n",
    "            patience_counter = 0\n",
    "            best_epoch_mean_abs_disparity = mean_non_zero_abs_disparity\n",
    "            state_dict_path = os.path.join(output_dir, f'student_model_weights_ckd_wider_checkpoint_lambda{lmda}.pth')\n",
    "            torch.save(student.state_dict(), state_dict_path)\n",
    "            model_path = os.path.join(output_dir, f'student_model_ckd_wider_checkpoint_lambda{lmda}.pth')\n",
    "            torch.save(student, model_path)\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break  \n",
    "\n",
    "        # Writing epoch data to file\n",
    "        file_path = os.path.join(output_dir, f'student_validation_{lmda}.txt')\n",
    "        with open(file_path, 'a') as file:\n",
    "            file.write(f'*****Epoch {epoch + 1}/{epochs}*****\\n')\n",
    "            file.write(f'*****Train Loss: {epoch_loss:.6f} Val Loss: {total_val_loss:.6f}*****\\n')\n",
    "            file.write(f'*****Validation Accuracy: {accuracy * 100:.2f}%*****\\n')\n",
    "            file.write(f'*****Total Avg Disparity: {mean_non_zero_abs_disparity}*****\\n')\n",
    "    \n",
    "            # Writing disparities by class label\n",
    "            for class_label, recall_diff in class_recall_mapping.items():\n",
    "                file.write(f\"Class {class_label}: Recall Difference = {recall_diff}\\n\")\n",
    "        print(f\"Data has been appended to {file_path}\")\n",
    "\n",
    "    plot_loss_curve(val_losses)\n",
    "    print(f'Finished Training Student for lambda value of {lmda}')\n",
    "    return val_disparities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17a06902-9ad4-47a6-9a67-181e2dc43218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 162/162 [01:50<00:00,  1.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 55/55 [00:19<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Epoch 1/1*****\n",
      "*****Train Loss:  1.555877 Val Loss:  1.181927*****\n",
      "*****Validation Accuracy: 64.50%*****\n",
      "*****Total Avg Disparity: 0.10165457036611443*****\n",
      "\n",
      "Class Team_Sports: Recall Difference = 0.07268282094343714\n",
      "Class Celebration: Recall Difference = -0.010544554455445554\n",
      "Class Parade: Recall Difference = 0.02367424242424243\n",
      "Class Waiter_Or_Waitress: Recall Difference = -0.03436426116838498\n",
      "Class Individual_Sports: Recall Difference = -0.12262016229712869\n",
      "Class Surgeons: Recall Difference = 0.01552106430155209\n",
      "Class Spa: Recall Difference = -0.16666666666666663\n",
      "Class Law_Enforcement: Recall Difference = 0.20352112676056333\n",
      "Class Business: Recall Difference = 0.008623442989460273\n",
      "Class Dresses: Recall Difference = -0.41516966067864275\n",
      "Class Water_Activities: Recall Difference = 0.044602938045561036\n",
      "Class Picnic: Recall Difference = -0.2875816993464052\n",
      "Class Rescue: Recall Difference = 0.01443850267379676\n",
      "Class Cheering: Recall Difference = 0.046235679214402636\n",
      "Class Performance_And_Entertainment: Recall Difference = 0.09340423707007461\n",
      "Class Family: Recall Difference = -0.06682206682206687\n",
      "Data has been appended to Smaller_Student_Models_11/teacher_validation_0.txt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Teacher for lambda val of 0\n",
      "Teacher weights and architecture saved and exported for lambda: 0\n"
     ]
    }
   ],
   "source": [
    "# Initialize the dictionary for results\n",
    "lambda_results = {}\n",
    "\n",
    "# Loop for training the teacher model with different lambda values\n",
    "for i in lmda_list_teacher:\n",
    "    # Reset the teacher model for each lambda\n",
    "    teacher_model = torchvision.models.efficientnet_b3(weights='DEFAULT')    \n",
    "    # Replace the last fully connected layer with a new one\n",
    "    teacher_model.classifier = nn.Linear(1536, num_classes)\n",
    "    teacher_optimizer = optim.Adam(teacher_model.parameters(), lr=best_lr_teacher)\n",
    "    \n",
    "    # Initialize the adversary for the teacher\n",
    "    adv = Adversary()\n",
    "    teacher_optimizer_adv = optim.Adam(adv.parameters(), lr=best_lr_teacher)\n",
    "\n",
    "    # pretrain_teacher(teacher_model, trainloader, criterion_clf, teacher_optimizer, device, epochs_pretrain)\n",
    "    # pretrain_adversary(adv, student_model, optimizer_adv, trainloader, adv_criterion, device, epochs_pretrain)\n",
    "    \n",
    "    # Train the teacher model with adversarial training\n",
    "    teacher_mean_abs_val_disparity = train_teacher(teacher_model, adv, trainloader, criterion_clf, adv_criterion, teacher_optimizer, teacher_optimizer_adv, device, epochs, i, patience=patience_teacher)\n",
    "\n",
    "    # Save the teacher model and its state to the 'output_dir'\n",
    "    teacher_model_weights_path = os.path.join(output_dir, f'teacher_model_weights_ckd_wider_lambda{i}.pth')\n",
    "    torch.save(teacher_model.state_dict(), teacher_model_weights_path)\n",
    "    teacher_model_path = os.path.join(output_dir, f'teacher_model_ckd_wider_lambda{i}.pth')\n",
    "    torch.save(teacher_model, teacher_model_path)\n",
    "    print('Teacher weights and architecture saved and exported for lambda:', i)\n",
    "\n",
    "    # Store the teacher results in the dictionary\n",
    "    lambda_results[i] = {\n",
    "        'teacher_mean_abs_val_disparity': teacher_mean_abs_val_disparity\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f00ed527-6705-4f12-b7af-2c5d0a1e753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 162/162 [00:56<00:00,  2.87it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 55/55 [00:18<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Epoch 1/1*****\n",
      "*****Train Loss:  0.387573 Val Loss:  0.352433*****\n",
      "*****Validation Accuracy: 22.60%*****\n",
      "*****Total Avg Disparity: 0.05552857940179361*****\n",
      "\n",
      "Class Team_Sports: Recall Difference = -0.029520969713587086\n",
      "Class Celebration: Recall Difference = 0.0\n",
      "Class Parade: Recall Difference = 0.10321969696969699\n",
      "Class Waiter_Or_Waitress: Recall Difference = 0.0\n",
      "Class Individual_Sports: Recall Difference = 0.07361891385767788\n",
      "Class Surgeons: Recall Difference = 0.0\n",
      "Class Spa: Recall Difference = 0.0\n",
      "Class Law_Enforcement: Recall Difference = 0.0\n",
      "Class Business: Recall Difference = 0.0\n",
      "Class Dresses: Recall Difference = 0.0\n",
      "Class Water_Activities: Recall Difference = -0.01575473706621247\n",
      "Class Picnic: Recall Difference = 0.0\n",
      "Class Rescue: Recall Difference = 0.0\n",
      "Class Cheering: Recall Difference = 0.0\n",
      "Class Performance_And_Entertainment: Recall Difference = 0.0\n",
      "Class Family: Recall Difference = 0.0\n",
      "Data has been appended to Smaller_Student_Models_11/student_validation_0.txt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAHHCAYAAABEEKc/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/MklEQVR4nO3de1RVdeL//xcH5YDIRSBAjETREbWUFCHKj5cRA7Ox1AonUuPTaOUtpWx0nLwuw5lmHMrr1HRxUNMss1JHx06ZNZL0xcguSKWZooKaIwjWMTn794e/Tp+zAUUCD+jzsdZeq/Pe7/2+7FPDa/Z+7308DMMwBAAAACeLuwcAAADQ2BCQAAAATAhIAAAAJgQkAAAAEwISAACACQEJAADAhIAEAABgQkACAAAwISABAACYEJAAuMWBAwfk4eGhl156yd1DAYAqCEgALmrIkCFq0aKFTp8+XWOdtLQ0eXl56bvvvqvXvrdv3y4PDw+9+uqr9dpuQ9m3b58efPBBtW/fXt7e3vL399ctt9yip59+Wt9//727hweglghIAC4qLS1N33//vV5//fVq9585c0ZvvPGGUlJSFBwcfJlH13hs2rRJN9xwg1555RX95je/0aJFi5SZmanrrrtOU6dO1SOPPOLuIQKopWbuHgCAxm/IkCHy8/PT6tWrNWrUqCr733jjDVVUVCgtLc0No2scvvnmG40YMUJt27bVO++8o9atWzv3jR8/Xl9//bU2bdpUL31VVFTI19e3XtoCUD2uIAG4KB8fHw0bNkw2m03Hjh2rsn/16tXy8/PTkCFDdPLkST322GO64YYb1LJlS/n7+2vQoEH65JNPGnSM+/fv1913362goCC1aNFCN910U7WBZNGiReratatatGihVq1aKS4uTqtXr3buP336tCZPnqyoqChZrVaFhoZq4MCB2r179wX7//Of/6zy8nI9//zzLuHoJx06dHBeQbrQ+isPDw/Nnj3b+Xn27Nny8PDQF198oXvvvVetWrVS79699Ze//EUeHh769ttvq7Qxffp0eXl56b///a+zbNeuXUpJSVFAQIBatGihvn376j//+c8F5wRczQhIAGolLS1N586d0yuvvOJSfvLkSW3dulVDhw6Vj4+P9u/frw0bNuj222/XwoULNXXqVH366afq27evjhw50iBjKykp0c0336ytW7dq3Lhxmj9/vn744QcNGTLE5bbgc889p0mTJqlLly7KysrSnDlzFBsbq127djnrPPTQQ1q2bJmGDx+upUuX6rHHHpOPj48KCgouOIa33npL7du3180339wgc7z77rt15swZPfnkkxozZozuueceeXh4VPk+JOmVV17RrbfeqlatWkmS3nnnHfXp00dlZWWaNWuWnnzySZ06dUq//vWvlZub2yDjBZo8AwBq4dy5c0br1q2NxMREl/Lly5cbkoytW7cahmEYP/zwg1FZWelS55tvvjGsVqsxd+5clzJJxosvvnjBft99911DkrFu3boa60yePNmQZLz//vvOstOnTxvt2rUzoqKinOO54447jK5du16wv4CAAGP8+PEXrGNWWlpqSDLuuOOOWtW/0NwlGbNmzXJ+njVrliHJ+O1vf1ulbmJiotGzZ0+XstzcXEOS8c9//tMwDMNwOBxGx44djeTkZMPhcDjrnTlzxmjXrp0xcODAWo0ZuNpwBQlArXh6emrEiBHKycnRgQMHnOWrV69WWFiYBgwYIEmyWq2yWM7/T0tlZaW+++47tWzZUp06dbrobaq62rx5s+Lj49W7d29nWcuWLTV27FgdOHBAX3zxhSQpMDBQRUVF+uijj2psKzAwULt27bqkq11lZWWSJD8/vzrO4OIeeuihKmWpqanKy8vTvn37nGVr166V1WrVHXfcIUnKz8/XV199pXvvvVffffedTpw4oRMnTqiiokIDBgzQjh075HA4GmzcQFNFQAJQaz8twv5pzU5RUZHef/99jRgxQp6enpIkh8Ohv/3tb+rYsaOsVqtCQkJ0zTXXaM+ePSotLW2QcX377bfq1KlTlfLOnTs790vS73//e7Vs2VLx8fHq2LGjxo8fX2Udzp///Gd99tlnioyMVHx8vGbPnq39+/dfsH9/f39JuuBrEH6pdu3aVSm7++67ZbFYtHbtWkmSYRhat26dBg0a5BzTV199JUkaPXq0rrnmGpftH//4h+x2e4N9L0BTRkACUGs9e/ZUTEyMXn75ZUnSyy+/LMMwXJ5ee/LJJ5WRkaE+ffpo5cqV2rp1q7Zt26auXbu6/UpF586dVVhYqDVr1qh379567bXX1Lt3b82aNctZ55577tH+/fu1aNEiRURE6KmnnlLXrl31r3/9q8Z2/f39FRERoc8++6xW4/Dw8Ki2vLKyssZjfHx8qpRFRETof/7nf5zrkD788EMdPHhQqampzjo/nfOnnnpK27Ztq3Zr2bJlrcYNXE14zB/AJUlLS9MTTzyhPXv2aPXq1erYsaN69erl3P/qq6+qf//+ev75512OO3XqlEJCQhpkTG3btlVhYWGV8r179zr3/8TX11epqalKTU3V2bNnNWzYMM2fP1/Tp0+Xt7e3JKl169YaN26cxo0bp2PHjqlHjx6aP3++Bg0aVOMYbr/9dj377LPKyclRYmLiBcf70+LpU6dOuZRX90TaxaSmpmrcuHEqLCzU2rVr1aJFC/3mN79x7o+OjpZ0PsQlJSVdcvvA1YorSAAuyU9Xi2bOnKn8/Pwq7z7y9PSUYRguZevWrdPhw4cbbEy33XabcnNzlZOT4yyrqKjQs88+q6ioKHXp0kWSqrzl28vLS126dJFhGPrxxx9VWVlZ5XZTaGioIiIiZLfbLziGxx9/XL6+vvrd736nkpKSKvv37dunp59+WtL5sBISEqIdO3a41Fm6dGntJ/3/Gz58uDw9PfXyyy9r3bp1uv32213ekdSzZ09FR0frL3/5i8rLy6scf/z48UvuE7gacAUJwCVp166dbr75Zr3xxhuSVCUg3X777Zo7d67S09N1880369NPP9WqVavUvn37X9Tva6+95rwi9H+NHj1a06ZN08svv6xBgwZp0qRJCgoK0ooVK/TNN9/otddecy4av/XWWxUeHq5bbrlFYWFhKigo0OLFizV48GD5+fnp1KlTuvbaa3XXXXepe/fuatmypd5++2199NFH+utf/3rB8UVHR2v16tVKTU1V586dNWrUKF1//fU6e/asdu7cqXXr1un+++931v/d736nBQsW6He/+53i4uK0Y8cOffnll5d8XkJDQ9W/f38tXLhQp0+fdrm9JkkWi0X/+Mc/NGjQIHXt2lXp6elq06aNDh8+rHfffVf+/v566623Lrlf4Irn3ofoADRFS5YsMSQZ8fHxVfb98MMPxqOPPmq0bt3a8PHxMW655RYjJyfH6Nu3r9G3b19nvUt9zL+m7adH+/ft22fcddddRmBgoOHt7W3Ex8cbGzdudGnr73//u9GnTx8jODjYsFqtRnR0tDF16lSjtLTUMAzDsNvtxtSpU43u3bsbfn5+hq+vr9G9e3dj6dKltT43X375pTFmzBgjKirK8PLyMvz8/IxbbrnFWLRokfHDDz846505c8Z44IEHjICAAMPPz8+45557jGPHjtX4mP/x48dr7PO5554zJBl+fn7G999/X22djz/+2Bg2bJhz7m3btjXuuecew2az1XpuwNXEwzBM18IBAACucqxBAgAAMCEgAQAAmBCQAAAATAhIAAAAJgQkAAAAEwISAACACS+KrCOHw6EjR47Iz8+vxt9VAgAAjYthGDp9+rQiIiKcL5GtDgGpjo4cOaLIyEh3DwMAANTBoUOHdO2119a4n4BUR35+fpLOn2B/f383jwYAANRGWVmZIiMjnX/Ha0JAqqOfbqv5+/sTkAAAaGIutjyGRdoAAAAmBCQAAAATAhIAAIAJAQkAAMCEgAQAAGBCQAIAADAhIAEAAJgQkAAAAEwISAAAACYEJAAAAJNGEZCWLFmiqKgoeXt7KyEhQbm5uTXWXb9+veLi4hQYGChfX1/FxsYqOzvbpY6Hh0e121NPPeWsc/LkSaWlpcnf31+BgYF64IEHVF5e3mBzBAAATYfbA9LatWuVkZGhWbNmaffu3erevbuSk5N17NixausHBQVpxowZysnJ0Z49e5Senq709HRt3brVWefo0aMu2wsvvCAPDw8NHz7cWSctLU2ff/65tm3bpo0bN2rHjh0aO3Zsg88XAAA0fh6GYRjuHEBCQoJ69eqlxYsXS5IcDociIyM1ceJETZs2rVZt9OjRQ4MHD9a8efOq3X/nnXfq9OnTstlskqSCggJ16dJFH330keLi4iRJW7Zs0W233aaioiJFRERctM+ysjIFBASotLSUH6sFAKCJqO3fb7deQTp79qzy8vKUlJTkLLNYLEpKSlJOTs5FjzcMQzabTYWFherTp0+1dUpKSrRp0yY98MADzrKcnBwFBgY6w5EkJSUlyWKxaNeuXdW2Y7fbVVZW5rIBAIArk1sD0okTJ1RZWamwsDCX8rCwMBUXF9d4XGlpqVq2bCkvLy8NHjxYixYt0sCBA6utu2LFCvn5+WnYsGHOsuLiYoWGhrrUa9asmYKCgmrsNzMzUwEBAc4tMjKyttMEAABNjNvXINWFn5+f8vPz9dFHH2n+/PnKyMjQ9u3bq637wgsvKC0tTd7e3r+oz+nTp6u0tNS5HTp06Be1BwAAGq9m7uw8JCREnp6eKikpcSkvKSlReHh4jcdZLBZ16NBBkhQbG6uCggJlZmaqX79+LvXef/99FRYWau3atS7l4eHhVRaBnzt3TidPnqyxX6vVKqvVWtupAQCAJsytV5C8vLzUs2dP5+Jp6fwibZvNpsTExFq343A4ZLfbq5Q///zz6tmzp7p37+5SnpiYqFOnTikvL89Z9s4778jhcCghIaEOMwEAAFcSt15BkqSMjAyNHj1acXFxio+PV1ZWlioqKpSeni5JGjVqlNq0aaPMzExJ59cCxcXFKTo6Wna7XZs3b1Z2draWLVvm0m5ZWZnWrVunv/71r1X67Ny5s1JSUjRmzBgtX75cP/74oyZMmKARI0bU6gk2AABwZXN7QEpNTdXx48c1c+ZMFRcXKzY2Vlu2bHEu3D548KAslp8vdFVUVGjcuHEqKiqSj4+PYmJitHLlSqWmprq0u2bNGhmGod/+9rfV9rtq1SpNmDBBAwYMkMVi0fDhw/XMM8803EQBAECT4fb3IDVVvAcJAICmp0m8BwkAAKAxIiABAACYEJAAAABMCEgAAAAmBCQAAAATAhIAAIAJAQkAAMCEgAQAAGBCQAIAADAhIAEAAJgQkAAAAEwISAAAACYEJAAAABMCEgAAgAkBCQAAwISABAAAYEJAAgAAMCEgAQAAmBCQAAAATAhIAAAAJgQkAAAAEwISAACACQEJAADAhIAEAABgQkACAAAwISABAACYEJAAAABMCEgAAAAmBCQAAAATAhIAAIAJAQkAAMCEgAQAAGBCQAIAADAhIAEAAJgQkAAAAEwISAAAACYEJAAAABO3B6QlS5YoKipK3t7eSkhIUG5ubo11169fr7i4OAUGBsrX11exsbHKzs6uUq+goEBDhgxRQECAfH191atXLx08eNC5v1+/fvLw8HDZHnrooQaZHwAAaHqaubPztWvXKiMjQ8uXL1dCQoKysrKUnJyswsJChYaGVqkfFBSkGTNmKCYmRl5eXtq4caPS09MVGhqq5ORkSdK+ffvUu3dvPfDAA5ozZ478/f31+eefy9vb26WtMWPGaO7cuc7PLVq0aNjJAgCAJsPDMAzDXZ0nJCSoV69eWrx4sSTJ4XAoMjJSEydO1LRp02rVRo8ePTR48GDNmzdPkjRixAg1b9682itLP+nXr59iY2OVlZVV57GXlZUpICBApaWl8vf3r3M7AADg8qnt32+33WI7e/as8vLylJSU9PNgLBYlJSUpJyfnoscbhiGbzabCwkL16dNH0vmAtWnTJv3qV79ScnKyQkNDlZCQoA0bNlQ5ftWqVQoJCdH111+v6dOn68yZMxfsz263q6yszGUDAABXJrcFpBMnTqiyslJhYWEu5WFhYSouLq7xuNLSUrVs2VJeXl4aPHiwFi1apIEDB0qSjh07pvLyci1YsEApKSn697//raFDh2rYsGF67733nG3ce++9Wrlypd59911Nnz5d2dnZuu+++y443szMTAUEBDi3yMjIXzB7AADQmLl1DVJd+Pn5KT8/X+Xl5bLZbMrIyFD79u3Vr18/ORwOSdIdd9yhKVOmSJJiY2O1c+dOLV++XH379pUkjR071tneDTfcoNatW2vAgAHat2+foqOjq+13+vTpysjIcH4uKysjJAEAcIVyW0AKCQmRp6enSkpKXMpLSkoUHh5e43EWi0UdOnSQdD78FBQUKDMzU/369VNISIiaNWumLl26uBzTuXNnffDBBzW2mZCQIEn6+uuvawxIVqtVVqu1VnMDAABNm9tusXl5ealnz56y2WzOMofDIZvNpsTExFq343A4ZLfbnW326tVLhYWFLnW+/PJLtW3btsY28vPzJUmtW7e+hBkAAIArlVtvsWVkZGj06NGKi4tTfHy8srKyVFFRofT0dEnSqFGj1KZNG2VmZko6vw4oLi5O0dHRstvt2rx5s7Kzs7Vs2TJnm1OnTlVqaqr69Omj/v37a8uWLXrrrbe0fft2SedfA7B69WrddtttCg4O1p49ezRlyhT16dNH3bp1u+znAAAAND5uDUipqak6fvy4Zs6cqeLiYsXGxmrLli3OhdsHDx6UxfLzRa6KigqNGzdORUVF8vHxUUxMjFauXKnU1FRnnaFDh2r58uXKzMzUpEmT1KlTJ7322mvq3bu3pPNXmd5++21nGIuMjNTw4cP1xz/+8fJOHgAANFpufQ9SU8Z7kAAAaHoa/XuQAAAAGisCEgAAgAkBCQAAwISABAAAYEJAAgAAMCEgAQAAmBCQAAAATAhIAAAAJgQkAAAAEwISAACACQEJAADAhIAEAABgQkACAAAwISABAACYEJAAAABMCEgAAAAmBCQAAAATAhIAAIAJAQkAAMCEgAQAAGBCQAIAADAhIAEAAJgQkAAAAEwISAAAACYEJAAAABMCEgAAgAkBCQAAwISABAAAYEJAAgAAMCEgAQAAmBCQAAAATAhIAAAAJgQkAAAAEwISAACACQEJAADAhIAEAABg4vaAtGTJEkVFRcnb21sJCQnKzc2tse769esVFxenwMBA+fr6KjY2VtnZ2VXqFRQUaMiQIQoICJCvr6969eqlgwcPOvf/8MMPGj9+vIKDg9WyZUsNHz5cJSUlDTI/AADQ9Lg1IK1du1YZGRmaNWuWdu/ere7duys5OVnHjh2rtn5QUJBmzJihnJwc7dmzR+np6UpPT9fWrVuddfbt26fevXsrJiZG27dv1549e/TEE0/I29vbWWfKlCl66623tG7dOr333ns6cuSIhg0b1uDzBQAATYOHYRiGuzpPSEhQr169tHjxYkmSw+FQZGSkJk6cqGnTptWqjR49emjw4MGaN2+eJGnEiBFq3rx5tVeWJKm0tFTXXHONVq9erbvuukuStHfvXnXu3Fk5OTm66aabatVvWVmZAgICVFpaKn9//1odAwAA3Ku2f7/ddgXp7NmzysvLU1JS0s+DsViUlJSknJycix5vGIZsNpsKCwvVp08fSecD1qZNm/SrX/1KycnJCg0NVUJCgjZs2OA8Li8vTz/++KNLvzExMbruuutq1S8AALjyuS0gnThxQpWVlQoLC3MpDwsLU3FxcY3HlZaWqmXLlvLy8tLgwYO1aNEiDRw4UJJ07NgxlZeXa8GCBUpJSdG///1vDR06VMOGDdN7770nSSouLpaXl5cCAwMvqV+73a6ysjKXDQAAXJmauXsAl8rPz0/5+fkqLy+XzWZTRkaG2rdvr379+snhcEiS7rjjDk2ZMkWSFBsbq507d2r58uXq27dvnfvNzMzUnDlz6mUOAACgcXPbFaSQkBB5enpWeXqspKRE4eHhNR5nsVjUoUMHxcbG6tFHH9Vdd92lzMxMZ5vNmjVTly5dXI7p3Lmz8ym28PBwnT17VqdOnbqkfqdPn67S0lLndujQoUuZLgAAaELcFpC8vLzUs2dP2Ww2Z5nD4ZDNZlNiYmKt23E4HLLb7c42e/XqpcLCQpc6X375pdq2bStJ6tmzp5o3b+7Sb2FhoQ4ePHjBfq1Wq/z9/V02AABwZXLrLbaMjAyNHj1acXFxio+PV1ZWlioqKpSeni5JGjVqlNq0aeO8QpSZmam4uDhFR0fLbrdr8+bNys7O1rJly5xtTp06VampqerTp4/69++vLVu26K233tL27dslSQEBAXrggQeUkZGhoKAg+fv7a+LEiUpMTKz1E2wAAODK5taAlJqaquPHj2vmzJkqLi5WbGystmzZ4ly4ffDgQVksP1/kqqio0Lhx41RUVCQfHx/FxMRo5cqVSk1NddYZOnSoli9frszMTE2aNEmdOnXSa6+9pt69ezvr/O1vf5PFYtHw4cNlt9uVnJyspUuXXr6JAwCARs2t70FqyngPEgAATU+jfw8SAABAY0VAAgAAMCEgAQAAmBCQAAAATAhIAAAAJgQkAAAAEwISAACACQEJAADAhIAEAABgQkACAAAwISABAACYEJAAAABMCEgAAAAmBCQAAAATAhIAAIAJAQkAAMCEgAQAAGBCQAIAADAhIAEAAJgQkAAAAEwISAAAACYEJAAAABMCEgAAgAkBCQAAwISABAAAYEJAAgAAMCEgAQAAmBCQAAAATAhIAAAAJgQkAAAAEwISAACACQEJAADAhIAEAABgQkACAAAwISABAACYEJAAAABMCEgAAAAmjSIgLVmyRFFRUfL29lZCQoJyc3NrrLt+/XrFxcUpMDBQvr6+io2NVXZ2tkud+++/Xx4eHi5bSkqKS52oqKgqdRYsWNAg8wMAAE1LM3cPYO3atcrIyNDy5cuVkJCgrKwsJScnq7CwUKGhoVXqBwUFacaMGYqJiZGXl5c2btyo9PR0hYaGKjk52VkvJSVFL774ovOz1Wqt0tbcuXM1ZswY52c/P796nh0AAGiK3B6QFi5cqDFjxig9PV2StHz5cm3atEkvvPCCpk2bVqV+v379XD4/8sgjWrFihT744AOXgGS1WhUeHn7Bvv38/C5aBwAAXH3ceovt7NmzysvLU1JSkrPMYrEoKSlJOTk5Fz3eMAzZbDYVFhaqT58+Lvu2b9+u0NBQderUSQ8//LC+++67KscvWLBAwcHBuvHGG/XUU0/p3LlzNfZlt9tVVlbmsgEAgCuTW68gnThxQpWVlQoLC3MpDwsL0969e2s8rrS0VG3atJHdbpenp6eWLl2qgQMHOvenpKRo2LBhateunfbt26c//OEPGjRokHJycuTp6SlJmjRpknr06KGgoCDt3LlT06dP19GjR7Vw4cJq+8zMzNScOXPqYdYAAKCx8zAMw3BX50eOHFGbNm20c+dOJSYmOssff/xxvffee9q1a1e1xzkcDu3fv1/l5eWy2WyaN2+eNmzYUOX220/279+v6Ohovf322xowYEC1dV544QU9+OCDKi8vr3a9kt1ul91ud34uKytTZGSkSktL5e/vfwmzBgAA7lJWVqaAgICL/v126xWkkJAQeXp6qqSkxKW8pKTkgmuDLBaLOnToIEmKjY1VQUGBMjMzawxI7du3V0hIiL7++usaA1JCQoLOnTunAwcOqFOnTlX2W63WaoMTAAC48rh1DZKXl5d69uwpm83mLHM4HLLZbC5XlC7G4XC4XN0xKyoq0nfffafWrVvXWCc/P18Wi6XaJ+cAAMDVxe1PsWVkZGj06NGKi4tTfHy8srKyVFFR4XyqbdSoUWrTpo0yMzMlnV8LFBcXp+joaNntdm3evFnZ2dlatmyZJKm8vFxz5szR8OHDFR4ern379unxxx9Xhw4dnE+55eTkaNeuXerfv7/8/PyUk5OjKVOm6L777lOrVq3ccyIAAECj4faAlJqaquPHj2vmzJkqLi5WbGystmzZ4ly4ffDgQVksP1/oqqio0Lhx41RUVCQfHx/FxMRo5cqVSk1NlSR5enpqz549WrFihU6dOqWIiAjdeuutmjdvnvMWmdVq1Zo1azR79mzZ7Xa1a9dOU6ZMUUZGxuU/AQAAoNFx6yLtpqy2i7wAAEDjUdu/343ip0YAAAAaEwISAACACQEJAADAhIAEAABgQkACAAAwISABAACY1CkgHTp0SEVFRc7Pubm5mjx5sp599tl6GxgAAIC71Ckg3XvvvXr33XclScXFxRo4cKByc3M1Y8YMzZ07t14HCAAAcLnVKSB99tlnio+PlyS98soruv7667Vz506tWrVKL730Un2ODwAA4LKrU0D68ccfnT/b8fbbb2vIkCGSpJiYGB09erT+RgcAAOAGdQpIXbt21fLly/X+++9r27ZtSklJkSQdOXJEwcHB9TpAAACAy61OAelPf/qT/v73v6tfv3767W9/q+7du0uS3nzzTeetNwAAgKaqzj9WW1lZqbKyMrVq1cpZduDAAbVo0UKhoaH1NsDGih+rBQCg6WnQH6v9/vvvZbfbneHo22+/VVZWlgoLC6+KcAQAAK5sdQpId9xxh/75z39Kkk6dOqWEhAT99a9/1Z133qlly5bV6wABAAAutzoFpN27d+t//ud/JEmvvvqqwsLC9O233+qf//ynnnnmmXodIAAAwOVWp4B05swZ+fn5SZL+/e9/a9iwYbJYLLrpppv07bff1usAAQAALrc6BaQOHTpow4YNOnTokLZu3apbb71VknTs2DEWLAMAgCavTgFp5syZeuyxxxQVFaX4+HglJiZKOn816cYbb6zXAQIAAFxudX7Mv7i4WEePHlX37t1lsZzPWbm5ufL391dMTEy9DrIx4jF/AACantr+/W5W1w7Cw8MVHh6uoqIiSdK1117LSyIBAMAVoU632BwOh+bOnauAgAC1bdtWbdu2VWBgoObNmyeHw1HfYwQAALis6nQFacaMGXr++ee1YMEC3XLLLZKkDz74QLNnz9YPP/yg+fPn1+sgAQAALqc6rUGKiIjQ8uXLNWTIEJfyN954Q+PGjdPhw4frbYCNFWuQAABoehr0p0ZOnjxZ7ULsmJgYnTx5si5NAgAANBp1Ckjdu3fX4sWLq5QvXrxY3bp1+8WDAgAAcKc6rUH685//rMGDB+vtt992vgMpJydHhw4d0ubNm+t1gAAAAJdbna4g9e3bV19++aWGDh2qU6dO6dSpUxo2bJg+//xzZWdn1/cYAQAALqs6vyiyOp988ol69OihysrK+mqy0WKRNgAATU+DLtIGAAC4khGQAAAATAhIAAAAJpf0FNuwYcMuuP/UqVO/ZCwAAACNwiUFpICAgIvuHzVq1C8aEAAAgLtdUkB68cUXG2ocAAAAjQZrkAAAAEwISAAAACaNIiAtWbJEUVFR8vb2VkJCgnJzc2usu379esXFxSkwMFC+vr6KjY2t8vbu+++/Xx4eHi5bSkqKS52TJ08qLS1N/v7+CgwM1AMPPKDy8vIGmR8AAGha3B6Q1q5dq4yMDM2aNUu7d+9W9+7dlZycrGPHjlVbPygoSDNmzFBOTo727Nmj9PR0paena+vWrS71UlJSdPToUef28ssvu+xPS0vT559/rm3btmnjxo3asWOHxo4d22DzBAAATUe9/tRIXSQkJKhXr15avHixJMnhcCgyMlITJ07UtGnTatVGjx49NHjwYM2bN0/S+StIp06d0oYNG6qtX1BQoC5duuijjz5SXFycJGnLli267bbbVFRUpIiIiIv2yU+NAADQ9DSJnxo5e/as8vLylJSU5CyzWCxKSkpSTk7ORY83DEM2m02FhYXq06ePy77t27crNDRUnTp10sMPP6zvvvvOuS8nJ0eBgYHOcCRJSUlJslgs2rVrV7V92e12lZWVuWwAAODKdEmP+de3EydOqLKyUmFhYS7lYWFh2rt3b43HlZaWqk2bNrLb7fL09NTSpUs1cOBA5/6UlBQNGzZM7dq10759+/SHP/xBgwYNUk5Ojjw9PVVcXKzQ0FCXNps1a6agoCAVFxdX22dmZqbmzJnzC2YLAACaCrcGpLry8/NTfn6+ysvLZbPZlJGRofbt26tfv36SpBEjRjjr3nDDDerWrZuio6O1fft2DRgwoE59Tp8+XRkZGc7PZWVlioyM/EXzAAAAjZNbA1JISIg8PT1VUlLiUl5SUqLw8PAaj7NYLOrQoYMkKTY2VgUFBcrMzHQGJLP27dsrJCREX3/9tQYMGKDw8PAqi8DPnTunkydP1tiv1WqV1Wq9hNkBAICmyq1rkLy8vNSzZ0/ZbDZnmcPhkM1mU2JiYq3bcTgcstvtNe4vKirSd999p9atW0uSEhMTderUKeXl5TnrvPPOO3I4HEpISKjDTAAAwJXE7bfYMjIyNHr0aMXFxSk+Pl5ZWVmqqKhQenq6JGnUqFFq06aNMjMzJZ1fCxQXF6fo6GjZ7XZt3rxZ2dnZWrZsmSSpvLxcc+bM0fDhwxUeHq59+/bp8ccfV4cOHZScnCxJ6ty5s1JSUjRmzBgtX75cP/74oyZMmKARI0bU6gk2AABwZXN7QEpNTdXx48c1c+ZMFRcXKzY2Vlu2bHEu3D548KAslp8vdFVUVGjcuHEqKiqSj4+PYmJitHLlSqWmpkqSPD09tWfPHq1YsUKnTp1SRESEbr31Vs2bN8/lFtmqVas0YcIEDRgwQBaLRcOHD9czzzxzeScPAAAaJbe/B6mp4j1IAAA0PU3iPUgAAACNEQEJAADAhIAEAABgQkACAAAwISABAACYEJAAAABMCEgAAAAmBCQAAAATAhIAAIAJAQkAAMCEgAQAAGBCQAIAADAhIAEAAJgQkAAAAEwISAAAACYEJAAAABMCEgAAgAkBCQAAwISABAAAYEJAAgAAMCEgAQAAmBCQAAAATAhIAAAAJgQkAAAAEwISAACACQEJAADAhIAEAABgQkACAAAwISABAACYEJAAAABMCEgAAAAmBCQAAAATAhIAAIAJAQkAAMCEgAQAAGBCQAIAADAhIAEAAJg0ioC0ZMkSRUVFydvbWwkJCcrNza2x7vr16xUXF6fAwED5+voqNjZW2dnZNdZ/6KGH5OHhoaysLJfyqKgoeXh4uGwLFiyorykBAIAmrJm7B7B27VplZGRo+fLlSkhIUFZWlpKTk1VYWKjQ0NAq9YOCgjRjxgzFxMTIy8tLGzduVHp6ukJDQ5WcnOxS9/XXX9eHH36oiIiIavueO3euxowZ4/zs5+dXv5MDAABNktuvIC1cuFBjxoxRenq6unTpouXLl6tFixZ64YUXqq3fr18/DR06VJ07d1Z0dLQeeeQRdevWTR988IFLvcOHD2vixIlatWqVmjdvXm1bfn5+Cg8Pd26+vr71Pj8AAND0uDUgnT17Vnl5eUpKSnKWWSwWJSUlKScn56LHG4Yhm82mwsJC9enTx1nucDg0cuRITZ06VV27dq3x+AULFig4OFg33nijnnrqKZ07d67Guna7XWVlZS4bAAC4Mrn1FtuJEydUWVmpsLAwl/KwsDDt3bu3xuNKS0vVpk0b2e12eXp6aunSpRo4cKBz/5/+9Cc1a9ZMkyZNqrGNSZMmqUePHgoKCtLOnTs1ffp0HT16VAsXLqy2fmZmpubMmXOJMwQAAE2R29cg1YWfn5/y8/NVXl4um82mjIwMtW/fXv369VNeXp6efvpp7d69Wx4eHjW2kZGR4fznbt26ycvLSw8++KAyMzNltVqr1J8+fbrLMWVlZYqMjKzfiQEAgEbBrQEpJCREnp6eKikpcSkvKSlReHh4jcdZLBZ16NBBkhQbG6uCggJlZmaqX79+ev/993Xs2DFdd911zvqVlZV69NFHlZWVpQMHDlTbZkJCgs6dO6cDBw6oU6dOVfZbrdZqgxMAALjyuHUNkpeXl3r27CmbzeYsczgcstlsSkxMrHU7DodDdrtdkjRy5Ejt2bNH+fn5zi0iIkJTp07V1q1ba2wjPz9fFoul2ifnAADA1cXtt9gyMjI0evRoxcXFKT4+XllZWaqoqFB6erokadSoUWrTpo0yMzMlnV8LFBcXp+joaNntdm3evFnZ2dlatmyZJCk4OFjBwcEufTRv3lzh4eHOK0M5OTnatWuX+vfvLz8/P+Xk5GjKlCm677771KpVq8s4ewAA0Bi5PSClpqbq+PHjmjlzpoqLixUbG6stW7Y4F24fPHhQFsvPF7oqKio0btw4FRUVycfHRzExMVq5cqVSU1Nr3afVatWaNWs0e/Zs2e12tWvXTlOmTHFZYwQAAK5eHoZhGO4eRFNUVlamgIAAlZaWyt/f393DAQAAtVDbv99uf1EkAABAY0NAAgAAMCEgAQAAmBCQAAAATAhIAAAAJgQkAAAAEwISAACACQEJAADAhIAEAABgQkACAAAwISABAACYEJAAAABMCEgAAAAmBCQAAAATAhIAAIAJAQkAAMCEgAQAAGBCQAIAADAhIAEAAJgQkAAAAEwISAAAACYEJAAAABMCEgAAgAkBCQAAwISABAAAYEJAAgAAMCEgAQAAmBCQAAAATAhIAAAAJgQkAAAAEwISAACACQEJAADAhIAEAABgQkACAAAwISABAACYEJAAAABMGkVAWrJkiaKiouTt7a2EhATl5ubWWHf9+vWKi4tTYGCgfH19FRsbq+zs7BrrP/TQQ/Lw8FBWVpZL+cmTJ5WWliZ/f38FBgbqgQceUHl5eX1NCQAANGFuD0hr165VRkaGZs2apd27d6t79+5KTk7WsWPHqq0fFBSkGTNmKCcnR3v27FF6errS09O1devWKnVff/11ffjhh4qIiKiyLy0tTZ9//rm2bdumjRs3aseOHRo7dmy9zw8AADQ9HoZhGO4cQEJCgnr16qXFixdLkhwOhyIjIzVx4kRNmzatVm306NFDgwcP1rx585xlhw8fVkJCgrZu3arBgwdr8uTJmjx5siSpoKBAXbp00UcffaS4uDhJ0pYtW3TbbbepqKio2kBlVlZWpoCAAJWWlsrf3/8SZw0AANyhtn+/3XoF6ezZs8rLy1NSUpKzzGKxKCkpSTk5ORc93jAM2Ww2FRYWqk+fPs5yh8OhkSNHaurUqeratWuV43JychQYGOgMR5KUlJQki8WiXbt2VduX3W5XWVmZywYAAK5Mbg1IJ06cUGVlpcLCwlzKw8LCVFxcXONxpaWlatmypby8vDR48GAtWrRIAwcOdO7/05/+pGbNmmnSpEnVHl9cXKzQ0FCXsmbNmikoKKjGfjMzMxUQEODcIiMjaztNAADQxDRz9wDqws/PT/n5+SovL5fNZlNGRobat2+vfv36KS8vT08//bR2794tDw+Peutz+vTpysjIcH4uKysjJAEAcIVya0AKCQmRp6enSkpKXMpLSkoUHh5e43EWi0UdOnSQJMXGxqqgoECZmZnq16+f3n//fR07dkzXXXeds35lZaUeffRRZWVl6cCBAwoPD6+yCPzcuXM6efJkjf1arVZZrda6ThUAADQhbr3F5uXlpZ49e8pmsznLHA6HbDabEhMTa92Ow+GQ3W6XJI0cOVJ79uxRfn6+c4uIiNDUqVOdT7olJibq1KlTysvLc7bxzjvvyOFwKCEhoZ5mBwAAmiq332LLyMjQ6NGjFRcXp/j4eGVlZamiokLp6emSpFGjRqlNmzbKzMyUdH4tUFxcnKKjo2W327V582ZlZ2dr2bJlkqTg4GAFBwe79NG8eXOFh4erU6dOkqTOnTsrJSVFY8aM0fLly/Xjjz9qwoQJGjFiRK2eYAMAAFc2twek1NRUHT9+XDNnzlRxcbFiY2O1ZcsW58LtgwcPymL5+UJXRUWFxo0bp6KiIvn4+CgmJkYrV65UamrqJfW7atUqTZgwQQMGDJDFYtHw4cP1zDPP1OvcAABA0+T29yA1VbwHCQCApqdJvAcJAACgMSIgAQAAmBCQAAAATAhIAAAAJgQkAAAAEwISAACACQEJAADAhIAEAABgQkACAAAwISABAACYEJAAAABMCEgAAAAmBCQAAAATAhIAAIAJAQkAAMCEgAQAAGBCQAIAADAhIAEAAJgQkAAAAEwISAAAACYEJAAAABMCEgAAgAkBCQAAwISABAAAYEJAAgAAMCEgAQAAmBCQAAAATAhIAAAAJgQkAAAAEwISAACACQEJAADAhIAEAABgQkACAAAwISABAACYEJAAAABMCEgAAAAmBCQAAACTRhGQlixZoqioKHl7eyshIUG5ubk11l2/fr3i4uIUGBgoX19fxcbGKjs726XO7NmzFRMTI19fX7Vq1UpJSUnatWuXS52oqCh5eHi4bAsWLGiQ+QEAgKbF7QFp7dq1ysjI0KxZs7R79251795dycnJOnbsWLX1g4KCNGPGDOXk5GjPnj1KT09Xenq6tm7d6qzzq1/9SosXL9ann36qDz74QFFRUbr11lt1/Phxl7bmzp2ro0ePOreJEyc26FwBAEDT4GEYhuHOASQkJKhXr15avHixJMnhcCgyMlITJ07UtGnTatVGjx49NHjwYM2bN6/a/WVlZQoICNDbb7+tAQMGSDp/BWny5MmaPHlyncb9U5ulpaXy9/evUxsAAODyqu3fb7deQTp79qzy8vKUlJTkLLNYLEpKSlJOTs5FjzcMQzabTYWFherTp0+NfTz77LMKCAhQ9+7dXfYtWLBAwcHBuvHGG/XUU0/p3LlzNfZlt9tVVlbmsgEAgCtTM3d2fuLECVVWViosLMylPCwsTHv37q3xuNLSUrVp00Z2u12enp5aunSpBg4c6FJn48aNGjFihM6cOaPWrVtr27ZtCgkJce6fNGmSevTooaCgIO3cuVPTp0/X0aNHtXDhwmr7zMzM1Jw5c37BbAEAQFPh1oBUV35+fsrPz1d5eblsNpsyMjLUvn179evXz1mnf//+ys/P14kTJ/Tcc8/pnnvu0a5duxQaGipJysjIcNbt1q2bvLy89OCDDyozM1NWq7VKn9OnT3c5pqysTJGRkQ03SQAA4DZuDUghISHy9PRUSUmJS3lJSYnCw8NrPM5isahDhw6SpNjYWBUUFCgzM9MlIPn6+qpDhw7q0KGDbrrpJnXs2FHPP/+8pk+fXm2bCQkJOnfunA4cOKBOnTpV2W+1WqsNTgAA4Mrj1jVIXl5e6tmzp2w2m7PM4XDIZrMpMTGx1u04HA7Z7fZfVCc/P18Wi8V5hQkAAFy93H6LLSMjQ6NHj1ZcXJzi4+OVlZWliooKpaenS5JGjRqlNm3aKDMzU9L5tUBxcXGKjo6W3W7X5s2blZ2drWXLlkmSKioqNH/+fA0ZMkStW7fWiRMntGTJEh0+fFh33323JCknJ0e7du1S//795efnp5ycHE2ZMkX33XefWrVq5Z4TAQAAGg23B6TU1FQdP35cM2fOVHFxsWJjY7Vlyxbnwu2DBw/KYvn5QldFRYXGjRunoqIi+fj4KCYmRitXrlRqaqokydPTU3v37tWKFSt04sQJBQcHq1evXnr//ffVtWtXSedvl61Zs0azZ8+W3W5Xu3btNGXKFJc1RgAA4Orl9vcgNVW8BwkAgKanSbwHCQAAoDEiIAEAAJgQkAAAAEwISAAAACYEJAAAABMCEgAAgAkBCQAAwISABAAAYEJAAgAAMCEgAQAAmBCQAAAATNz+Y7VN1U8/YVdWVubmkQAAgNr66e/2xX6KloBUR6dPn5YkRUZGunkkAADgUp0+fVoBAQE17vcwLhahUC2Hw6EjR47Iz89PHh4e7h6OW5WVlSkyMlKHDh264C8j45fjXF8enOfLg/N8eXCeXRmGodOnTysiIkIWS80rjbiCVEcWi0XXXnutu4fRqPj7+/Mf32XCub48OM+XB+f58uA8/+xCV45+wiJtAAAAEwISAACACQEJv5jVatWsWbNktVrdPZQrHuf68uA8Xx6c58uD81w3LNIGAAAw4QoSAACACQEJAADAhIAEAABgQkACAAAwISChWkuWLFFUVJS8vb2VkJCg3NzcGuv++OOPmjt3rqKjo+Xt7a3u3btry5YtVeodPnxY9913n4KDg+Xj46MbbrhB/+///b+GnEajV9/nubKyUk888YTatWsnHx8fRUdHa968eRf9zaEr2Y4dO/Sb3/xGERER8vDw0IYNGy56zPbt29WjRw9ZrVZ16NBBL730UpU6l/LdXQ0a4jxnZmaqV69e8vPzU2hoqO68804VFhY2zASaiIb69/knCxYskIeHhyZPnlxvY26yDMBkzZo1hpeXl/HCCy8Yn3/+uTFmzBgjMDDQKCkpqbb+448/bkRERBibNm0y9u3bZyxdutTw9vY2du/e7axz8uRJo23btsb9999v7Nq1y9i/f7+xdetW4+uvv75c02p0GuI8z58/3wgODjY2btxofPPNN8a6deuMli1bGk8//fTlmlajs3nzZmPGjBnG+vXrDUnG66+/fsH6+/fvN1q0aGFkZGQYX3zxhbFo0SLD09PT2LJli7POpX53V4OGOM/JycnGiy++aHz22WdGfn6+cdtttxnXXXedUV5e3sCzabwa4jz/JDc314iKijK6detmPPLIIw0zgSaEgIQq4uPjjfHjxzs/V1ZWGhEREUZmZma19Vu3bm0sXrzYpWzYsGFGWlqa8/Pvf/97o3fv3g0z4CaqIc7z4MGDjf/93/+9YJ2rWW3+oDz++ONG165dXcpSU1ON5ORk5+dL/e6uNvV1ns2OHTtmSDLee++9+hhmk1ef5/n06dNGx44djW3bthl9+/YlIBmGwS02uDh79qzy8vKUlJTkLLNYLEpKSlJOTk61x9jtdnl7e7uU+fj46IMPPnB+fvPNNxUXF6e7775boaGhuvHGG/Xcc881zCSagIY6zzfffLNsNpu+/PJLSdInn3yiDz74QIMGDWqAWVyZcnJyXL4XSUpOTnZ+L3X57lDVxc5zdUpLSyVJQUFBDTq2K0ltz/P48eM1ePDgKnWvZgQkuDhx4oQqKysVFhbmUh4WFqbi4uJqj0lOTtbChQv11VdfyeFwaNu2bVq/fr2OHj3qrLN//34tW7ZMHTt21NatW/Xwww9r0qRJWrFiRYPOp7FqqPM8bdo0jRgxQjExMWrevLluvPFGTZ48WWlpaQ06nytJcXFxtd9LWVmZvv/++zp9d6jqYufZzOFwaPLkybrlllt0/fXXX65hNnm1Oc9r1qzR7t27lZmZ6Y4hNloEJPxiTz/9tDp27KiYmBh5eXlpwoQJSk9Pl8Xy879eDodDPXr00JNPPqkbb7xRY8eO1ZgxY7R8+XI3jrxpqc15fuWVV7Rq1SqtXr1au3fv1ooVK/SXv/zlqg2iuHKMHz9en332mdasWePuoVxRDh06pEceeUSrVq2qcoX6akdAgouQkBB5enqqpKTEpbykpETh4eHVHnPNNddow4YNqqio0Lfffqu9e/eqZcuWat++vbNO69at1aVLF5fjOnfurIMHD9b/JJqAhjrPU6dOdV5FuuGGGzRy5EhNmTKF/2d4CcLDw6v9Xvz9/eXj41On7w5VXew8/18TJkzQxo0b9e677+raa6+9nMNs8i52nvPy8nTs2DH16NFDzZo1U7NmzfTee+/pmWeeUbNmzVRZWemmkbsfAQkuvLy81LNnT9lsNmeZw+GQzWZTYmLiBY/19vZWmzZtdO7cOb322mu64447nPtuueWWKo/nfvnll2rbtm39TqCJaKjzfObMGZcrSpLk6ekph8NRvxO4giUmJrp8L5K0bds25/fyS747/Oxi51mSDMPQhAkT9Prrr+udd95Ru3btLvcwm7yLnecBAwbo008/VX5+vnOLi4tTWlqa8vPz5enp6Y5hNw7uXiWOxmfNmjWG1Wo1XnrpJeOLL74wxo4dawQGBhrFxcWGYRjGyJEjjWnTpjnrf/jhh8Zrr71m7Nu3z9ixY4fx61//2mjXrp3x3//+11knNzfXaNasmTF//nzjq6++MlatWmW0aNHCWLly5eWeXqPREOd59OjRRps2bZyP+a9fv94ICQkxHn/88cs9vUbj9OnTxscff2x8/PHHhiRj4cKFxscff2x8++23hmEYxrRp04yRI0c66//0WPTUqVONgoICY8mSJdU+5n+h7+5q1BDn+eGHHzYCAgKM7du3G0ePHnVuZ86cuezzaywa4jyb8RTbeQQkVGvRokXGddddZ3h5eRnx8fHGhx9+6NzXt29fY/To0c7P27dvNzp37mxYrVYjODjYGDlypHH48OEqbb711lvG9ddfb1itViMmJsZ49tlnL8dUGrX6Ps9lZWXGI488Ylx33XWGt7e30b59e2PGjBmG3W6/XFNqdN59911DUpXtp3M7evRoo2/fvlWOiY2NNby8vIz27dsbL774YpV2L/TdXY0a4jxX156kar+Pq0VD/fv8fxGQzvMwjKv4FbsAAADVYA0SAACACQEJAADAhIAEAABgQkACAAAwISABAACYEJAAAABMCEgAAAAmBCQAqCceHh7asGGDu4cBoB4QkABcEe6//355eHhU2VJSUtw9NABNUDN3DwAA6ktKSopefPFFlzKr1eqm0QBoyriCBOCKYbVaFR4e7rK1atVK0vnbX8uWLdOgQYPk4+Oj9u3b69VXX3U5/tNPP9Wvf/1r+fj4KDg4WGPHjlV5eblLnRdeeEFdu3aV1WpV69atNWHCBJf9J06c0NChQ9WiRQt17NhRb775ZsNOGkCDICABuGo88cQTGj58uD755BOlpaVpxIgRKigokCRVVFQoOTlZrVq10kcffaR169bp7bffdglAy5Yt0/jx4zV27Fh9+umnevPNN9WhQweXPubMmaN77rlHe/bs0W233aa0tDSdPHnyss4TQD1w96/lAkB9GD16tOHp6Wn4+vq6bPPnzzcM4/wvwz/00EMuxyQkJBgPP/ywYRiG8eyzzxqtWrUyysvLnfs3bdpkWCwWo7i42DAMw4iIiDBmzJhR4xgkGX/84x+dn8vLyw1Jxr/+9a96myeAy4M1SACuGP3799eyZctcyoKCgpz/nJiY6LIvMTFR+fn5kqSCggJ1795dvr6+zv233HKLHA6HCgsL5eHhoSNHjmjAgAEXHEO3bt2c/+zr6yt/f38dO3asrlMC4CYEJABXDF9f3yq3vOqLj49Preo1b97c5bOHh4ccDkdDDAlAA2INEoCrxocffljlc+fOnSVJnTt31ieffKKKigrn/v/85z+yWCzq1KmT/Pz8FBUVJZvNdlnHDMA9uIIE4Ipht9tVXFzsUtasWTOFhIRIktatW6e4uDj17t1bq1atUm5urp5//nlJUlpammbNmqXRo0dr9uzZOn78uCZOnKiRI0cqLCxMkjR79mw99NBDCg0N1aBBg3T69Gn95z//0cSJEy/vRAE0OAISgCvGli1b1Lp1a5eyTp06ae/evZLOP2G2Zs0ajRs3Tq1bt9bLL7+sLl26SJJatGihrVu36pFHHlGvXr3UokULDR8+XAsXLnS2NXr0aP3www/629/+pscee0whISG66667Lt8EAVw2HoZhGO4eBAA0NA8PD73++uu688473T0UAE0Aa5AAAABMCEgAAAAmrEECcFVgNQGAS8EVJAAAABMCEgAAgAkBCQAAwISABAAAYEJAAgAAMCEgAQAAmBCQAAAATAhIAAAAJgQkAAAAk/8Pw5WHUftJMLgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Student for lambda value of 0\n",
      "Student model CustomSmallCNN saved for lambda 0 to Smaller_Student_Models_11/models/CustomSmallCNN_lambda0_20231206_093934.pth\n",
      "Weights saved for lambda 0 to Smaller_Student_Models_11/weights/CustomSmallCNN_weights_lambda0_20231206_093934.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 162/162 [01:48<00:00,  1.50it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 55/55 [00:20<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Epoch 1/1*****\n",
      "*****Train Loss:  0.201419 Val Loss:  0.145546*****\n",
      "*****Validation Accuracy: 63.26%*****\n",
      "*****Total Avg Disparity: 0.17714795224019117*****\n",
      "\n",
      "Class Team_Sports: Recall Difference = 0.09393243536575613\n",
      "Class Celebration: Recall Difference = 0.1093564356435644\n",
      "Class Parade: Recall Difference = 0.008522727272727071\n",
      "Class Waiter_Or_Waitress: Recall Difference = 0.014891179839633284\n",
      "Class Individual_Sports: Recall Difference = -0.21598002496878888\n",
      "Class Surgeons: Recall Difference = -0.08130081300812997\n",
      "Class Spa: Recall Difference = -0.2499999999999999\n",
      "Class Law_Enforcement: Recall Difference = 0.21314553990610352\n",
      "Class Business: Recall Difference = -0.06685829873309906\n",
      "Class Dresses: Recall Difference = -0.8742514970059881\n",
      "Class Water_Activities: Recall Difference = -0.021715988929103647\n",
      "Class Picnic: Recall Difference = -0.40522875816993453\n",
      "Class Rescue: Recall Difference = 0.1147058823529411\n",
      "Class Cheering: Recall Difference = 0.05094108019639934\n",
      "Class Performance_And_Entertainment: Recall Difference = 0.1356033965177117\n",
      "Class Family: Recall Difference = -0.17793317793317803\n",
      "Data has been appended to Smaller_Student_Models_11/student_validation_0.txt\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Student for lambda value of 0\n",
      "Student model EfficientNet saved for lambda 0 to Smaller_Student_Models_11/models/EfficientNet_lambda0_20231206_094049.pth\n",
      "Weights saved for lambda 0 to Smaller_Student_Models_11/weights/EfficientNet_weights_lambda0_20231206_094049.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███████████████████████████████████████▊                                                                         | 57/162 [00:38<01:10,  1.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m     pretrain_adversary(adv, teacher_model, optimizer_adv, trainloader, adv_criterion, device, epochs_pretrain)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Train the student model with knowledge distillation and disparity reduction\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m student_mean_abs_val_disparity \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_student_with_distillation_disparity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_clf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                                                           \u001b[49m\u001b[43madv_criterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstudent_optimizer_adv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                                                           \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlmda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience_student\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Save the trained student model and its weights\u001b[39;00m\n\u001b[1;32m     34\u001b[0m model_save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(models_dir, model_filename)\n",
      "Cell \u001b[0;32mIn[28], line 219\u001b[0m, in \u001b[0;36mtrain_student_with_distillation_disparity\u001b[0;34m(student, teacher, adv, trainloader, testloader, criterion, adv_criterion, optimizers, optimizera, device, alpha, temperature, epochs, lmda, patience)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lmda \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    218\u001b[0m     optimizera\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 219\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    221\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop for training the student model with different lambda values\n",
    "for i in lmda_list_student:\n",
    "    # Load the teacher model with lambda 0\n",
    "    teacher_model_path = os.path.join(output_dir, 'teacher_model_ckd_wider_lambda0.pth')\n",
    "    teacher_model = torch.load(teacher_model_path)\n",
    "    teacher_model_weights_path = os.path.join(output_dir, 'teacher_model_weights_ckd_wider_lambda0.pth')\n",
    "    teacher_model.load_state_dict(torch.load(teacher_model_weights_path))\n",
    "    teacher_model = teacher_model.to(device)\n",
    "        \n",
    "    for student_model in student_models:\n",
    "        model_name = type(student_model).__name__\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_filename = f\"{model_name}_lambda{i}_{timestamp}.pth\"\n",
    "        weights_filename = f\"{model_name}_weights_lambda{i}_{timestamp}.pth\"\n",
    "\n",
    "        # Reset the optimizer for the student model for each lambda\n",
    "        student_optimizer = optim.Adam(student_model.parameters(), lr=best_lr_student)\n",
    "        \n",
    "        # Initialize the adversary\n",
    "        adv = Adversary()\n",
    "        student_optimizer_adv = optim.Adam(adv.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Pretrain student and adversary if i != 0\n",
    "        if i != 0:\n",
    "            pretrain_student(student_model, teacher_model, trainloader, criterion_clf, student_optimizer, device, alpha, temperature, epochs_pretrain)\n",
    "            pretrain_adversary(adv, teacher_model, optimizer_adv, trainloader, adv_criterion, device, epochs_pretrain)\n",
    "        \n",
    "        # Train the student model with knowledge distillation and disparity reduction\n",
    "        student_mean_abs_val_disparity = train_student_with_distillation_disparity(student_model, teacher_model, adv, trainloader, testloader, criterion_clf, \n",
    "                                                                                   adv_criterion, student_optimizer, student_optimizer_adv, device, alpha, \n",
    "                                                                                   temperature, epochs, lmda=i, patience=patience_student)\n",
    "    \n",
    "        # Save the trained student model and its weights\n",
    "        model_save_path = os.path.join(models_dir, model_filename)\n",
    "        weights_save_path = os.path.join(weights_dir, weights_filename)\n",
    "    \n",
    "        torch.save(student_model, model_save_path)\n",
    "        torch.save(student_model.state_dict(), weights_save_path)\n",
    "    \n",
    "        print(f\"Student model {model_name} saved for lambda {i} to {model_save_path}\")\n",
    "        print(f\"Weights saved for lambda {i} to {weights_save_path}\")\n",
    "    \n",
    "        # Update the dictionary with the student results\n",
    "        if i not in lambda_results:\n",
    "            lambda_results[i] = {model_name: {'student_mean_abs_val_disparity': student_mean_abs_val_disparity}}\n",
    "        else:\n",
    "            if model_name not in lambda_results[i]:\n",
    "                lambda_results[i][model_name] = {'student_mean_abs_val_disparity': student_mean_abs_val_disparity}\n",
    "            else:\n",
    "                lambda_results[i][model_name].update({'student_mean_abs_val_disparity': student_mean_abs_val_disparity})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073656a3-9d3d-469d-a66f-0afc3c289ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs)\n",
    "            student_outputs = student(inputs)\n",
    "            \n",
    "        teacher_preds = torch.argmax(teacher_outputs, dim=1).cpu().numpy()\n",
    "        student_preds = torch.argmax(student_outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(teacher_preds)\n",
    "        all_student_preds.append(student_preds)\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'all_labels': all_labels,\n",
    "        'all_teacher_preds': all_teacher_preds,\n",
    "        'all_student_preds': all_student_preds\n",
    "    }\n",
    "\n",
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    data = next(dataiter)\n",
    "    inputs = data['img']\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    teacher_time, student_time = 0, 0\n",
    "\n",
    "    if teacher is not None:\n",
    "        teacher = teacher.to(device)\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs)\n",
    "        teacher_time = time.time() - start_time\n",
    "\n",
    "    if student is not None:\n",
    "        student = student.to(device)\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            student_outputs = student(inputs)\n",
    "        student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249e04fd-db83-4893-bfd5-31d8808243d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each lambda value\n",
    "for lmda_teacher in lmda_list_teacher:\n",
    "    for lmda_student in lmda_list_student:\n",
    "\n",
    "        # Load the teacher model for the current lambda from the 'output_dir'\n",
    "        teacher_model_path = os.path.join(output_dir, f'teacher_model_ckd_wider_lambda{lmda_teacher}.pth')\n",
    "        teacher_model = torch.load(teacher_model_path)\n",
    "        \n",
    "        # Load the student model for the current lambda from the 'output_dir'\n",
    "        student_model_path = os.path.join(output_dir, f'student_model_ckd_wider_lambda{lmda_student}.pth')\n",
    "        student_model = torch.load(student_model_path)\n",
    "    \n",
    "        # Compute performance metrics\n",
    "        performance_metrics = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "    \n",
    "        # Compute model sizes\n",
    "        teacher_params, student_params = compare_model_size(teacher_model, student_model)\n",
    "    \n",
    "        # Construct a unique key for the current combination of lambda values\n",
    "        lambda_key = (lmda_teacher, lmda_student)\n",
    "\n",
    "        # Update results for the current lambda value\n",
    "        if lambda_key in lambda_results:\n",
    "            lambda_results[lambda_key].update({\n",
    "                'performance_metrics': performance_metrics,\n",
    "                'teacher_params': teacher_params,\n",
    "                'student_params': student_params\n",
    "            })\n",
    "        else:\n",
    "            lambda_results[lambda_key] = {\n",
    "                'performance_metrics': performance_metrics,\n",
    "                'teacher_params': teacher_params,\n",
    "                'student_params': student_params\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c127f4e-cc52-45ff-a035-9e0184fb9a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store accuracies\n",
    "teacher_accuracies = []\n",
    "student_accuracies = []\n",
    "lambda_pairs = list(lambda_results.keys())\n",
    "\n",
    "# Iterate over the keys in lambda_results\n",
    "for key in lambda_pairs:\n",
    "    # Check if the key is a tuple (indicating a lambda pair)\n",
    "    if isinstance(key, tuple) and len(key) == 2:\n",
    "        lmda_teacher, lmda_student = key\n",
    "    else:\n",
    "        # If the key is not a tuple, skip this iteration\n",
    "        continue\n",
    "\n",
    "    # Access the performance metrics for each pair\n",
    "    teacher_accuracy = lambda_results[(lmda_teacher, lmda_student)]['performance_metrics']['metrics']['accuracy'][0]\n",
    "    student_accuracy = lambda_results[(lmda_teacher, lmda_student)]['performance_metrics']['metrics']['accuracy'][1]\n",
    "\n",
    "    # Append accuracies to the lists\n",
    "    teacher_accuracies.append((lmda_teacher, teacher_accuracy))\n",
    "    student_accuracies.append((lmda_student, student_accuracy))\n",
    "\n",
    "# To plot, you might need to separate the lambda values and accuracies\n",
    "teacher_lambdas, teacher_acc = zip(*teacher_accuracies)\n",
    "student_lambdas, student_acc = zip(*student_accuracies)\n",
    "\n",
    "# Plotting only with markers and no lines\n",
    "plt.scatter(teacher_lambdas, teacher_acc, label='Teacher Accuracy', marker='o')\n",
    "plt.scatter(student_lambdas, student_acc, label='Student Accuracy', marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Comparison Across Lambdas')\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d58ea-0417-4a20-bc3f-4b69f15b9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store precisions\n",
    "teacher_precisions = []\n",
    "student_precisions = []\n",
    "lambda_pairs = list(lambda_results.keys())\n",
    "\n",
    "# Iterate over the keys in lambda_results\n",
    "for key in lambda_pairs:\n",
    "    # Check if the key is a tuple (indicating a lambda pair)\n",
    "    if isinstance(key, tuple) and len(key) == 2:\n",
    "        lmda_teacher, lmda_student = key\n",
    "        # Access the precision metrics for each pair\n",
    "        teacher_precision = lambda_results[(lmda_teacher, lmda_student)]['performance_metrics']['metrics']['precision'][0]\n",
    "        student_precision = lambda_results[(lmda_teacher, lmda_student)]['performance_metrics']['metrics']['precision'][1]\n",
    "    else:\n",
    "        # If the key is not a tuple, skip this iteration\n",
    "        continue\n",
    "\n",
    "    # Append precisions to the lists along with lambda values\n",
    "    teacher_precisions.append((lmda_teacher, teacher_precision))\n",
    "    student_precisions.append((lmda_student, student_precision))\n",
    "\n",
    "# Extracting lambda values and precisions\n",
    "teacher_lambdas, teacher_prec = zip(*teacher_precisions)\n",
    "student_lambdas, student_prec = zip(*student_precisions)\n",
    "\n",
    "# Creating a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(teacher_lambdas, teacher_prec, label='Teacher Precision', marker='o')\n",
    "plt.scatter(student_lambdas, student_prec, label='Student Precision', marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Comparison Across Lambdas')\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3ee757-c1af-4f1a-85bd-136bc85e6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store recalls\n",
    "teacher_recalls = []\n",
    "student_recalls = []\n",
    "lambda_pairs = list(lambda_results.keys())\n",
    "\n",
    "# Iterate over the keys in lambda_results\n",
    "for key in lambda_pairs:\n",
    "    # Check if the key is a tuple (indicating a lambda pair)\n",
    "    if isinstance(key, tuple) and len(key) == 2:\n",
    "        lmda_teacher, lmda_student = key\n",
    "        # Access the recall metrics for each pair\n",
    "        teacher_recall = lambda_results[(lmda_teacher, lmda_student)]['performance_metrics']['metrics']['recall'][0]\n",
    "        student_recall = lambda_results[(lmda_teacher, lmda_student)]['performance_metrics']['metrics']['recall'][1]\n",
    "    else:\n",
    "        # If the key is not a tuple, skip this iteration\n",
    "        continue\n",
    "\n",
    "    # Append recalls to the lists along with lambda values\n",
    "    teacher_recalls.append((lmda_teacher, teacher_recall))\n",
    "    student_recalls.append((lmda_student, student_recall))\n",
    "\n",
    "# Extracting lambda values and recalls\n",
    "teacher_lambdas, teacher_rec = zip(*teacher_recalls)\n",
    "student_lambdas, student_rec = zip(*student_recalls)\n",
    "\n",
    "# Creating a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(teacher_lambdas, teacher_rec, label='Teacher Recall', marker='o')\n",
    "plt.scatter(student_lambdas, student_rec, label='Student Recall', marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall Comparison Across Lambdas')\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d4e93d-9344-49fc-978f-a00d8d8e8eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store F1 scores\n",
    "teacher_f1s = []\n",
    "student_f1s = []\n",
    "lambda_pairs = list(lambda_results.keys())\n",
    "\n",
    "# Iterate over the keys in lambda_results\n",
    "for key in lambda_pairs:\n",
    "    # Check if the key is a tuple (indicating a lambda pair)\n",
    "    if isinstance(key, tuple) and len(key) == 2:\n",
    "        lmda_teacher, lmda_student = key\n",
    "        # Access the F1 scores for each pair\n",
    "        teacher_f1 = lambda_results[(lmda_teacher, lmda_student)]['performance_metrics']['metrics']['f1'][0]\n",
    "        student_f1 = lambda_results[(lmda_teacher, lmda_student)]['performance_metrics']['metrics']['f1'][1]\n",
    "    else:\n",
    "        # If the key is not a tuple, skip this iteration\n",
    "        continue\n",
    "\n",
    "    # Append F1 scores to the lists along with lambda values\n",
    "    teacher_f1s.append((lmda_teacher, teacher_f1))\n",
    "    student_f1s.append((lmda_student, student_f1))\n",
    "\n",
    "# Extracting lambda values and F1 scores\n",
    "teacher_lambdas, teacher_f1_scores = zip(*teacher_f1s)\n",
    "student_lambdas, student_f1_scores = zip(*student_f1s)\n",
    "\n",
    "# Creating a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(teacher_lambdas, teacher_f1_scores, label='Teacher F1 Score', marker='o')\n",
    "plt.scatter(student_lambdas, student_f1_scores, label='Student F1 Score', marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score Comparison Across Lambdas')\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b769e1-25e1-423c-b77f-af275eb11152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store model sizes\n",
    "teacher_sizes = []\n",
    "student_sizes = []\n",
    "lambda_pairs = list(lambda_results.keys())\n",
    "\n",
    "# Iterate over the keys in lambda_results\n",
    "for key in lambda_pairs:\n",
    "    # Check if the key is a tuple (indicating a lambda pair)\n",
    "    if isinstance(key, tuple) and len(key) == 2:\n",
    "        lmda_teacher, lmda_student = key\n",
    "        # Access the model sizes for each pair\n",
    "        teacher_size = lambda_results[(lmda_teacher, lmda_student)]['teacher_params'] / 1e6  # Convert to millions\n",
    "        student_size = lambda_results[(lmda_teacher, lmda_student)]['student_params'] / 1e6\n",
    "    else:\n",
    "        # If the key is not a tuple, skip this iteration\n",
    "        continue\n",
    "\n",
    "    # Append model sizes to the lists along with lambda values\n",
    "    teacher_sizes.append((lmda_teacher, teacher_size))\n",
    "    student_sizes.append((lmda_student, student_size))\n",
    "\n",
    "# Extracting lambda values and model sizes\n",
    "teacher_lambdas, teacher_model_sizes = zip(*teacher_sizes)\n",
    "student_lambdas, student_model_sizes = zip(*student_sizes)\n",
    "\n",
    "# Creating a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(teacher_lambdas, teacher_model_sizes, label='Teacher Model Size', marker='o')\n",
    "plt.scatter(student_lambdas, student_model_sizes, label='Student Model Size', marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Model Size (Millions of Parameters)')\n",
    "plt.title('Model Size Comparison Across Lambdas')\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8aae8d-9a0d-45b3-9fc2-c27bc5fcaa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionaries to store inference times for each lambda value\n",
    "teacher_times = {}\n",
    "student_times = {}\n",
    "\n",
    "# Loop through each lambda value\n",
    "for lmda_teacher in lmda_list_teacher:\n",
    "    # Load the teacher model for the current lambda\n",
    "    teacher_model_path = os.path.join(output_dir, f'teacher_model_ckd_wider_lambda{lmda_teacher}.pth')\n",
    "    teacher_model = torch.load(teacher_model_path)\n",
    "\n",
    "    teacher_time, _ = compare_inference_time(teacher_model, None, testloader)\n",
    "    teacher_times[lmda_teacher] = teacher_time  # Store the inference time for the teacher model\n",
    "\n",
    "for lmda_student in lmda_list_student:\n",
    "    # Load the student model for the current lambda\n",
    "    student_model_path = os.path.join(output_dir, f'student_model_ckd_wider_lambda{lmda_student}.pth')\n",
    "    student_model = torch.load(student_model_path)\n",
    "\n",
    "    _, student_time = compare_inference_time(None, student_model, testloader)\n",
    "    student_times[lmda_student] = student_time  # Store the inference time for the student model\n",
    "\n",
    "# Extracting lambda values and inference times\n",
    "teacher_lambdas, teacher_inference_times = zip(*teacher_times.items())\n",
    "student_lambdas, student_inference_times = zip(*student_times.items())\n",
    "\n",
    "# Creating a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(teacher_lambdas, teacher_inference_times, label='Teacher Inference Time', marker='o')\n",
    "plt.scatter(student_lambdas, student_inference_times, label='Student Inference Time', marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Inference Time (s)')\n",
    "plt.title('Inference Time Comparison Across Lambdas')\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7cb7f0-12a9-4cec-8188-048c3ffcadad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store disparity values\n",
    "teacher_disparities = []\n",
    "student_disparities = []\n",
    "lambda_pairs = list(lambda_results.keys())\n",
    "\n",
    "# Iterate over the keys in lambda_results\n",
    "for key in lambda_pairs:\n",
    "    # Check if it's an integer key (indicating a lambda value for student)\n",
    "    if isinstance(key, int):\n",
    "        # Check and extract teacher disparity if it exists\n",
    "        if 'teacher_mean_abs_val_disparity' in lambda_results[key]:\n",
    "            teacher_disparity = lambda_results[key]['teacher_mean_abs_val_disparity']\n",
    "            # Take the last value if there are multiple\n",
    "            teacher_disparity = teacher_disparity[-1] if isinstance(teacher_disparity, list) else teacher_disparity\n",
    "            teacher_disparities.append((key, teacher_disparity))\n",
    "\n",
    "        # Extract student disparity\n",
    "        if 'student_mean_abs_val_disparity' in lambda_results[key]:\n",
    "            student_disparity = lambda_results[key]['student_mean_abs_val_disparity']\n",
    "            # Take the last value if there are multiple\n",
    "            student_disparity = student_disparity[-1] if isinstance(student_disparity, list) else student_disparity\n",
    "            student_disparities.append((key, student_disparity))\n",
    "\n",
    "# Extracting lambda values and disparity values\n",
    "teacher_lambdas, teacher_disparity_values = zip(*teacher_disparities) if teacher_disparities else ([], [])\n",
    "student_lambdas, student_disparity_values = zip(*student_disparities) if student_disparities else ([], [])\n",
    "\n",
    "# Creating a scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "if teacher_disparities:\n",
    "    plt.scatter(teacher_lambdas, teacher_disparity_values, label='Teacher Average Disparity', marker='o')\n",
    "if student_disparities:\n",
    "    plt.scatter(student_lambdas, student_disparity_values, label='Student Average Disparity', marker='o')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('Average Disparity')\n",
    "plt.title('Average Disparity Comparison Across Lambdas')\n",
    "plt.legend()\n",
    "\n",
    "# Displaying the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80376f-d5b1-4008-8099-18693972c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution(predictions, class_names, title):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.countplot(x=predictions)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(all_labels, predictions, class_names, title):\n",
    "    cm = confusion_matrix(all_labels, predictions)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    sns.heatmap(pd.DataFrame(cm, index=class_names, columns=class_names), annot=True, fmt='g')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to generate predictions and compute metrics\n",
    "def generate_predictions_and_metrics(model, dataloader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['img'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds)\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Loop over each lambda value for the teacher model\n",
    "for lmda_teacher in lmda_list_teacher:\n",
    "    # Load the teacher model from the 'output_dir'\n",
    "    teacher_model_path = os.path.join(output_dir, f'teacher_model_ckd_wider_lambda{lmda_teacher}.pth')\n",
    "    teacher_model = torch.load(teacher_model_path)\n",
    "\n",
    "    # Generate predictions for the teacher model\n",
    "    all_labels, all_teacher_preds = generate_predictions_and_metrics(teacher_model, testloader)\n",
    "\n",
    "    # Plot distribution and confusion matrix for the teacher model\n",
    "    plot_distribution(all_teacher_preds, class_names_new, f'Teacher Model Predictions (Lambda={lmda_teacher})')\n",
    "    plot_confusion_matrix(all_labels, all_teacher_preds, class_names_new, f'Teacher Confusion Matrix (Lambda={lmda_teacher})')\n",
    "\n",
    "    # Print classification report for the teacher model\n",
    "    teacher_report = classification_report(all_labels, all_teacher_preds, target_names=class_names_new, zero_division=0)\n",
    "    print(f'Classification Report - Teacher Model (Lambda={lmda_teacher})')\n",
    "    print(teacher_report)\n",
    "\n",
    "# Loop over each lambda value for the student model\n",
    "for lmda_student in lmda_list_student:\n",
    "    # Load the student model from the 'output_dir'\n",
    "    student_model_path = os.path.join(output_dir, f'student_model_ckd_wider_lambda{lmda_student}.pth')\n",
    "    student_model = torch.load(student_model_path)\n",
    "\n",
    "    # Generate predictions for the student model\n",
    "    all_labels, all_student_preds = generate_predictions_and_metrics(student_model, testloader)\n",
    "\n",
    "    # Plot distribution and confusion matrix for the student model\n",
    "    plot_distribution(all_student_preds, class_names_new, f'Student Model Predictions (Lambda={lmda_student})')\n",
    "    plot_confusion_matrix(all_labels, all_student_preds, class_names_new, f'Student Confusion Matrix (Lambda={lmda_student})')\n",
    "\n",
    "    # Print classification report for the student model\n",
    "    student_report = classification_report(all_labels, all_student_preds, target_names=class_names_new, zero_division=0)\n",
    "    print(f'Classification Report - Student Model (Lambda={lmda_student})')\n",
    "    print(student_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf516d7-2f71-4abd-b904-0e1364cca7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_variance_tradeoff(model_results, model_type, lambdas):\n",
    "    bias_values = []\n",
    "    accuracy_values = []\n",
    "\n",
    "    if model_type == 'teacher':\n",
    "        for lmda in lambdas:\n",
    "            if lmda in model_results and 'teacher_mean_abs_val_disparity' in model_results[lmda]:\n",
    "                bias_values.append(model_results[lmda]['teacher_mean_abs_val_disparity'][0])\n",
    "                performance_key = next((key for key in model_results if isinstance(key, tuple) and key[0] == lmda), None)\n",
    "                if performance_key:\n",
    "                    accuracy_values.append(model_results[performance_key]['performance_metrics']['metrics']['accuracy'][0])\n",
    "        model_name = \"Teacher\"\n",
    "    elif model_type == 'student':\n",
    "        for lmda in lambdas:\n",
    "            if lmda in model_results and 'student_mean_abs_val_disparity' in model_results[lmda]:\n",
    "                bias_values.append(model_results[lmda]['student_mean_abs_val_disparity'])\n",
    "                performance_key = next((key for key in model_results if isinstance(key, tuple) and key[1] == lmda), None)\n",
    "                if performance_key:\n",
    "                    accuracy_values.append(model_results[performance_key]['performance_metrics']['metrics']['accuracy'][1])\n",
    "        model_name = \"Student\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. Choose 'teacher' or 'student'.\")\n",
    "\n",
    "\n",
    "    # Weight for the trade-off (can be adjusted based on preference)\n",
    "    bias_weight = 1\n",
    "\n",
    "    # Calculate the weighted ratio\n",
    "    weighted_ratios = np.array(accuracy_values) / (1 + bias_weight * np.array(bias_values))\n",
    "    closest_to_one_index = np.argmin(np.abs(weighted_ratios - 1))\n",
    "    optimal_bias = bias_values[closest_to_one_index]\n",
    "    optimal_accuracy = accuracy_values[closest_to_one_index]\n",
    "    optimal_ratio = weighted_ratios[closest_to_one_index]\n",
    "\n",
    "    # Plotting the bias-variance trade-off curve\n",
    "    plt.plot(bias_values, accuracy_values, marker='o', linestyle='-', label=f'{model_name} Trade-off Points')\n",
    "\n",
    "    # Mark all points with their lambda values\n",
    "    for i, (bias, acc, lmbda) in enumerate(zip(bias_values, accuracy_values, lambdas)):\n",
    "        plt.annotate(f'λ={lmbda}', (bias, acc), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    # Highlight the optimal point\n",
    "    plt.scatter(optimal_bias, optimal_accuracy, color='r', s=100, marker='X', label=f'Optimal Point (λ={lambdas[closest_to_one_index]})')\n",
    "    plt.xlabel('Disparity')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'{model_name} Accuracy-Fairness Trade-off Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Print optimal values\n",
    "    print(f\"Optimal Lambda for {model_name}: {lambdas[closest_to_one_index]}\")\n",
    "    print(f\"Optimal Bias/Disparity for {model_name}: {optimal_bias}\")\n",
    "    print(f\"Optimal Accuracy for {model_name}: {optimal_accuracy}\")\n",
    "    print(f\"Optimal Weighted Ratio for {model_name}: {optimal_ratio:.2f}\")\n",
    "    \n",
    "# Plot for Teacher\n",
    "plot_bias_variance_tradeoff(lambda_results, 'teacher', lmda_list_teacher)\n",
    "\n",
    "# Plot for Student\n",
    "plot_bias_variance_tradeoff(lambda_results, 'student', lmda_list_student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb6ac3-3528-4d29-8379-e00360f13350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_performance_metrics_for_demo(teacher, student, dataloader):\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "\n",
    "    detailed_info = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        # Assuming gender or other attributes are part of 'target'\n",
    "        attributes = batch['target'].to(device)  \n",
    "\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs)\n",
    "            student_outputs = student(inputs)\n",
    "\n",
    "        teacher_preds = torch.argmax(teacher_outputs, dim=1)\n",
    "        student_preds = torch.argmax(student_outputs, dim=1)\n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "            if teacher_preds[i] != labels[i] and student_preds[i] == labels[i]:\n",
    "                info = {\n",
    "                    'image': inputs[i],\n",
    "                    'actual_class': labels[i].item(),\n",
    "                    'teacher_pred_class': teacher_preds[i].item(),\n",
    "                    'student_pred_class': student_preds[i].item(),\n",
    "                    'actual_attribute': attributes[i].item(),  # Modify based on your dataset\n",
    "                    # If your model also predicts attributes, include them here\n",
    "                }\n",
    "                detailed_info.append(info)\n",
    "\n",
    "    return detailed_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf82b8f-459a-4c7c-8fc1-b857b7d22c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images_with_class_filter(info_list, display_class, new_label_mapping, rows=10, cols=5):\n",
    "    # Filter the info list based on the specified class, student correct and teacher incorrect predictions\n",
    "    filtered_info = [info for info in info_list if info['actual_class'] == display_class and \n",
    "                     info['student_pred_class'] == display_class and \n",
    "                     info['teacher_pred_class'] != display_class]\n",
    "\n",
    "    # Calculate the number of images to display based on the length of the filtered list\n",
    "    num_images = len(filtered_info)\n",
    "    total_plots = min(rows * cols, num_images)  # Ensure we don't exceed the number of filtered images\n",
    "\n",
    "    # Determine the number of rows needed based on the number of images\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "\n",
    "    # Create a figure with the adjusted number of rows and columns\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in range(total_plots):\n",
    "        data = filtered_info[i]\n",
    "        image = data['image']\n",
    "        actual_class = new_label_mapping[data['actual_class']]  # Get class name from the mapping\n",
    "        teacher_pred_class = new_label_mapping[data['teacher_pred_class']]  # Get class name from the mapping\n",
    "        student_pred_class = new_label_mapping[data['student_pred_class']]  # Get class name from the mapping\n",
    "        actual_attribute = round(data['actual_attribute'], 3)\n",
    "    \n",
    "        # Normalize the image for display\n",
    "        image_display = image.cpu().numpy().transpose(1, 2, 0)\n",
    "        image_display = (image_display - image_display.min()) / (image_display.max() - image_display.min())\n",
    "    \n",
    "        # Set the title with the class and attribute information\n",
    "        title = f'Attr: {actual_attribute}\\nTrue: {actual_class}\\nTeacher: {teacher_pred_class}\\nStudent: {student_pred_class}'\n",
    "    \n",
    "        axes[i].imshow(image_display)\n",
    "        axes[i].set_title(title, fontsize=12, pad=4)\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Turn off any unused axes\n",
    "    for i in range(total_plots, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Adjust layout for clarity\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.5, hspace=0.6)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27609f-8105-4516-b9a3-fd93421f87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_path = os.path.join(output_dir, f'teacher_model_ckd_wider_lambda0.pth')\n",
    "teacher_model = torch.load(teacher_model_path)\n",
    "student_model_path = os.path.join(output_dir, f'student_model_ckd_wider_lambda4.pth')\n",
    "student_model = torch.load(student_model_path)\n",
    "\n",
    "# Get detailed info where student is correct and teacher is wrong\n",
    "detailed_info = compare_performance_metrics_for_demo(teacher_model, student_model, testloader)\n",
    "\n",
    "for i in range(num_classes):\n",
    "    # Display images with details\n",
    "    print('#'*60)\n",
    "    print(f'CLASS {i}: {new_label_mapping[i]}')\n",
    "    print('#'*60)\n",
    "    plot_images_with_class_filter(detailed_info, display_class=i, new_label_mapping=new_label_mapping, rows=10, cols=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762714a3-8447-4e96-93b3-e878e28a577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disparities_accuracies_student(lmda, dataloader):\n",
    "    model_path = os.path.join(output_dir,f'student_model_ckd_wider_lambda{lmda}.pth') \n",
    "    model = torch.load(model_path)\n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "    num_batches = 0.0\n",
    "    confusion_male = 0\n",
    "    confusion_female = 0\n",
    "    val_accuracies = []\n",
    "    val_disparities = []\n",
    "    with torch.no_grad():\n",
    "        for val_data in tqdm(testloader):\n",
    "            val_inputs = val_data['img'].to(device)\n",
    "            val_labels = val_data['label'].to(device)\n",
    "            val_targets = val_data['target'].to(device)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_student_outputs = model(val_inputs)   \n",
    "\n",
    "\n",
    "            # Compute the validation accuracy\n",
    "            _, predicted = torch.max(val_student_outputs, 1)\n",
    "            total_samples += val_labels.size(0)\n",
    "            total_correct += (predicted == val_labels).sum().item()\n",
    "            num_batches += 1\n",
    "            recall_diff = evaluate_model_with_gender_multiclass(predicted, val_labels, val_targets, num_classes=num_classes)\n",
    "            confusion_male += recall_diff[1]\n",
    "            confusion_female += recall_diff[2]\n",
    "\n",
    "        confusion_male  = confusion_male/num_batches\n",
    "        confusion_female = confusion_female/num_batches\n",
    "\n",
    "        epoch_disparity = calculate_recall_multiclass(confusion_male) - calculate_recall_multiclass(confusion_female)\n",
    "        non_zero_abs_values = np.abs(epoch_disparity[epoch_disparity != 0])\n",
    "        mean_non_zero_abs_disparity = np.mean(non_zero_abs_values)\n",
    "        val_disparities.append(mean_non_zero_abs_disparity)\n",
    "        accuracy = total_correct / total_samples\n",
    "        val_accuracies.append(accuracy)\n",
    "        print(f'Lambda: {lmda}'\n",
    "        f'*****Validation Accuracy: {accuracy * 100:.2f}%*****\\n'\n",
    "        f'*****Total Avg Disparity: {mean_non_zero_abs_disparity}*****\\n')\n",
    "        class_recall_mapping = {class_name: epoch_disparity[int(class_label)] for class_label, class_name in class_idx.items()}\n",
    "\n",
    "        # Print disparities by class label\n",
    "        for class_label, recall_diff in class_recall_mapping.items():\n",
    "            print(f\"Class {class_label}: Recall Difference = {recall_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6e9fc3-aa4e-4afb-ab41-c38bcf103d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lmda_list_student:\n",
    "    get_disparities_accuracies_student(i, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55edace1-1004-449f-a8b1-baa5ff965081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disparities_accuracies_teacher(lmda, dataloader):\n",
    "    model_path = os.path.join(output_dir,f'teacher_model_ckd_wider_lambda{lmda}.pth') \n",
    "    model = torch.load(model_path)\n",
    "    total_samples = 0\n",
    "    total_correct = 0\n",
    "    num_batches = 0.0\n",
    "    confusion_male = 0\n",
    "    confusion_female = 0\n",
    "    val_accuracies = []\n",
    "    val_disparities = []\n",
    "    with torch.no_grad():\n",
    "        for val_data in tqdm(testloader):\n",
    "            val_inputs = val_data['img'].to(device)\n",
    "            val_labels = val_data['label'].to(device)\n",
    "            val_targets = val_data['target'].to(device)\n",
    "\n",
    "            # Forward pass for validation\n",
    "            val_student_outputs = model(val_inputs)   \n",
    "\n",
    "\n",
    "            # Compute the validation accuracy\n",
    "            _, predicted = torch.max(val_student_outputs, 1)\n",
    "            total_samples += val_labels.size(0)\n",
    "            total_correct += (predicted == val_labels).sum().item()\n",
    "            num_batches += 1\n",
    "            recall_diff = evaluate_model_with_gender_multiclass(predicted, val_labels, val_targets, num_classes=num_classes)\n",
    "            confusion_male += recall_diff[1]\n",
    "            confusion_female += recall_diff[2]\n",
    "\n",
    "        confusion_male  = confusion_male/num_batches\n",
    "        confusion_female = confusion_female/num_batches\n",
    "\n",
    "        epoch_disparity = calculate_recall_multiclass(confusion_male) - calculate_recall_multiclass(confusion_female)\n",
    "        non_zero_abs_values = np.abs(epoch_disparity[epoch_disparity != 0])\n",
    "        mean_non_zero_abs_disparity = np.mean(non_zero_abs_values)\n",
    "        val_disparities.append(mean_non_zero_abs_disparity)\n",
    "        accuracy = total_correct / total_samples\n",
    "        val_accuracies.append(accuracy)\n",
    "        print(f'Lambda: {lmda}'\n",
    "        f'*****Validation Accuracy: {accuracy * 100:.2f}%*****\\n'\n",
    "        f'*****Total Avg Disparity: {mean_non_zero_abs_disparity}*****\\n')\n",
    "        class_recall_mapping = {class_name: epoch_disparity[int(class_label)] for class_label, class_name in class_idx.items()}\n",
    "\n",
    "        # Print disparities by class label\n",
    "        for class_label, recall_diff in class_recall_mapping.items():\n",
    "            print(f\"Class {class_label}: Recall Difference = {recall_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb88e3-786e-4e44-90b2-39f1c97f40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_disparities_accuracies_teacher(0, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bd8cd-9815-43e4-a4d3-465c1d545058",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "# Data for plotting\n",
    "labels = ['Teacher (λ=0)', 'Student (λ=0)', 'Student (λ=8)']\n",
    "waiter_or_waitress_values = [-0.0925, -0.1082, 0.0046]\n",
    "individual_sports_values = [-0.1208, -0.1345, -0.0794]\n",
    "\n",
    "# Number of bars\n",
    "n_bars = len(labels)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "bar_width = 0.35  # Width of the bars\n",
    "\n",
    "# Positions of the bars on the x-axis\n",
    "r1 = range(n_bars)\n",
    "r2 = [x + bar_width for x in r1]\n",
    "\n",
    "# Creating the bars with improved colors\n",
    "plt.bar(r1, waiter_or_waitress_values, color=colors[0], width=bar_width, edgecolor='grey', label='Waiter Or Waitress')\n",
    "plt.bar(r2, individual_sports_values, color=colors[1], width=bar_width, edgecolor='grey', label='Individual Sports')\n",
    "\n",
    "# Adding data labels\n",
    "for i in range(n_bars):\n",
    "    plt.text(r1[i] - bar_width/8, waiter_or_waitress_values[i], f'{waiter_or_waitress_values[i]:.4f}', ha='center', va='bottom')\n",
    "    plt.text(r2[i] - bar_width/8, individual_sports_values[i], f'{individual_sports_values[i]:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# General layout\n",
    "plt.ylabel('Disparity Values')\n",
    "plt.title('Disparity')\n",
    "plt.xticks([r + bar_width/2 for r in range(n_bars)], labels)\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaabce0-7fbe-43f7-8405-69268ff6a6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
