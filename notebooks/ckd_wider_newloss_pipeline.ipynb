{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f79206-feac-4302-abb1-d699b6354a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from models_package.models import Teacher, Student\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# new libraries\n",
    "from data.data_loader import load_cifar10, load_cifar100, load_imagenet, load_prof\n",
    "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n",
    "from torchvision.models.resnet import ResNet18_Weights, ResNet34_Weights\n",
    "from utils.loss_functions import tkd_kdloss\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "import getpass\n",
    "import os\n",
    "from PIL import Image\n",
    "import tarfile\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbaca6c-d58e-4940-8897-de1eb935a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access_key = getpass.getpass(\"Enter your access: \")\n",
    "\n",
    "# secret_key = password = getpass.getpass(\"Enter your secret: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0d4329-0939-445e-98a6-685ab0d95632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run once to get images onto EC2\n",
    "\n",
    "# wider_dir = './WIDER'\n",
    "# if not os.path.exists(wider_dir):\n",
    "#     os.makedirs(wider_dir)\n",
    "\n",
    "# # Specify your S3 bucket and file path\n",
    "# bucket_name = '210bucket'\n",
    "# s3_file_path = 'wider_attribute_image.tgz'\n",
    "\n",
    "# # Initialize an S3 filesystem\n",
    "# s3 = s3fs.S3FileSystem(key=access_key, secret=secret_key)\n",
    "\n",
    "# # Download the .tgz file from S3\n",
    "# with s3.open(f\"{bucket_name}/{s3_file_path}\", 'rb') as s3_file:\n",
    "#     with tarfile.open(fileobj=s3_file, mode=\"r:gz\") as tar:\n",
    "#         # Specify the destination directory where you want to store the extracted contents\n",
    "#         extract_dir = wider_dir # Change this to your desired directory\n",
    "#         tar.extractall(path=extract_dir)\n",
    "\n",
    "# print(\"File downloaded and extracted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ac2268-adff-4ea5-beb6-e954594bd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify your S3 bucket and directory path\n",
    "# s3_directory_path = 'wider_attribute_annotation/'\n",
    "\n",
    "# local_directory = './WIDER/Annotations'  # Change this to your desired directory\n",
    "\n",
    "# s3_files = s3.ls(f\"{bucket_name}/{s3_directory_path}\")\n",
    "\n",
    "\n",
    "# # Create the local directory if it doesn't exist\n",
    "# os.makedirs(local_directory, exist_ok=True)\n",
    "\n",
    "# # Download each file from the S3 directory to the local directory\n",
    "# for s3_file in s3_files:\n",
    "#     # Get the filename from the S3 file path\n",
    "#     filename = os.path.basename(s3_file)\n",
    "    \n",
    "#     # Download the file to the local directory\n",
    "#     local_path = os.path.join(local_directory, filename)\n",
    "#     with s3.open(s3_file, 'rb') as s3_file_obj:\n",
    "#         with open(local_path, 'wb') as local_file:\n",
    "#             local_file.write(s3_file_obj.read())\n",
    "\n",
    "# print(\"Files downloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ab952-fdf3-4abd-b899-5a7e6c68ee68",
   "metadata": {},
   "source": [
    "# Load WIDER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5caa8a7b-2a7e-4a8e-a1ae-2a3c60675232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_wider(tag, value, data_path):\n",
    "#     img_path = os.path.join(data_path, \"Image\")\n",
    "#     ann_path = os.path.join(data_path, \"Annotations\")\n",
    "#     ann_file = os.path.join(ann_path, \"wider_attribute_{}.json\".format(tag))\n",
    "\n",
    "#     data = json.load(open(ann_file, \"r\"))\n",
    "\n",
    "#     final = []\n",
    "#     image_list = data['images']\n",
    "#     for image in image_list:\n",
    "#         for person in image[\"targets\"]: # iterate over each person\n",
    "#             tmp = {}\n",
    "#             tmp['img_path'] = os.path.join(img_path, image['file_name'])\n",
    "#             tmp['bbox'] = person['bbox']\n",
    "#             attr = person[\"attribute\"]\n",
    "#             for i, item in enumerate(attr):\n",
    "#                 if item == -1:\n",
    "#                     attr[i] = 0\n",
    "#                 if item == 0:\n",
    "#                     attr[i] = value  # pad un-specified samples\n",
    "#                 if item == 1:\n",
    "#                     attr[i] = 1\n",
    "#             tmp[\"target\"] = attr\n",
    "#             final.append(tmp)\n",
    "\n",
    "#     json.dump(final, open(\"data/wider/{}_wider.json\".format(tag), \"w\"))\n",
    "#     print(\"data/wider/{}_wider.json\".format(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4aa8bdb-873b-4fed-95f6-fb79b3c14b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run once\n",
    "# if not os.path.exists(\"data/wider\"):\n",
    "#     os.makedirs(\"data/wider\")\n",
    "\n",
    "# # 0 (zero) means negative, we treat un-specified attribute as negative in the trainval set\n",
    "# make_wider(tag='trainval', value=0, data_path='WIDER') \n",
    "# make_wider(tag='test', value=99, data_path='WIDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1c84514-66c9-4543-8448-cafa751730ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self,\n",
    "                ann_files,\n",
    "                augs,\n",
    "                img_size,\n",
    "                dataset,\n",
    "                ):\n",
    "        self.dataset = dataset\n",
    "        self.ann_files = ann_files\n",
    "        self.augment = self.augs_function(augs, img_size)\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "            ] \n",
    "            # In this paper, we normalize the image data to [0, 1]\n",
    "            # You can also use the so called 'ImageNet' Normalization method\n",
    "        )\n",
    "        self.anns = []\n",
    "        self.load_anns()\n",
    "        print(self.augment)\n",
    "\n",
    "        # in wider dataset we use vit models\n",
    "        # so transformation has been changed\n",
    "        if self.dataset == \"wider\":\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "                ] \n",
    "            )        \n",
    "\n",
    "    def augs_function(self, augs, img_size):            \n",
    "        t = []\n",
    "        if 'randomflip' in augs:\n",
    "            t.append(transforms.RandomHorizontalFlip())\n",
    "        if 'ColorJitter' in augs:\n",
    "            t.append(transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0))\n",
    "        if 'resizedcrop' in augs:\n",
    "            t.append(transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0)))\n",
    "        if 'RandAugment' in augs:\n",
    "            t.append(RandAugment())\n",
    "\n",
    "        t.append(transforms.Resize((img_size, img_size)))\n",
    "\n",
    "        return transforms.Compose(t)\n",
    "    \n",
    "    def load_anns(self):\n",
    "        self.anns = []\n",
    "        for ann_file in self.ann_files:\n",
    "            json_data = json.load(open(ann_file, \"r\"))\n",
    "            self.anns += json_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self)\n",
    "        ann = self.anns[idx]\n",
    "        img = Image.open(ann[\"img_path\"]).convert(\"RGB\")\n",
    "        \n",
    "        if self.dataset == \"wider\":\n",
    "            x, y, w, h = ann['bbox']\n",
    "            img_area = img.crop([x, y, x+w, y+h])\n",
    "            img_area = self.augment(img_area)\n",
    "            img_area = self.transform(img_area)\n",
    "        \n",
    "            # Extract label from image path\n",
    "            img_path = ann['img_path']\n",
    "            label = None\n",
    "        \n",
    "            if \"WIDER/Image/train\" in img_path:\n",
    "                # For images in the \"train\" folder, extract the numeric label after \"train/\"\n",
    "                label_str = img_path.split(\"WIDER/Image/train/\")[1].split(\"/\")[0]\n",
    "                label = int(label_str.split(\"--\")[0])  # Extract the numeric part\n",
    "            elif \"WIDER/Image/test\" in img_path:\n",
    "                # For images in the \"test\" folder, extract the numeric label after \"test/\"\n",
    "                label_str = img_path.split(\"WIDER/Image/test/\")[1].split(\"/\")[0]\n",
    "                label = int(label_str.split(\"--\")[0])  # Extract the numeric part\n",
    "\n",
    "            message = {\n",
    "                \"label\": label,\n",
    "                \"target\": torch.Tensor(ann['target']),\n",
    "                \"img\": img_area\n",
    "            }\n",
    "        return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea2490c2-046a-4a19-9717-642adbabb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = ['data/wider/trainval_wider.json']\n",
    "test_file = ['data/wider/test_wider.json']\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fe2adb5-687d-4055-a108-a9c041836b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataSet(train_file, augs = ['randomflip'], img_size = 224, dataset = 'wider')\n",
    "subset_indices = range(0, 1000)  # Select indices 0 to 999\n",
    "train_dataset = Subset(train_dataset, subset_indices)\n",
    "\n",
    "test_dataset = DataSet(test_file, augs = [], img_size = 224, dataset = 'wider')\n",
    "trainloader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n",
    "testloader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e05aef27-8960-4c37-91ba-04fad3547654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'target': tensor([1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.]),\n",
       " 'img': tensor([[[-0.0275,  0.0039,  0.0510,  ..., -0.1137, -0.1294, -0.1608],\n",
       "          [-0.0118,  0.0196,  0.0510,  ..., -0.1059, -0.1294, -0.1686],\n",
       "          [-0.0196, -0.0118,  0.0039,  ..., -0.0667, -0.0980, -0.1373],\n",
       "          ...,\n",
       "          [-0.3647, -0.3569, -0.3412,  ..., -0.7333, -0.7176, -0.7020],\n",
       "          [-0.5294, -0.5137, -0.5059,  ..., -0.8196, -0.8039, -0.7804],\n",
       "          [-0.5922, -0.5922, -0.5922,  ..., -0.8588, -0.8431, -0.8196]],\n",
       " \n",
       "         [[ 0.1137,  0.0196, -0.1373,  ..., -0.0667, -0.0980, -0.1373],\n",
       "          [ 0.1529,  0.0588, -0.0980,  ..., -0.0510, -0.0980, -0.1451],\n",
       "          [ 0.1529,  0.0667, -0.0824,  ..., -0.0118, -0.0667, -0.1137],\n",
       "          ...,\n",
       "          [-0.3255, -0.3176, -0.3020,  ..., -0.7176, -0.7098, -0.7020],\n",
       "          [-0.4902, -0.4745, -0.4667,  ..., -0.8039, -0.7961, -0.7804],\n",
       "          [-0.5608, -0.5608, -0.5529,  ..., -0.8431, -0.8353, -0.8196]],\n",
       " \n",
       "         [[-0.1137, -0.1451, -0.2000,  ..., -0.1686, -0.1765, -0.2078],\n",
       "          [-0.0824, -0.1137, -0.1686,  ..., -0.1686, -0.1843, -0.2157],\n",
       "          [-0.0824, -0.1059, -0.1686,  ..., -0.1294, -0.1529, -0.1922],\n",
       "          ...,\n",
       "          [-0.3725, -0.3647, -0.3490,  ..., -0.7255, -0.7176, -0.7020],\n",
       "          [-0.5216, -0.5059, -0.4980,  ..., -0.8118, -0.7961, -0.7804],\n",
       "          [-0.5765, -0.5765, -0.5686,  ..., -0.8510, -0.8353, -0.8196]]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d18d4b-a3c0-4c5b-ad75-e74b6b1b7296",
   "metadata": {},
   "source": [
    "# Start Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1331935-ff38-4bdb-826d-6ae5bfe51ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_recall(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp > 0 else 1e-6\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49c941e9-954c-4d7b-932e-06d3297c02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    inputs, _ = next(iter(dataloader))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher(inputs)\n",
    "    teacher_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        student_outputs = student(inputs)\n",
    "    student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n",
    "\n",
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs.to(device))\n",
    "            student_outputs = student(inputs.to(device))\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(torch.argmax(teacher_outputs, dim=1).cpu().numpy())\n",
    "        all_student_preds.append(torch.argmax(student_outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),  # Updated line\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def plot_comparison(labels, teacher_values, student_values, title, ylabel):\n",
    "    # Convert parameter count to millions\n",
    "    if 'Parameter Count' in title or 'Parameter Count' in ylabel:\n",
    "        teacher_values = [value / 1e6 for value in teacher_values]\n",
    "        student_values = [value / 1e6 for value in student_values]\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, teacher_values, width, label='Teacher')\n",
    "    rects2 = ax.bar(x + width/2, student_values, width, label='Student')\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e8e4cb8-8eae-4aff-ba07-b22fc5920dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01379 # 0.096779\n",
    "num_epochs = 3  # 200\n",
    "num_workers = 2\n",
    "batch_size = 256\n",
    "temperature = 4.0\n",
    "alpha = 0.9\n",
    "momentum = 0.9\n",
    "num_classes = 61\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "beta = 0.5\n",
    "\n",
    "# new parameters\n",
    "# lr_input = 0.1\n",
    "# momentum_input = 0.9\n",
    "weight_decay_input = 5e-4\n",
    "# epochs = 20\n",
    "# T = 4.0 # temperatureture\n",
    "# alpha = 0.9\n",
    "patience = 5  # for early stopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4102246d-d7c8-4230-ae3b-735081a608eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load IdenProf dataset\n",
    "# train_path = '/home/ubuntu/capstone/W210-Capstone/notebooks/idenprof/train'\n",
    "# test_path = '/home/ubuntu/capstone/W210-Capstone/notebooks/idenprof/test'\n",
    "# trainloader, testloader  = load_prof(train_path, test_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c6b93cb-37c9-448f-a8d9-424d3c4d40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "###################### Testing 1 ######################\n",
    "# Create instances of your models\n",
    "teacher_model = torchvision.models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1).cuda()\n",
    "teacher_model.eval()  # Set teacher model to evaluation mode\n",
    "student_model = torchvision.models.resnet18(weights=None).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dc75db2-db0a-4f48-bd40-215fc135d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate the models\n",
    "# ###################### Testing 2 ######################\n",
    "# # Create instances of your models\n",
    "# teacher_model = Teacher()\n",
    "# teacher_model.eval()  # Set teacher model to evaluation mode\n",
    "# student_model = Student()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7b4731c-f105-4192-9650-6587c46f6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler for the student model\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Optimizer and scheduler for the teacher model\n",
    "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "teacher_scheduler = torch.optim.lr_scheduler.StepLR(teacher_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Assuming the device is a CUDA device if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "477780cd-e318-4e47-a35d-103b6e4506ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RecallDifferenceLoss(nn.Module):\n",
    "    def __init__(self, num_classes, num_attributes, weight_recall=1.0):\n",
    "        super(RecallDifferenceLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_attributes = num_attributes\n",
    "        self.weight_recall = weight_recall\n",
    "\n",
    "    def forward(self, output, label, attributes):\n",
    "        # Initialize the loss\n",
    "        loss = 0.0\n",
    "\n",
    "        for attribute_value in range(self.num_attributes):\n",
    "            # Select images with the specified attribute value for each image in the batch\n",
    "            mask = (attributes.unsqueeze(1) == attribute_value).to(torch.float32)\n",
    "\n",
    "            # Calculate the number of true positives and total positives for the selected images\n",
    "            true_positives = torch.sum((output.argmax(1) == label) * mask, dim=0)\n",
    "            total_positives = torch.sum(mask, dim=0)\n",
    "\n",
    "            # Calculate recall (true positives / total positives) with smoothing to avoid division by zero\n",
    "            recall = (true_positives + 1e-5) / (total_positives + 1e-5)\n",
    "\n",
    "            # Compute the recall difference between images with different attributes\n",
    "            recall_diff = 1.0 - recall\n",
    "\n",
    "            # Accumulate the loss\n",
    "            loss += self.weight_recall * recall_diff\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "recall_difference_loss = RecallDifferenceLoss(num_classes, num_attributes=14, weight_recall=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3b7719f-4d65-4601-8e1d-1015de2201cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### finding the optimal learning rate\n",
    "# def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, num_epochs=5, lr_range=(1e-4, 1e-1), plot_loss=True):\n",
    "#     model.train()\n",
    "#     model.to(device)\n",
    "#     lr_values = np.logspace(np.log10(lr_range[0]), np.log10(lr_range[1]), num_epochs * len(trainloader))  # Generate learning rates for each batch\n",
    "#     lr_iter = iter(lr_values)\n",
    "#     losses = []\n",
    "#     lrs = []\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         for i, (inputs, labels, annotation) in enumerate(tqdm(trainloader)):\n",
    "#             lr = next(lr_iter)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = lr  # Set new learning rate\n",
    "            \n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             losses.append(loss.item())\n",
    "#             lrs.append(lr)\n",
    "    \n",
    "#     # Calculate the derivative of the loss\n",
    "#     loss_derivative = np.gradient(losses)\n",
    "    \n",
    "#     # Find the learning rate corresponding to the minimum derivative (steepest decline)\n",
    "#     best_lr_index = np.argmin(loss_derivative)\n",
    "#     best_lr = lrs[best_lr_index]\n",
    "    \n",
    "#     if plot_loss:\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.figure()\n",
    "#         plt.plot(lrs, losses)\n",
    "#         plt.xscale('log')\n",
    "#         plt.xlabel('Learning Rate')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title('Learning Rate Range Test')\n",
    "#         plt.axvline(x=best_lr, color='red', linestyle='--', label=f'Best LR: {best_lr}')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "    \n",
    "#     print(f'Best learning rate: {best_lr}')\n",
    "#     return best_lr\n",
    "\n",
    "# ############# input ############## \n",
    "# batch_size = 16  #to find the optimal learning rate\n",
    "# best_lr = train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs=3)  \n",
    "# print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "286386b2-d6e9-447e-95b5-b3dd8f18010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the teacher model\n",
    "def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, num_epochs=1, patience=5):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    best_train_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "            inputs = data['img'].cuda()\n",
    "            labels = data['label'].cuda()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            if index % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        epoch_loss /= num_batches  \n",
    "        \n",
    "        # Check for early stopping\n",
    "        if epoch_loss < best_train_loss:\n",
    "            best_train_loss = epoch_loss\n",
    "            patience_counter = 0 \n",
    "            # checkpoint\n",
    "            torch.save(model.state_dict(), f'teacher_model_weights_ckd_prof_checkpoint.pth')\n",
    "            torch.save(model, f'teacher_model_ckd_prof_checkpoint.pth')\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Finished Training Teacher\")\n",
    "\n",
    "\n",
    "# Function to train the student model with knowledge distillation\n",
    "def train_student_with_distillation_disparity(student, teacher, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs, patience=5):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    student.to(device)\n",
    "    teacher.to(device)\n",
    "    best_train_loss = float('inf')  \n",
    "    patience_counter = 0 \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        epoch_disparity = 0.0\n",
    "        running_recall = 0.0\n",
    "\n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "            inputs = data['img'].cuda()\n",
    "            labels = data['label'].cuda()\n",
    "            annot = data['target'].cuda()\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student(inputs)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "            \n",
    "            ce_loss = criterion(student_outputs, labels)\n",
    "            kd_loss = tkd_kdloss(student_outputs, teacher_outputs, temperature=temperature)  # from utils.loss_functions\n",
    "        \n",
    "            # Calculate the recall difference loss\n",
    "            recall_difference = recall_difference_loss(student_outputs, label=labels, attributes=annot)\n",
    "        \n",
    "            # Combine the losses\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss + beta * recall_difference\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "            if index % 100 == 99:  \n",
    "                print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "                running_loss = 0.0\n",
    "    \n",
    "        epoch_loss /= num_batches\n",
    "\n",
    "        # Check for early stopping\n",
    "        if epoch_loss < best_train_loss:\n",
    "            best_train_loss = epoch_loss\n",
    "            patience_counter = 0 \n",
    "            torch.save(student.state_dict(), f'student_model_weights_ckd_prof_checkpoint.pth')\n",
    "            torch.save(student, f'student_model_ckd_prof_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break  \n",
    "\n",
    "        scheduler.step() \n",
    "\n",
    "    print(\"Finished Training Student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "540397f4-0bb9-4f55-88a5-bdd7671a913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 4/4 [00:07<00:00,  1.88s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:07<00:00,  1.80s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:07<00:00,  1.80s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:07<00:00,  1.79s/it]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:07<00:00,  1.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Teacher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/4 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (14) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Call the function to train the student model with knowledge distillation\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain_student_with_distillation_disparity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 79\u001b[0m, in \u001b[0;36mtrain_student_with_distillation_disparity\u001b[0;34m(student, teacher, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs, patience)\u001b[0m\n\u001b[1;32m     76\u001b[0m kd_loss \u001b[38;5;241m=\u001b[39m tkd_kdloss(student_outputs, teacher_outputs, temperature\u001b[38;5;241m=\u001b[39mtemperature)  \u001b[38;5;66;03m# from utils.loss_functions\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Calculate the recall difference loss\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m recall_difference \u001b[38;5;241m=\u001b[39m \u001b[43mrecall_difference_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Combine the losses\u001b[39;00m\n\u001b[1;32m     82\u001b[0m loss \u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m kd_loss \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha) \u001b[38;5;241m*\u001b[39m ce_loss \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m recall_difference\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 21\u001b[0m, in \u001b[0;36mRecallDifferenceLoss.forward\u001b[0;34m(self, output, label, attributes)\u001b[0m\n\u001b[1;32m     18\u001b[0m mask \u001b[38;5;241m=\u001b[39m (attributes\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m attribute_value)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Calculate the number of true positives and total positives for the selected images\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m true_positives \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     22\u001b[0m total_positives \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(mask, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Calculate recall (true positives / total positives) with smoothing to avoid division by zero\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (14) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# Assuming the device is a CUDA device if available\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Call the function to train the teacher model\n",
    "train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs=num_epochs)\n",
    "\n",
    "# Call the function to train the student model with knowledge distillation\n",
    "train_student_with_distillation_disparity(student_model, teacher_model, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca08a84f-2f5d-4a5b-8c0e-d77a18802260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c30aad75-7eb3-4a59-b1ac-dc3582cfa65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student weights and architecture saved and exported\n",
      "teacher weights and architecture saved and exported\n"
     ]
    }
   ],
   "source": [
    "###################### Testing 1 ######################\n",
    "# Save the student and teacher model weights and architecture\n",
    "torch.save(student_model.state_dict(), 'student_model_weights_ckd_prof.pth')\n",
    "torch.save(student_model, 'student_model_ckd_prof.pth')\n",
    "print('student weights and architecture saved and exported')\n",
    "\n",
    "torch.save(teacher_model.state_dict(), 'teacher_model_weights_ckd_prof.pth')\n",
    "torch.save(teacher_model, 'teacher_model_ckd_prof.pth')\n",
    "print('teacher weights and architecture saved and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e29951-1e09-4d6d-8d04-15897dd00723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################### Testing 2 ######################\n",
    "# # Save the student and teacher model weights and architecture\n",
    "# torch.save(student_model.state_dict(), 'student_model_weights_ckd_2.pth')\n",
    "# torch.save(student_model, 'student_model_ckd_2.pth')\n",
    "# print('weights and architecture saved and exported')\n",
    "\n",
    "# torch.save(teacher_model.state_dict(), 'teacher_model_weights_ckd_2.pth')\n",
    "# torch.save(teacher_model, 'teacher_model_ckd_2.pth')\n",
    "# print('teacher weights and architecture saved and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995827c-1726-44e2-89aa-5f346a3d4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the comparison and plotting functions after training\n",
    "teacher_params, student_params = compare_model_size(teacher_model, student_model)\n",
    "teacher_time, student_time = compare_inference_time(teacher_model, student_model, trainloader)\n",
    "performance_metrics = compare_performance_metrics(teacher_model, student_model, trainloader)\n",
    "\n",
    "# Extracting the metric values for plotting\n",
    "performance_labels = ['accuracy', 'precision', 'recall', 'f1']\n",
    "teacher_performance_values = [performance_metrics[metric][0] for metric in performance_labels]\n",
    "student_performance_values = [performance_metrics[metric][1] for metric in performance_labels]\n",
    "\n",
    "# Plotting the comparison for performance metrics\n",
    "plot_comparison(performance_labels, teacher_performance_values, student_performance_values, 'Performance Comparison', 'Score')\n",
    "\n",
    "# Plotting the comparison for model size\n",
    "model_size_labels = ['Model Size']\n",
    "teacher_model_size_values = [teacher_params]\n",
    "student_model_size_values = [student_params]\n",
    "plot_comparison(model_size_labels, teacher_model_size_values, student_model_size_values, 'Model Size Comparison', 'Parameter Count (millions)')\n",
    "\n",
    "# Plotting the comparison for inference time\n",
    "inference_time_labels = ['Inference Time']\n",
    "teacher_inference_time_values = [teacher_time]\n",
    "student_inference_time_values = [student_time]\n",
    "plot_comparison(inference_time_labels, teacher_inference_time_values, student_inference_time_values, 'Inference Time Comparison', 'Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4fbd0-9771-4790-ae5a-f576a7132fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9859d-e3e5-401a-afd5-cd8bbbed5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "teacher_model = torchvision.models.resnet34(weights=None)\n",
    "\n",
    "weights_path = 'teacher_model_weights_ckd_prof.pth'\n",
    "\n",
    "teacher_model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "student_model = torchvision.models.resnet18(weights=None)\n",
    "\n",
    "weights_path = 'student_model_weights_ckd_prof.pth'\n",
    "\n",
    "student_model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "# Call the comparison and plotting functions after training\n",
    "teacher_params, student_params = compare_model_size(teacher_model, student_model)\n",
    "teacher_time, student_time = compare_inference_time(teacher_model, student_model, testloader)\n",
    "performance_metrics = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "\n",
    "# Extracting the metric values for plotting\n",
    "performance_labels = ['accuracy', 'precision', 'recall', 'f1']\n",
    "teacher_performance_values = [performance_metrics[metric][0] for metric in performance_labels]\n",
    "student_performance_values = [performance_metrics[metric][1] for metric in performance_labels]\n",
    "\n",
    "# Plotting the comparison for performance metrics\n",
    "plot_comparison(performance_labels, teacher_performance_values, student_performance_values, 'Performance Comparison', 'Score')\n",
    "\n",
    "# Plotting the comparison for model size\n",
    "model_size_labels = ['Model Size']\n",
    "teacher_model_size_values = [teacher_params]\n",
    "student_model_size_values = [student_params]\n",
    "plot_comparison(model_size_labels, teacher_model_size_values, student_model_size_values, 'Model Size Comparison', 'Parameter Count (millions)')\n",
    "\n",
    "# Plotting the comparison for inference time\n",
    "inference_time_labels = ['Inference Time']\n",
    "teacher_inference_time_values = [teacher_time]\n",
    "student_inference_time_values = [student_time]\n",
    "plot_comparison(inference_time_labels, teacher_inference_time_values, student_inference_time_values, 'Inference Time Comparison', 'Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f197092-08e8-402e-86da-e78278727f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i, (image, label, annot) in enumerate(trainloader): \n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a15892-747e-40ed-9d6a-ed5160b58757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
