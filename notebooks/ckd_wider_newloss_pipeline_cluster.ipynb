{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f79206-feac-4302-abb1-d699b6354a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import tarfile\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import s3fs\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data.data_loader import load_cifar10, load_cifar100, load_imagenet, load_prof\n",
    "from utils.loss_functions import tkd_kdloss\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck, ResNet18_Weights, ResNet34_Weights\n",
    "\n",
    "from models_package.models import Teacher, Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbaca6c-d58e-4940-8897-de1eb935a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access_key = getpass.getpass(\"Enter your access: \")\n",
    "\n",
    "# secret_key = password = getpass.getpass(\"Enter your secret: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0d4329-0939-445e-98a6-685ab0d95632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run once to get images onto EC2\n",
    "\n",
    "# wider_dir = './WIDER'\n",
    "# if not os.path.exists(wider_dir):\n",
    "#     os.makedirs(wider_dir)\n",
    "\n",
    "# # Specify your S3 bucket and file path\n",
    "# bucket_name = '210bucket'\n",
    "# s3_file_path = 'wider_attribute_image.tgz'\n",
    "\n",
    "# # Initialize an S3 filesystem\n",
    "# s3 = s3fs.S3FileSystem(key=access_key, secret=secret_key)\n",
    "\n",
    "# # Download the .tgz file from S3\n",
    "# with s3.open(f\"{bucket_name}/{s3_file_path}\", 'rb') as s3_file:\n",
    "#     with tarfile.open(fileobj=s3_file, mode=\"r:gz\") as tar:\n",
    "#         # Specify the destination directory where you want to store the extracted contents\n",
    "#         extract_dir = wider_dir # Change this to your desired directory\n",
    "#         tar.extractall(path=extract_dir)\n",
    "\n",
    "# print(\"File downloaded and extracted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ac2268-adff-4ea5-beb6-e954594bd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify your S3 bucket and directory path\n",
    "# s3_directory_path = 'wider_attribute_annotation/'\n",
    "\n",
    "# local_directory = './WIDER/Annotations'  # Change this to your desired directory\n",
    "\n",
    "# s3_files = s3.ls(f\"{bucket_name}/{s3_directory_path}\")\n",
    "\n",
    "\n",
    "# # Create the local directory if it doesn't exist\n",
    "# os.makedirs(local_directory, exist_ok=True)\n",
    "\n",
    "# # Download each file from the S3 directory to the local directory\n",
    "# for s3_file in s3_files:\n",
    "#     # Get the filename from the S3 file path\n",
    "#     filename = os.path.basename(s3_file)\n",
    "    \n",
    "#     # Download the file to the local directory\n",
    "#     local_path = os.path.join(local_directory, filename)\n",
    "#     with s3.open(s3_file, 'rb') as s3_file_obj:\n",
    "#         with open(local_path, 'wb') as local_file:\n",
    "#             local_file.write(s3_file_obj.read())\n",
    "\n",
    "# print(\"Files downloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ab952-fdf3-4abd-b899-5a7e6c68ee68",
   "metadata": {},
   "source": [
    "# Load WIDER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5caa8a7b-2a7e-4a8e-a1ae-2a3c60675232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_wider(tag, value, data_path):\n",
    "#     img_path = os.path.join(data_path, \"Image\")\n",
    "#     ann_path = os.path.join(data_path, \"Annotations\")\n",
    "#     ann_file = os.path.join(ann_path, \"wider_attribute_{}.json\".format(tag))\n",
    "\n",
    "#     data = json.load(open(ann_file, \"r\"))\n",
    "\n",
    "#     final = []\n",
    "#     image_list = data['images']\n",
    "#     for image in image_list:\n",
    "#         for person in image[\"targets\"]: # iterate over each person\n",
    "#             tmp = {}\n",
    "#             tmp['img_path'] = os.path.join(img_path, image['file_name'])\n",
    "#             tmp['bbox'] = person['bbox']\n",
    "#             attr = person[\"attribute\"]\n",
    "#             for i, item in enumerate(attr):\n",
    "#                 if item == -1:\n",
    "#                     attr[i] = 0\n",
    "#                 if item == 0:\n",
    "#                     attr[i] = value  # pad un-specified samples\n",
    "#                 if item == 1:\n",
    "#                     attr[i] = 1\n",
    "#             tmp[\"target\"] = attr\n",
    "#             final.append(tmp)\n",
    "\n",
    "#     json.dump(final, open(\"data/wider/{}_wider.json\".format(tag), \"w\"))\n",
    "#     print(\"data/wider/{}_wider.json\".format(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4aa8bdb-873b-4fed-95f6-fb79b3c14b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run once\n",
    "# if not os.path.exists(\"data/wider\"):\n",
    "#     os.makedirs(\"data/wider\")\n",
    "\n",
    "# # 0 (zero) means negative, we treat un-specified attribute as negative in the trainval set\n",
    "# make_wider(tag='trainval', value=0, data_path='WIDER') \n",
    "# make_wider(tag='test', value=99, data_path='WIDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc3c0ec8-98dc-4933-b9a3-e96fce39c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01379 # 0.096779\n",
    "num_epochs = 2  # 200\n",
    "num_workers = 2\n",
    "temperature = 4.0\n",
    "alpha = 0.9\n",
    "momentum = 0.9\n",
    "num_classes = 61\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "beta = 0.5\n",
    "\n",
    "# new parameters\n",
    "# lr_input = 0.1\n",
    "# momentum_input = 0.9\n",
    "weight_decay_input = 5e-4\n",
    "# epochs = 20\n",
    "# T = 4.0 # temperatureture\n",
    "# alpha = 0.9\n",
    "patience = 5  # for early stopping\n",
    "\n",
    "batch_size = 256\n",
    "num_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2770eac-10b7-47de-ba06-9aa63cd7ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c84514-66c9-4543-8448-cafa751730ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self,\n",
    "                ann_files,\n",
    "                augs,\n",
    "                img_size,\n",
    "                dataset,\n",
    "                ):\n",
    "        self.dataset = dataset\n",
    "        self.ann_files = ann_files\n",
    "        self.augment = self.augs_function(augs, img_size)\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "            ] \n",
    "            # In this paper, we normalize the image data to [0, 1]\n",
    "            # You can also use the so called 'ImageNet' Normalization method\n",
    "        )\n",
    "        self.anns = []\n",
    "        self.load_anns()\n",
    "        print(self.augment)\n",
    "\n",
    "        # in wider dataset we use vit models\n",
    "        # so transformation has been changed\n",
    "        if self.dataset == \"wider\":\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "                ] \n",
    "            )        \n",
    "\n",
    "    def augs_function(self, augs, img_size):            \n",
    "        t = []\n",
    "        if 'randomflip' in augs:\n",
    "            t.append(transforms.RandomHorizontalFlip())\n",
    "        if 'ColorJitter' in augs:\n",
    "            t.append(transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0))\n",
    "        if 'resizedcrop' in augs:\n",
    "            t.append(transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0)))\n",
    "        if 'RandAugment' in augs:\n",
    "            t.append(RandAugment())\n",
    "\n",
    "        t.append(transforms.Resize((img_size, img_size)))\n",
    "\n",
    "        return transforms.Compose(t)\n",
    "    \n",
    "    def load_anns(self):\n",
    "        self.anns = []\n",
    "        for ann_file in self.ann_files:\n",
    "            json_data = json.load(open(ann_file, \"r\"))\n",
    "            self.anns += json_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Make sure the index is within bounds\n",
    "        idx = idx % len(self)\n",
    "        ann = self.anns[idx]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to open the image file\n",
    "            img = Image.open(ann[\"img_path\"]).convert(\"RGB\")\n",
    "\n",
    "            # If this is the wider dataset, proceed with specific processing\n",
    "            if self.dataset == \"wider\":\n",
    "                x, y, w, h = ann['bbox']\n",
    "                img_area = img.crop([x, y, x+w, y+h])\n",
    "                img_area = self.augment(img_area)\n",
    "                img_area = self.transform(img_area)\n",
    "                \n",
    "                # Extract label from image path\n",
    "                img_path = ann['img_path']\n",
    "                label = self.extract_label(img_path)  # You might need to implement this method\n",
    "                \n",
    "                return {\n",
    "                    \"label\": label,\n",
    "                    \"target\": torch.Tensor(ann['target']),\n",
    "                    \"img\": img_area\n",
    "                }\n",
    "            \n",
    "            # Else, add handling for other dataset types\n",
    "            else:\n",
    "                # Handle other dataset types here\n",
    "                pass\n",
    "\n",
    "        except Exception as e:\n",
    "            # If any error occurs during the processing of an image, log the error and the index\n",
    "            print(f\"Error processing image at index {idx}: {e}\")\n",
    "            # Instead of returning None, raise the exception\n",
    "            raise\n",
    "\n",
    "    def extract_label(self, img_path):\n",
    "        # Implement this method based on how you need to extract labels\n",
    "        # This is just a placeholder implementation\n",
    "        label = 0\n",
    "        # Extract the label from the image path\n",
    "        if \"WIDER/Image/train\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/train/\")[1].split(\"/\")[0]\n",
    "            label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/test\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/test/\")[1].split(\"/\")[0]\n",
    "            label = int(label_str.split(\"--\")[0])\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea2490c2-046a-4a19-9717-642adbabb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = ['data/wider/trainval_wider.json']\n",
    "test_file = ['data/wider/test_wider.json']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "012d6bd8-61d5-4a20-845b-689234718508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # Filter out any None items in the batch\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    # If after filtering the batch is empty, handle this case by either returning an empty tensor or raising an exception\n",
    "    if len(batch) == 0:\n",
    "        # Option 1: Return a placeholder tensor (adapt the shape to match your data)\n",
    "        # return torch.tensor([]), torch.tensor([])\n",
    "        # Option 2: Raise an exception\n",
    "        raise ValueError(\"Batch is empty after filtering out None items.\")\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fe2adb5-687d-4055-a108-a9c041836b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataSet(train_file, augs = ['randomflip'], img_size = 224, dataset = 'wider')\n",
    "# subset_indices = range(0, 1000)  # Select indices 0 to 999\n",
    "# train_dataset = Subset(train_dataset, subset_indices)\n",
    "\n",
    "test_dataset = DataSet(test_file, augs = [], img_size = 224, dataset = 'wider')\n",
    "\n",
    "for i in range(len(train_dataset)):\n",
    "    item = train_dataset[i]\n",
    "    if item is None:\n",
    "        print(f\"None item found at index: {i}\")\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=custom_collate)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e05aef27-8960-4c37-91ba-04fad3547654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset - Total Samples: 28345, Unique Labels: {0, 1, 3, 4, 6, 7, 11, 15, 17, 18, 19, 20, 22, 25, 27, 28, 30, 31, 33, 35, 36, 37, 39, 43, 44, 50, 51, 54, 57, 58}\n"
     ]
    }
   ],
   "source": [
    "def print_dataset_details(dataset):\n",
    "    unique_labels = set()\n",
    "    for i in range(len(dataset)):\n",
    "        item = dataset[i]\n",
    "        unique_labels.add(item['label'])\n",
    "    print(f\"Dataset - Total Samples: {len(dataset)}, Unique Labels: {unique_labels}\")\n",
    "\n",
    "print_dataset_details(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7fa8326-5c8a-4b70-92a8-2a553f065ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=3)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Assuming your dataset is named 'my_dataset'\n",
    "# Assuming 'img' is a tensor of shape (3, H, W) for each sample\n",
    "\n",
    "# Function to flatten the image tensors\n",
    "def flatten_images(images):\n",
    "    return images.view(images.size(0), -1).numpy()\n",
    "\n",
    "# Parameters\n",
    "num_clusters = 20\n",
    "batch_size = 32  # Adjust as needed\n",
    "\n",
    "# Create a DataLoader for your original dataset\n",
    "\n",
    "# Initialize MiniBatchKMeans\n",
    "kmeans = MiniBatchKMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "# Initialize an empty list to store the cluster labels\n",
    "cluster_labels = []\n",
    "# Process the dataset in batches\n",
    "for batch in trainloader:\n",
    "    img_batch = batch['img']\n",
    "    flattened_images = flatten_images(img_batch)\n",
    "\n",
    "    # Partial fit on the batch\n",
    "    cluster_labels.extend(kmeans.fit_predict(flattened_images))\n",
    "\n",
    "\n",
    "\n",
    "# Add the cluster labels to the dataset\n",
    "for i, sample in enumerate(my_dataset):\n",
    "    sample['cluster_label'] = int(cluster_labels[i])\n",
    "\n",
    "# Create a new DataLoader based on the clustered data\n",
    "class ClusteredDataset(Dataset):\n",
    "    def __init__(self, dataset, cluster_labels):\n",
    "        self.dataset = dataset\n",
    "        self.cluster_labels = cluster_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.dataset[idx]\n",
    "        cluster_label = self.cluster_labels[idx]\n",
    "        sample['cluster_label'] = cluster_label\n",
    "        return sample\n",
    "\n",
    "# Create a new dataset instance with clustered labels\n",
    "clustered_dataset = ClusteredDataset(my_dataset, cluster_labels)\n",
    "\n",
    "# Create a new dataloader for the clustered dataset\n",
    "clustered_dataloader = DataLoader(clustered_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aba353c-f2a3-4a82-a272-49dafe3362d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to display images and their labels\n",
    "def show_samples_comparison(original_dataset, clustered_dataset, dataloader, num_samples=5):\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, 2*num_samples))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Get the original indices from the first batch\n",
    "    original_indices = [batch['idx'] for batch in dataloader]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Original sample\n",
    "        original_sample = original_dataset[original_indices[i]]\n",
    "        original_img = original_sample['img'].permute(1, 2, 0).numpy()\n",
    "        axes[i, 0].imshow(original_img)\n",
    "        axes[i, 0].set_title(f\"Original Label: {original_sample['label']}\")\n",
    "\n",
    "        # Clustered sample\n",
    "        clustered_sample = clustered_dataset[original_indices[i]]\n",
    "        clustered_img = clustered_sample['img'].permute(1, 2, 0).numpy()\n",
    "        axes[i, 1].imshow(clustered_img)\n",
    "        axes[i, 1].set_title(f\"Cluster Label: {clustered_sample['cluster_label']}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Create a new DataLoader for the original dataset to get indices\n",
    "original_dataloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=False)\n",
    "original_indices = [batch['idx'] for batch in original_dataloader]\n",
    "\n",
    "# Set the 'idx' field for each sample in the original dataset\n",
    "for i, sample in enumerate(my_dataset):\n",
    "    sample['idx'] = i\n",
    "\n",
    "# Show samples\n",
    "show_samples_comparison(my_dataset, clustered_dataset, original_dataloader, num_samples=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d18d4b-a3c0-4c5b-ad75-e74b6b1b7296",
   "metadata": {},
   "source": [
    "# Start Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12792eb1-3b7c-40ba-ad6d-20de9a363770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_recall(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp > 0 else 1e-6\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c941e9-954c-4d7b-932e-06d3297c02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    data = next(dataiter)\n",
    "    inputs = data['img']\n",
    "    \n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher(inputs)\n",
    "    teacher_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        student_outputs = student(inputs)\n",
    "    student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n",
    "\n",
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs)\n",
    "            student_outputs = student(inputs)\n",
    "            \n",
    "        teacher_preds = torch.argmax(teacher_outputs, dim=1).cpu().numpy()\n",
    "        student_preds = torch.argmax(student_outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Debug: Check if all predictions are the same\n",
    "        print(\"Teacher predictions:\", np.unique(teacher_preds))\n",
    "        print(\"Student predictions:\", np.unique(student_preds))\n",
    "        \n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(teacher_preds)\n",
    "        all_student_preds.append(student_preds)\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    # Debug: Check final label distribution\n",
    "    print(\"Label distribution:\", np.unique(all_labels, return_counts=True))\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def plot_comparison(labels, teacher_values, student_values, title, ylabel):\n",
    "    # Convert parameter count to millions\n",
    "    if 'Parameter Count' in title or 'Parameter Count' in ylabel:\n",
    "        teacher_values = [value / 1e6 for value in teacher_values]\n",
    "        student_values = [value / 1e6 for value in student_values]\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, teacher_values, width, label='Teacher')\n",
    "    rects2 = ax.bar(x + width/2, student_values, width, label='Student')\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4102246d-d7c8-4230-ae3b-735081a608eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load IdenProf dataset\n",
    "# train_path = '/home/ubuntu/capstone/W210-Capstone/notebooks/idenprof/train'\n",
    "# test_path = '/home/ubuntu/capstone/W210-Capstone/notebooks/idenprof/test'\n",
    "# trainloader, testloader  = load_prof(train_path, test_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6b93cb-37c9-448f-a8d9-424d3c4d40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "###################### Testing 1 ######################\n",
    "# Create instances of your models\n",
    "teacher_model = torchvision.models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1).to(device)\n",
    "teacher_model.eval()  # Set teacher model to evaluation mode\n",
    "student_model = torchvision.models.resnet18(weights=None).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4731c-f105-4192-9650-6587c46f6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler for the student model\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Optimizer and scheduler for the teacher model\n",
    "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "teacher_scheduler = torch.optim.lr_scheduler.StepLR(teacher_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477780cd-e318-4e47-a35d-103b6e4506ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_disparity_loss(outputs, targets, attributes, num_classes):\n",
    "    \"\"\"\n",
    "    Compute the recall disparity loss.\n",
    "\n",
    "    :param outputs: Tensor of shape (batch_size, num_classes), model's class probabilities or logits.\n",
    "    :param targets: Tensor of shape (batch_size,), true class indices.\n",
    "    :param attributes: Tensor of shape (batch_size, num_attributes), binary attributes for each instance.\n",
    "    :param num_classes: int, number of classes.\n",
    "    \"\"\"\n",
    "    # Ensure we're working with probabilities\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "    # Initialize tensors to store recall for when attributes are present and absent\n",
    "    recall_when_present = torch.zeros(num_classes, attributes.size(1))\n",
    "    recall_when_absent = torch.zeros(num_classes, attributes.size(1))\n",
    "\n",
    "    # Calculate recall for each class based on attributes being present or absent\n",
    "    for class_idx in range(num_classes):\n",
    "        for attr_idx in range(attributes.size(1)):\n",
    "            # Indices of instances with the current class and attribute present/absent\n",
    "            class_and_attr_present = (targets == class_idx) & (attributes[:, attr_idx] == 1)\n",
    "            class_and_attr_absent = (targets == class_idx) & (attributes[:, attr_idx] == 0)\n",
    "\n",
    "            # True positives for current class when attribute is present/absent\n",
    "            true_positive_present = ((preds == class_idx) & class_and_attr_present).sum().float()\n",
    "            true_positive_absent = ((preds == class_idx) & class_and_attr_absent).sum().float()\n",
    "\n",
    "            # Condition positives for current class when attribute is present/absent\n",
    "            condition_positive_present = class_and_attr_present.sum().float()\n",
    "            condition_positive_absent = class_and_attr_absent.sum().float()\n",
    "\n",
    "            # Recall for current class when attribute is present/absent\n",
    "            recall_when_present[class_idx, attr_idx] = true_positive_present / (condition_positive_present + 1e-8)\n",
    "            recall_when_absent[class_idx, attr_idx] = true_positive_absent / (condition_positive_absent + 1e-8)\n",
    "\n",
    "    # Disparity is the absolute difference between recall when attribute is present and when it's absent\n",
    "    disparity = (recall_when_present - recall_when_absent).abs()\n",
    "\n",
    "    # Final loss is the mean of disparity across all classes and attributes\n",
    "    loss = disparity.mean()\n",
    "\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7719f-4d65-4601-8e1d-1015de2201cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### finding the optimal learning rate\n",
    "# def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, num_epochs=5, lr_range=(1e-4, 1e-1), plot_loss=True):\n",
    "#     model.train()\n",
    "#     model.to(device)\n",
    "#     lr_values = np.logspace(np.log10(lr_range[0]), np.log10(lr_range[1]), num_epochs * len(trainloader))  # Generate learning rates for each batch\n",
    "#     lr_iter = iter(lr_values)\n",
    "#     losses = []\n",
    "#     lrs = []\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         for i, (inputs, labels, annotation) in enumerate(tqdm(trainloader)):\n",
    "#             lr = next(lr_iter)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = lr  # Set new learning rate\n",
    "            \n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             losses.append(loss.item())\n",
    "#             lrs.append(lr)\n",
    "    \n",
    "#     # Calculate the derivative of the loss\n",
    "#     loss_derivative = np.gradient(losses)\n",
    "    \n",
    "#     # Find the learning rate corresponding to the minimum derivative (steepest decline)\n",
    "#     best_lr_index = np.argmin(loss_derivative)\n",
    "#     best_lr = lrs[best_lr_index]\n",
    "    \n",
    "#     if plot_loss:\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.figure()\n",
    "#         plt.plot(lrs, losses)\n",
    "#         plt.xscale('log')\n",
    "#         plt.xlabel('Learning Rate')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title('Learning Rate Range Test')\n",
    "#         plt.axvline(x=best_lr, color='red', linestyle='--', label=f'Best LR: {best_lr}')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "    \n",
    "#     print(f'Best learning rate: {best_lr}')\n",
    "#     return best_lr\n",
    "\n",
    "# ############# input ############## \n",
    "# batch_size = 16  #to find the optimal learning rate\n",
    "# best_lr = train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs=3)  \n",
    "# print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286386b2-d6e9-447e-95b5-b3dd8f18010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the teacher model\n",
    "def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, num_epochs=1, patience=5):\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    best_train_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            if index % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        epoch_loss /= num_batches  \n",
    "        \n",
    "        # Check for early stopping\n",
    "        if epoch_loss < best_train_loss:\n",
    "            best_train_loss = epoch_loss\n",
    "            patience_counter = 0 \n",
    "            # checkpoint\n",
    "            torch.save(model.state_dict(), f'teacher_model_weights_ckd_prof_checkpoint.pth')\n",
    "            torch.save(model, f'teacher_model_ckd_prof_checkpoint.pth')\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Finished Training Teacher\")\n",
    "\n",
    "\n",
    "# Function to train the student model with knowledge distillation\n",
    "def train_student_with_distillation_disparity(student, teacher, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs, patience=5):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    student.to(device)\n",
    "    teacher.to(device)\n",
    "    best_train_loss = float('inf')  \n",
    "    patience_counter = 0 \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        epoch_disparity = 0.0\n",
    "        running_recall = 0.0\n",
    "\n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            annot = data['target'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student(inputs)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "            \n",
    "            ce_loss = criterion(student_outputs, labels)\n",
    "            kd_loss = tkd_kdloss(student_outputs, teacher_outputs, temperature=temperature)  # Make sure this returns a scalar\n",
    "            recall_difference = recall_disparity_loss(student_outputs, labels, annot, num_classes)\n",
    "            \n",
    "            # If not scalar, sum up to make sure the loss is scalar\n",
    "            if kd_loss.ndim != 0:\n",
    "                kd_loss = kd_loss.sum()\n",
    "            if recall_difference.ndim != 0:\n",
    "                recall_difference = recall_difference.sum()\n",
    "            \n",
    "            # Now combine the losses\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss + beta * recall_difference\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "            if index % 100 == 99:  \n",
    "                print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "                running_loss = 0.0\n",
    "    \n",
    "        epoch_loss /= num_batches\n",
    "\n",
    "        # Check for early stopping\n",
    "        if epoch_loss < best_train_loss:\n",
    "            best_train_loss = epoch_loss\n",
    "            patience_counter = 0 \n",
    "            torch.save(student.state_dict(), f'student_model_weights_ckd_prof_checkpoint.pth')\n",
    "            torch.save(student, f'student_model_ckd_prof_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break  \n",
    "\n",
    "        scheduler.step() \n",
    "\n",
    "    print(\"Finished Training Student\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540397f4-0bb9-4f55-88a5-bdd7671a913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to train the teacher model\n",
    "train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs=num_epochs)\n",
    "\n",
    "# Call the function to train the student model with knowledge distillation\n",
    "train_student_with_distillation_disparity(student_model, teacher_model, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca08a84f-2f5d-4a5b-8c0e-d77a18802260",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30aad75-7eb3-4a59-b1ac-dc3582cfa65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Testing 1 ######################\n",
    "# Save the student and teacher model weights and architecture\n",
    "# torch.save(student_model.state_dict(), 'student_model_weights_ckd_prof.pth')\n",
    "# torch.save(student_model, 'student_model_ckd_prof.pth')\n",
    "# print('student weights and architecture saved and exported')\n",
    "\n",
    "# torch.save(teacher_model.state_dict(), 'teacher_model_weights_ckd_prof.pth')\n",
    "# torch.save(teacher_model, 'teacher_model_ckd_prof.pth')\n",
    "# print('teacher weights and architecture saved and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e29951-1e09-4d6d-8d04-15897dd00723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###################### Testing 2 ######################\n",
    "# # Save the student and teacher model weights and architecture\n",
    "# torch.save(student_model.state_dict(), 'student_model_weights_ckd_2.pth')\n",
    "# torch.save(student_model, 'student_model_ckd_2.pth')\n",
    "# print('weights and architecture saved and exported')\n",
    "\n",
    "# torch.save(teacher_model.state_dict(), 'teacher_model_weights_ckd_2.pth')\n",
    "# torch.save(teacher_model, 'teacher_model_ckd_2.pth')\n",
    "# print('teacher weights and architecture saved and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995827c-1726-44e2-89aa-5f346a3d4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the comparison and plotting functions after training\n",
    "teacher_params, student_params = compare_model_size(teacher_model, student_model)\n",
    "teacher_time, student_time = compare_inference_time(teacher_model, student_model, trainloader)\n",
    "performance_metrics = compare_performance_metrics(teacher_model, student_model, trainloader)\n",
    "\n",
    "# Extracting the metric values for plotting\n",
    "performance_labels = ['accuracy', 'precision', 'recall', 'f1']\n",
    "teacher_performance_values = [performance_metrics[metric][0] for metric in performance_labels]\n",
    "student_performance_values = [performance_metrics[metric][1] for metric in performance_labels]\n",
    "\n",
    "# Plotting the comparison for performance metrics\n",
    "plot_comparison(performance_labels, teacher_performance_values, student_performance_values, 'Performance Comparison', 'Score')\n",
    "\n",
    "# Plotting the comparison for model size\n",
    "model_size_labels = ['Model Size']\n",
    "teacher_model_size_values = [teacher_params]\n",
    "student_model_size_values = [student_params]\n",
    "plot_comparison(model_size_labels, teacher_model_size_values, student_model_size_values, 'Model Size Comparison', 'Parameter Count (millions)')\n",
    "\n",
    "# Plotting the comparison for inference time\n",
    "inference_time_labels = ['Inference Time']\n",
    "teacher_inference_time_values = [teacher_time]\n",
    "student_inference_time_values = [student_time]\n",
    "plot_comparison(inference_time_labels, teacher_inference_time_values, student_inference_time_values, 'Inference Time Comparison', 'Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4fbd0-9771-4790-ae5a-f576a7132fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9859d-e3e5-401a-afd5-cd8bbbed5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_model = torchvision.models.resnet34(weights=None)\n",
    "\n",
    "# weights_path = 'teacher_model_weights_ckd_prof.pth'\n",
    "\n",
    "# teacher_model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "# student_model = torchvision.models.resnet18(weights=None)\n",
    "\n",
    "# weights_path = 'student_model_weights_ckd_prof.pth'\n",
    "\n",
    "# student_model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "# Call the comparison and plotting functions after training\n",
    "teacher_params, student_params = compare_model_size(teacher_model, student_model)\n",
    "teacher_time, student_time = compare_inference_time(teacher_model, student_model, testloader)\n",
    "performance_metrics = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "\n",
    "# Extracting the metric values for plotting\n",
    "performance_labels = ['accuracy', 'precision', 'recall', 'f1']\n",
    "teacher_performance_values = [performance_metrics[metric][0] for metric in performance_labels]\n",
    "student_performance_values = [performance_metrics[metric][1] for metric in performance_labels]\n",
    "\n",
    "# Plotting the comparison for performance metrics\n",
    "plot_comparison(performance_labels, teacher_performance_values, student_performance_values, 'Performance Comparison', 'Score')\n",
    "\n",
    "# Plotting the comparison for model size\n",
    "model_size_labels = ['Model Size']\n",
    "teacher_model_size_values = [teacher_params]\n",
    "student_model_size_values = [student_params]\n",
    "plot_comparison(model_size_labels, teacher_model_size_values, student_model_size_values, 'Model Size Comparison', 'Parameter Count (millions)')\n",
    "\n",
    "# Plotting the comparison for inference time\n",
    "inference_time_labels = ['Inference Time']\n",
    "teacher_inference_time_values = [teacher_time]\n",
    "student_inference_time_values = [student_time]\n",
    "plot_comparison(inference_time_labels, teacher_inference_time_values, student_inference_time_values, 'Inference Time Comparison', 'Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a15892-747e-40ed-9d6a-ed5160b58757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(preds, targets, condition):\n",
    "    \"\"\"\n",
    "    Calculate recall for a given condition in a multi-class setting.\n",
    "\n",
    "    :param preds: Predicted classes.\n",
    "    :param targets: True classes.\n",
    "    :param condition: Boolean tensor indicating the condition (subset) for which to calculate recall.\n",
    "    :return: Recall value.\n",
    "    \"\"\"\n",
    "    if condition.sum() == 0:  # No samples meet the condition\n",
    "        return 0.0\n",
    "\n",
    "    filtered_preds = preds[condition]\n",
    "    filtered_targets = targets[condition]\n",
    "\n",
    "    true_positive = (filtered_preds == filtered_targets).sum().float()\n",
    "    condition_positive = filtered_targets.size(0)\n",
    "\n",
    "    recall = true_positive / condition_positive if condition_positive > 0 else 0.0\n",
    "    return recall\n",
    "\n",
    "\n",
    "def evaluate_disparity(model, dataloader, num_classes):\n",
    "    \"\"\"\n",
    "    Evaluate the disparity on the test data.\n",
    "\n",
    "    :param model: The trained model.\n",
    "    :param dataloader: DataLoader for the test data.\n",
    "    :param num_classes: The number of classes in the dataset.\n",
    "    :return: Average disparity across all attributes and classes.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_disparity = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['img'].to(device)\n",
    "            targets = batch['label'].to(device)\n",
    "            attributes = batch['target'].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            batch_disparity = 0.0\n",
    "            for class_idx in range(num_classes):\n",
    "                for attr_idx in range(attributes.size(1)):\n",
    "                    condition_present = (attributes[:, attr_idx] == 1) & (targets == class_idx)\n",
    "                    condition_absent = (attributes[:, attr_idx] == 0) & (targets == class_idx)\n",
    "\n",
    "                    recall_present = calculate_recall(preds, targets, condition_present)\n",
    "                    recall_absent = calculate_recall(preds, targets, condition_absent)\n",
    "\n",
    "                    disparity = abs(recall_present - recall_absent)\n",
    "                    batch_disparity += disparity\n",
    "\n",
    "                    # Debug information\n",
    "                    print(f\"Class: {class_idx}, Attr: {attr_idx}, Disparity: {disparity}\")\n",
    "\n",
    "            batch_disparity /= (num_classes * attributes.size(1))  # Normalize by number of classes and attributes\n",
    "            total_disparity += batch_disparity\n",
    "            num_batches += 1\n",
    "\n",
    "    average_disparity = total_disparity / num_batches\n",
    "    return average_disparity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2be58-1645-404b-95e8-439591731303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "disparity = evaluate_disparity(student_model, testloader, num_classes=num_classes)\n",
    "print(f'Average recall disparity across all attributes and classes: {disparity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f4f0c-9b08-4e74-8905-637c8431d93d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
