{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f79206-feac-4302-abb1-d699b6354a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck, ResNet18_Weights, ResNet34_Weights, resnet18\n",
    "from torchvision.datasets import ImageFolder\n",
    "from utils.loss_functions import tkd_kdloss\n",
    "\n",
    "from models_package.models import Teacher, Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3c0ec8-98dc-4933-b9a3-e96fce39c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001379 # 0.096779\n",
    "num_epochs = 15 # 200\n",
    "temperature = 4.0\n",
    "alpha = 0.9\n",
    "momentum = 0.9\n",
    "num_classes = 30\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "beta = 0.0\n",
    "patience = 7  # for early stopping\n",
    "\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "class_labels = [0, 1, 3, 4, 6, 7, 11, 15, 17, 18, 19, 20, 22, 25, 27, 28, 30, 31, 33, 35, 36, 37, 39, 43, 44, 50, 51, 54, 57, 58]\n",
    "class_labels_new = torch.tensor([i for i in range(len(class_labels))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2770eac-10b7-47de-ba06-9aa63cd7ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c922f94-bf69-40d5-bc88-1e6fab4b4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedBatchSampler:\n",
    "    \"\"\"Stratified batch sampling\n",
    "    Provides equal representation of target classes in each batch\n",
    "    \"\"\"\n",
    "    def __init__(self, y, batch_size, shuffle=True):\n",
    "        if torch.is_tensor(y):\n",
    "            y = y.numpy()\n",
    "        assert len(y.shape) == 1, 'label array must be 1D'\n",
    "        n_batches = int(len(y) / batch_size)\n",
    "        self.skf = StratifiedKFold(n_splits=n_batches, shuffle=shuffle)\n",
    "        self.X = torch.randn(len(y),1).numpy()\n",
    "        self.y = y\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.skf.random_state = torch.randint(0,int(1e8),size=()).item()\n",
    "        for train_idx, test_idx in self.skf.split(self.X, self.y):\n",
    "            yield test_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c84514-66c9-4543-8448-cafa751730ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, ann_files, augs, img_size, dataset, undersample=False):\n",
    "        # Define the original class labels\n",
    "\n",
    "        # Create a mapping from old labels to new labels\n",
    "        self.label_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted(class_labels))}\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.ann_files = ann_files\n",
    "        self.augment = self.augs_function(augs, img_size)\n",
    "        # Initialize transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) if self.dataset == \"wider\" else \n",
    "            transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "        ]) \n",
    "\n",
    "        self.anns = []\n",
    "        self.load_anns()\n",
    "        if undersample: \n",
    "            self.undersample_anns()\n",
    "        print(self.augment)\n",
    "\n",
    "    def augs_function(self, augs, img_size):            \n",
    "        t = [transforms.Resize((img_size, img_size))]\n",
    "        if 'randomflip' in augs:\n",
    "            t.append(transforms.RandomHorizontalFlip())\n",
    "        if 'ColorJitter' in augs:\n",
    "            t.append(transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0))\n",
    "        if 'resizedcrop' in augs:\n",
    "            t.append(transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0)))\n",
    "        if 'RandAugment' in augs:\n",
    "            t.append(transforms.RandAugment())\n",
    "        return transforms.Compose(t)\n",
    "    \n",
    "    def load_anns(self):\n",
    "        self.anns = []\n",
    "        for ann_file in self.ann_files:\n",
    "            json_data = json.load(open(ann_file, \"r\"))\n",
    "            self.anns += json_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self)\n",
    "        ann = self.anns[idx]\n",
    "        try:\n",
    "            img = Image.open(f'{ann[\"img_path\"]}').convert(\"RGB\")\n",
    "            img_area = self.augment(img)\n",
    "            img_area = self.transform(img_area)\n",
    "    \n",
    "            # Extract attributes\n",
    "            attributes_list = []\n",
    "            if 'targets' in ann:\n",
    "                attributes_list = [target['attribute'] for target in ann['targets']]\n",
    "    \n",
    "            num_people = len(attributes_list)\n",
    "            if num_people > 0:\n",
    "                attributes_distribution = [max(sum(attribute), 0)/num_people for attribute in zip(*attributes_list)]\n",
    "                target_tensor = torch.tensor([attributes_distribution[0]], dtype=torch.float32)\n",
    "            else:\n",
    "                # Handle the case where there are no attributes\n",
    "                target_tensor = torch.tensor([0], dtype=torch.float32)  # or any default value\n",
    "    \n",
    "            img_path = f'{ann[\"img_path\"]}'\n",
    "            label = self.extract_label(img_path)\n",
    "            \n",
    "            return {\n",
    "                \"label\": label,\n",
    "                \"target\": target_tensor,\n",
    "                \"img\": img_area\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image at index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def extract_label(self, img_path):\n",
    "        original_label = None\n",
    "    \n",
    "        if \"WIDER/Image/train\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/train/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/test\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/test/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/val\" in img_path:  # Handle validation images\n",
    "            label_str = img_path.split(\"WIDER/Image/val/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "    \n",
    "        if original_label is not None:\n",
    "            remapped_label = self.label_mapping[original_label]\n",
    "            return remapped_label\n",
    "        else:\n",
    "            raise ValueError(f\"Label could not be extracted from path: {img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2490c2-046a-4a19-9717-642adbabb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = ['data/wider/trainval_wider.json']\n",
    "test_file = ['data/wider/test_wider.json']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "012d6bd8-61d5-4a20-845b-689234718508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        # Create a placeholder batch with zero tensors of the correct shape\n",
    "        placeholder_img = torch.zeros(1, 3, 224, 224)  # Adjust the shape to match your input\n",
    "        placeholder_label = torch.zeros(1, dtype=torch.long)\n",
    "        placeholder_target = torch.zeros(1, 1)  # Adjust this as needed\n",
    "        return {'img': placeholder_img, 'label': placeholder_label, 'target': placeholder_target}\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe2adb5-687d-4055-a108-a9c041836b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "    RandAugment(num_ops=2, magnitude=9, num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataSet(train_file, augs = ['RandAugment'], img_size = 226, dataset = 'wider', undersample=False)\n",
    "test_dataset = DataSet(test_file, augs = [], img_size = 226, dataset = 'wider', undersample=False)\n",
    "\n",
    "\n",
    "trainloader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=custom_collate)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f42f0ac9-3af2-4990-a8c1-f7f2af96c03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28345\n",
      "29179\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66cdd881-3047-4bab-84c8-9ceaf347b958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'target': tensor([0.]),\n",
       " 'img': tensor([[[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]],\n",
       " \n",
       "         [[-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          ...,\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.],\n",
       "          [-1., -1., -1.,  ..., -1., -1., -1.]]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ead33f3-32b2-46f1-96aa-feb4aac0128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_labels = []\n",
    "\n",
    "# # Iterate through the DataLoader to collect all labels\n",
    "# for batch in trainloader:\n",
    "#     labels = batch['label'].numpy()  # Convert to NumPy array if not already\n",
    "#     all_train_labels.extend(labels)\n",
    "\n",
    "# # Convert the list to a NumPy array\n",
    "# all_train_labels = np.array(all_train_labels)\n",
    "\n",
    "# # Find the unique classes and their frequencies\n",
    "# unique_classes, class_frequencies = np.unique(all_train_labels, return_counts=True)\n",
    "# normalized_class_weights = torch.from_numpy(class_frequencies).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f761db95-0936-4f2e-8005-7ed3079237e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def plot_label_frequencies(train_labels, class_names):\n",
    "#     # Count the occurrences of each label in the training set\n",
    "#     train_label_counts = np.bincount(train_labels)\n",
    "\n",
    "#     # Count the occurrences of each label in the test set\n",
    "\n",
    "#     # Create a bar plot\n",
    "#     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#     bar_width = 0.35\n",
    "#     index = np.arange(len(class_names))\n",
    "\n",
    "#     # Plot training set frequencies\n",
    "#     train_bars = ax.bar(index, train_label_counts, bar_width, label='Train Set')\n",
    "\n",
    "#     # Plot test set frequencies\n",
    "\n",
    "#     # Add labels, title, and legend\n",
    "#     ax.set_xlabel('Class')\n",
    "#     ax.set_ylabel('Frequency')\n",
    "#     ax.set_title('Label Frequencies in Train Sets')\n",
    "#     ax.set_xticks(index + bar_width / 2)\n",
    "#     ax.set_xticklabels(class_names)\n",
    "#     ax.legend()\n",
    "\n",
    "#     # Show the plot\n",
    "#     plt.show()\n",
    "\n",
    "# # Extract labels from the data\n",
    "# # train_labels = [train_dataset[i]['label'] for i in range(len(train_dataset))]\n",
    "# # test_labels = [test_dataset[i]['label'] for i in range(len(test_dataset))]\n",
    "\n",
    "# # List of class names\n",
    "# class_names = [f'Class {i+1}' for i in range(1, len(set(all_train_labels)) + 1)]\n",
    "\n",
    "# # Plot label frequencies\n",
    "# plot_label_frequencies(all_train_labels, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d18d4b-a3c0-4c5b-ad75-e74b6b1b7296",
   "metadata": {},
   "source": [
    "# Start Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49c941e9-954c-4d7b-932e-06d3297c02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    data = next(dataiter)\n",
    "    inputs = data['img']\n",
    "    \n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher(inputs)\n",
    "    teacher_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        student_outputs = student(inputs)\n",
    "    student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n",
    "\n",
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs)\n",
    "            student_outputs = student(inputs)\n",
    "            \n",
    "        teacher_preds = torch.argmax(teacher_outputs, dim=1).cpu().numpy()\n",
    "        student_preds = torch.argmax(student_outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(teacher_preds)\n",
    "        all_student_preds.append(student_preds)\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'all_labels': all_labels,\n",
    "        'all_teacher_preds': all_teacher_preds,\n",
    "        'all_student_preds': all_student_preds\n",
    "    }\n",
    "    \n",
    "def plot_comparison(labels, teacher_values, student_values, title, ylabel):\n",
    "    # Convert parameter count to millions\n",
    "    if 'Parameter Count' in title or 'Parameter Count' in ylabel:\n",
    "        teacher_values = [value / 1e6 for value in teacher_values]\n",
    "        student_values = [value / 1e6 for value in student_values]\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, teacher_values, width, label='Teacher')\n",
    "    rects2 = ax.bar(x + width/2, student_values, width, label='Student')\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c37c867-c1d1-438c-ac22-755904ce121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive_parity(actual_labels, y_pred, gender_labels, num_classes):\n",
    "    # Ensure actual_labels is an array of integers representing class labels\n",
    "    actual_labels = np.array(actual_labels, dtype=int)\n",
    "\n",
    "    # Convert gender_labels to binary (0 for female, 1 for male)\n",
    "    binary_gender_labels = (gender_labels >= 0.5).astype(int)\n",
    "\n",
    "    # Initialize arrays to store true positive rates for each gender and class\n",
    "    female_true_positive_rates = np.zeros(num_classes)\n",
    "    male_true_positive_rates = np.zeros(num_classes)\n",
    "\n",
    "    for class_label in range(num_classes):\n",
    "        # Identify indices corresponding to female and male for the current class\n",
    "        female_indices = np.logical_and(binary_gender_labels == 0, actual_labels == class_label)\n",
    "        male_indices = np.logical_and(binary_gender_labels == 1, actual_labels == class_label)\n",
    "\n",
    "        # Calculate true positive rates for the current class\n",
    "        female_true_positive_rates[class_label] = (\n",
    "            np.sum((y_pred[female_indices] == class_label) & (actual_labels[female_indices] == class_label)) /\n",
    "            np.sum(actual_labels == class_label)\n",
    "        )\n",
    "\n",
    "        male_true_positive_rates[class_label] = (\n",
    "            np.sum((y_pred[male_indices] == class_label) & (actual_labels[male_indices] == class_label)) /\n",
    "            np.sum(actual_labels == class_label)\n",
    "        )\n",
    "\n",
    "    # Calculate the absolute difference in true positive rates between male and female for each class\n",
    "    true_positive_parity_per_class = np.abs(female_true_positive_rates - male_true_positive_rates)\n",
    "\n",
    "    # Average over all classes to get an overall measure\n",
    "    true_positive_parity_average = np.mean(true_positive_parity_per_class)\n",
    "\n",
    "    return true_positive_parity_average\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a6b084d-9c3f-43cf-8460-8de4ac776a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecallDifferenceCalculator:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cumulative_true_positives_female = np.zeros(self.num_classes)\n",
    "        self.cumulative_actual_positives_female = np.zeros(self.num_classes)\n",
    "        self.cumulative_true_positives_male = np.zeros(self.num_classes)\n",
    "        self.cumulative_actual_positives_male = np.zeros(self.num_classes)\n",
    "\n",
    "    def update(self, actual_labels, y_pred, gender_labels):\n",
    "        # Convert lists to numpy arrays\n",
    "        actual_labels = np.array(actual_labels, dtype=int)\n",
    "        y_pred = np.array(y_pred, dtype=int)\n",
    "        gender_labels = np.array(gender_labels, dtype=int)\n",
    "\n",
    "        for class_label in range(self.num_classes):\n",
    "            for gender in [0, 1]:  # 0 for female, 1 for male\n",
    "                indices = np.logical_and(gender_labels == gender, actual_labels == class_label)\n",
    "                true_positives = np.sum(np.logical_and(indices, y_pred == class_label))\n",
    "                \n",
    "                if gender == 0:  # Female\n",
    "                    self.cumulative_true_positives_female[class_label] += true_positives\n",
    "                    self.cumulative_actual_positives_female[class_label] += np.sum(indices)\n",
    "                else:  # Male\n",
    "                    self.cumulative_true_positives_male[class_label] += true_positives\n",
    "                    self.cumulative_actual_positives_male[class_label] += np.sum(indices)\n",
    "\n",
    "\n",
    "    def compute_recall_difference(self):\n",
    "        recall_female = np.divide(self.cumulative_true_positives_female, self.cumulative_actual_positives_female, \n",
    "                                  where=self.cumulative_actual_positives_female != 0)\n",
    "        recall_male = np.divide(self.cumulative_true_positives_male, self.cumulative_actual_positives_male, \n",
    "                                where=self.cumulative_actual_positives_male != 0)\n",
    "\n",
    "        recall_difference_per_class = recall_male - recall_female\n",
    "        return dict(enumerate(recall_difference_per_class))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ba0ee98-39cc-442a-a0a4-192f8fd5dc66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall difference per class (check): {0: 5e-324, 1: -1.0, 2: 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Test compute_recall_difference method\n",
    "num_classes = 3\n",
    "actual_labels = [1, 2, 2]\n",
    "y_pred = [1, 2, 1]\n",
    "gender_labels = [0, 1, 1]  # Female for class 1, Male for class 2 and class 2\n",
    "\n",
    "# Instantiate and update the calculator\n",
    "calculator = RecallDifferenceCalculator(num_classes)\n",
    "calculator.update(actual_labels, y_pred, gender_labels)\n",
    "\n",
    "# Compute recall difference\n",
    "recall_difference = calculator.compute_recall_difference()\n",
    "print(\"Recall difference per class (check):\", recall_difference)\n",
    "\n",
    "# Expected values for simplified test\n",
    "expected_recall_difference_simplified = {1: -1.0, 2: 0.5}  # Class 1: -1.0 difference, Class 2: 0.5 difference\n",
    "\n",
    "# Assertion for simplified test\n",
    "assert all(np.isclose(recall_difference.get(k, None), v) for k, v in expected_recall_difference_simplified.items()), \"Simplified recall difference does not match expected values\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c6b93cb-37c9-448f-a8d9-424d3c4d40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "###################### Testing 1 ######################\n",
    "# Create instances of your models\n",
    "teacher_model = torchvision.models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1).to(device)\n",
    "teacher_model.fc = nn.Linear(512,30)\n",
    "student_model = torchvision.models.resnet18(weights=None).to(device)\n",
    "student_model.fc = nn.Linear(512,30)\n",
    "\n",
    "\n",
    "# # Load teacher\n",
    "# teacher_model = torch.load('teacher_model_ckd_prof.pth')\n",
    "# teacher_model.load_state_dict(torch.load('teacher_model_weights_ckd_prof_checkpoint.pth'))\n",
    "# teacher_model = teacher_model.to(device)\n",
    "# # Load the studnet\n",
    "# student_model = torch.load('student_model_ckd_prof.pth')\n",
    "# student_model.load_state_dict(torch.load('student_model_weights_ckd_prof_checkpoint.pth'))\n",
    "# student_model = student_model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "836f2993-0505-47c6-85ee-2ad2f45db946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adversary(nn.Module):\n",
    "    def __init__(self, identity_labels = 1):\n",
    "        super(Adversary, self).__init__()\n",
    "\n",
    "        self.a1 = nn.Linear(512,64)\n",
    "        self.a2 = nn.Linear(64, identity_labels)\n",
    "\n",
    "        nn.init.xavier_normal_(self.a1.weight)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        input_ids = input_ids.view(input_ids.size(0), -1)\n",
    "\n",
    "        #Adversary\n",
    "        adversary = F.relu(self.a1(input_ids))\n",
    "        adversary_output = self.a2(adversary)\n",
    "\n",
    "        return adversary_output\n",
    "\n",
    "\n",
    "adv = Adversary()\n",
    "adv = adv.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9de9c1c-649c-475f-9f52-c85a19d61bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules=list(student_model.children())[:-1]\n",
    "student_features=nn.Sequential(*modules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7b4731c-f105-4192-9650-6587c46f6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler for the student model\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Optimizer and scheduler for the teacher model\n",
    "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "teacher_scheduler = torch.optim.lr_scheduler.StepLR(teacher_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "\n",
    "# Instantiate the model and the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "adv_criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc1d2fe6-54a9-46eb-9db0-21691da79f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_student(student, teacher, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs, patience=5):\n",
    "    teacher.eval()\n",
    "    teacher.to(device)\n",
    "    best_val_loss = float('inf')  \n",
    "    patience_counter = 0 \n",
    "    student_epoch_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        student.train()\n",
    "        student.to(device)\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        # epoch_disparity = 0.0\n",
    "        \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student(inputs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "\n",
    "            \n",
    "            ce_loss = criterion(student_outputs, labels)\n",
    "            kd_loss = tkd_kdloss(student_outputs, teacher_outputs, temperature=temperature)  # Make sure this returns a scalar\n",
    "            \n",
    "            # If not scalar, sum up to make sure the loss is scalar\n",
    "            if kd_loss.ndim != 0:\n",
    "                kd_loss = kd_loss.sum()\n",
    "                \n",
    "            # Now combine the losses\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        print(f'*******Epoch {epoch}: loss - {epoch_loss}')\n",
    "        student_epoch_losses.append(epoch_loss)\n",
    "\n",
    "    return student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbefb9c9-e67a-4585-9d33-e807c114267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_adversary(adv, student, optimizer, trainloader, adv_criterion, epochs):\n",
    "  \n",
    "  pretrain_adversary_loss = 0\n",
    "  steps = 0\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_batches = 0\n",
    "    for i, data in enumerate(tqdm(trainloader)): # starting from the 0th batch\n",
    "        # get the inputs and labels\n",
    "        inputs = data['img'].to(device)\n",
    "        labels = data['label'].to(device)\n",
    "        targets = data['target'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        classifier_prev_output = student_features(inputs)\n",
    "        adversary_output = adv(classifier_prev_output)\n",
    "        adversary_loss = criterion(adversary_output, targets) # compute loss\n",
    "        adversary_loss.backward() # back prop\n",
    "        optimizer.step()\n",
    "        pretrain_adversary_loss += adversary_loss.item()\n",
    "        epoch_loss += adversary_loss.item()\n",
    "        epoch_batches += 1\n",
    "        steps += 1\n",
    "\n",
    "    print(\"Average Pretrain Adversary epoch loss: \", epoch_loss/epoch_batches)\n",
    "\n",
    "  return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b877954b-483b-455d-b5a2-211887f49d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_student(student_model, teacher_model, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs=3, patience=5)\n",
    "# pretrain_adversary(adv, student_model, optimizer, trainloader, adv_criterion, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3b7719f-4d65-4601-8e1d-1015de2201cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### finding the optimal learning rate\n",
    "# def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, num_epochs=5, lr_range=(1e-4, 1e-1), plot_loss=True):\n",
    "#     model.train()\n",
    "#     model.to(device)\n",
    "#     lr_values = np.logspace(np.log10(lr_range[0]), np.log10(lr_range[1]), num_epochs * len(trainloader))  # Generate learning rates for each batch\n",
    "#     lr_iter = iter(lr_values)\n",
    "#     losses = []\n",
    "#     lrs = []\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         for i, batch in enumerate(tqdm(trainloader)):\n",
    "#             lr = next(lr_iter)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = lr  # Set new learning rate\n",
    "            \n",
    "#             inputs, labels = batch['img'].to(device), batch['label'].to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             losses.append(loss.item())\n",
    "#             lrs.append(lr)\n",
    "    \n",
    "#     # Calculate the derivative of the loss\n",
    "#     loss_derivative = np.gradient(losses)\n",
    "    \n",
    "#     # Find the learning rate corresponding to the minimum derivative (steepest decline)\n",
    "#     best_lr_index = np.argmin(loss_derivative)\n",
    "#     best_lr = lrs[best_lr_index]\n",
    "    \n",
    "#     if plot_loss:\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.figure()\n",
    "#         plt.plot(lrs, losses)\n",
    "#         plt.xscale('log')\n",
    "#         plt.xlabel('Learning Rate')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title('Learning Rate Range Test')\n",
    "#         plt.axvline(x=best_lr, color='red', linestyle='--', label=f'Best LR: {best_lr}')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "    \n",
    "#     print(f'Best learning rate: {best_lr}')\n",
    "#     return best_lr\n",
    "\n",
    "# ############# input ############## \n",
    "# best_lr = train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs=3)  \n",
    "# print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "411c4f7a-d0b4-478f-9fa5-5e152e08ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(losses):\n",
    "    epochs = range(1, len(losses) + 1)\n",
    "    plt.plot(epochs, losses)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "286386b2-d6e9-447e-95b5-b3dd8f18010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the teacher model\n",
    "def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, epochs, patience=5):\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    epoch_losses = [] \n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "\n",
    "        epoch_loss /= num_batches  \n",
    "        epoch_losses.append(epoch_loss)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_val_loss = 0.0\n",
    "        num_batches = 0  \n",
    "        with torch.no_grad():\n",
    "            for val_data in tqdm(testloader):\n",
    "                val_inputs = val_data['img'].to(device)\n",
    "                val_labels = val_data['label'].to(device)\n",
    "    \n",
    "                # Forward pass for validation\n",
    "                val_outputs = model(val_inputs)\n",
    "    \n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                total_val_loss += val_loss.item()\n",
    "    \n",
    "                # Compute the validation accuracy\n",
    "                _, predicted = torch.max(val_outputs, 1)\n",
    "                total_samples += val_labels.size(0)\n",
    "                total_correct += (predicted == val_labels).sum().item()\n",
    "                num_batches += 1\n",
    "            total_val_loss /= num_batches\n",
    "            val_losses.append(total_val_loss)\n",
    "            accuracy = total_correct / total_samples\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss: .6f} Val Loss: {total_val_loss: .6f}| Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "            \n",
    "        # Check for early stopping\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            patience_counter = 0 \n",
    "            torch.save(model.state_dict(), f'teacher_model_weights_ckd_prof_checkpoint.pth')\n",
    "            torch.save(model, f'teacher_model_ckd_prof_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break  \n",
    "\n",
    "        scheduler.step()\n",
    "    plot_loss_curve(val_losses)\n",
    "    print(\"Finished Training Teacher\")\n",
    "\n",
    "def train_adversary(adv, student, optimizer, trainloader, adv_criterion, epochs):\n",
    "  \n",
    "  adv_loss = 0\n",
    "  steps = 0\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_batches = 0\n",
    "    for i, data in enumerate(tqdm(trainloader)): # starting from the 0th batch\n",
    "        # get the inputs and labels\n",
    "        inputs = data['img'].to(device)\n",
    "        labels = data['label'].to(device)\n",
    "        targets = data['target'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        classifier_prev_output = student_features(inputs)\n",
    "        \n",
    "        adversary_output = adv(classifier_prev_output)\n",
    "\n",
    "        adversary_loss = criterion(adversary_output, targets) # compute loss\n",
    "        adversary_loss.backward() # back prop\n",
    "        optimizer.step()\n",
    "        adv_loss += adversary_loss.item()\n",
    "        epoch_loss += adversary_loss.item()\n",
    "        epoch_batches += 1\n",
    "        steps += 1\n",
    "\n",
    "    print(\"Average Adversary epoch loss: \", epoch_loss/epoch_batches)\n",
    "\n",
    "  return adv\n",
    "# Function to train the student model with knowledge distillation\n",
    "def train_student_with_distillation_disparity(student, teacher, adv, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, epochs, lmba, patience=5):\n",
    "    teacher.eval()\n",
    "    teacher.to(device)\n",
    "    best_val_loss = float('inf')  \n",
    "    patience_counter = 0 \n",
    "    student_epoch_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        student.to(device)\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        # epoch_disparity = 0.0\n",
    "        \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            targets = data['target'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student(inputs)            \n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "                \n",
    "            classifier_prev_output = student_features(inputs)\n",
    "            adversary_output = adv(classifier_prev_output)\n",
    "            adversary_loss = criterion(adversary_output, targets)\n",
    "                \n",
    "            ce_loss = criterion(student_outputs, labels)\n",
    "            kd_loss = tkd_kdloss(student_outputs, teacher_outputs, temperature=temperature)  # Make sure this returns a scalar\n",
    "            \n",
    "\n",
    "            if kd_loss.ndim != 0:\n",
    "                kd_loss = kd_loss.sum()\n",
    "\n",
    "            \n",
    "            # Now combine the losses\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss - lmba * adversary_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            # epoch_disparity += disparity\n",
    "\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        # print(f'*******Epoch {epoch}: running_recall_with - {running_recall_with/num_batches}  |  running_recall_without - {running_recall_without/num_batches}  |  disparity - {epoch_disparity/num_batches}******')\n",
    "        student_epoch_losses.append(epoch_loss)\n",
    "\n",
    "        student.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_val_loss = 0.0\n",
    "        num_batches = 0\n",
    "        recalc.reset()\n",
    "        # Validation after each epoch\n",
    "        with torch.no_grad():\n",
    "            for val_data in tqdm(testloader):\n",
    "                val_inputs = val_data['img'].to(device)\n",
    "                val_labels = val_data['label'].to(device)\n",
    "                val_targets = val_data['target'].to(device)\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                val_student_outputs = student(val_inputs)\n",
    "                with torch.no_grad():\n",
    "                    val_teacher_outputs = teacher(val_inputs)\n",
    "\n",
    "\n",
    "                val_classifier_prev_output = student_features(val_inputs)\n",
    "                val_adversary_output = adv(val_classifier_prev_output)\n",
    "                val_adversary_loss = criterion(val_adversary_output, val_targets)\n",
    "                val_ce_loss = criterion(val_student_outputs, val_labels)\n",
    "                val_kd_loss = tkd_kdloss(val_student_outputs, val_teacher_outputs, temperature=temperature)  # Make sure this returns a scalar\n",
    "                \n",
    "                if val_kd_loss.ndim != 0:\n",
    "                    val_kd_loss = val_kd_loss.sum()\n",
    "\n",
    "                \n",
    "                val_loss = alpha * val_kd_loss + (1 - alpha) * val_ce_loss - lmba * val_adversary_loss\n",
    "                total_val_loss += val_loss.item()\n",
    "    \n",
    "                # Compute the validation accuracy\n",
    "                _, predicted = torch.max(val_student_outputs, 1)\n",
    "                total_samples += val_labels.size(0)\n",
    "                total_correct += (predicted == val_labels).sum().item()\n",
    "                num_batches += 1\n",
    "                recalc.update(val_labels, predicted, val_targets)\n",
    "\n",
    "            total_val_loss /= num_batches\n",
    "            val_losses.append(total_val_loss)\n",
    "            accuracy = total_correct / total_samples\n",
    "            recall_difference_epoch = recalc.compute_recall_difference()\n",
    "            print(f\"Epoch {epoch + 1}: Difference in Recall by Class: {recall_difference_epoch}\")\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss: .6f} Val Loss: {total_val_loss: .6f}| Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            patience_counter = 0 \n",
    "            torch.save(student.state_dict(), f'student_model_weights_ckd_prof_checkpoint.pth')\n",
    "            torch.save(student, f'student_model_ckd_prof_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break  \n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Finished Training Student\")\n",
    "    plot_loss_curve(val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "540397f4-0bb9-4f55-88a5-bdd7671a913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 111/111 [02:29<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Adversary epoch loss:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 111/111 [03:29<00:00,  1.89s/it]\n",
      "  0%|                                                                                                                     | 0/114 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate the RecallDifferenceCalculator\u001b[39;00m\n\u001b[1;32m      5\u001b[0m recalc \u001b[38;5;241m=\u001b[39m RecallDifferenceCalculator(num_classes)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain_student_with_distillation_disparity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudent_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlmba\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 197\u001b[0m, in \u001b[0;36mtrain_student_with_distillation_disparity\u001b[0;34m(student, teacher, adv, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, epochs, lmba, patience)\u001b[0m\n\u001b[1;32m    195\u001b[0m     total_correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m val_labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    196\u001b[0m     num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 197\u001b[0m     \u001b[43mrecalc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m total_val_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m num_batches\n\u001b[1;32m    200\u001b[0m val_losses\u001b[38;5;241m.\u001b[39mappend(total_val_loss)\n",
      "Cell \u001b[0;32mIn[33], line 14\u001b[0m, in \u001b[0;36mRecallDifferenceCalculator.update\u001b[0;34m(self, actual_labels, y_pred, gender_labels)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, actual_labels, y_pred, gender_labels):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Convert lists to numpy arrays\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     actual_labels \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactual_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_pred, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     16\u001b[0m     gender_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(gender_labels, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:1032\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs=num_epochs)\n",
    "train_adversary(adv, student_model, optimizer, trainloader, adv_criterion, epochs=1)\n",
    "\n",
    "# Instantiate the RecallDifferenceCalculator\n",
    "recalc = RecallDifferenceCalculator(num_classes)\n",
    "\n",
    "train_student_with_distillation_disparity(student_model, teacher_model, adv, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, lmba=0.25, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30aad75-7eb3-4a59-b1ac-dc3582cfa65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Testing 1 ######################\n",
    "# Save the student and teacher model weights and architecture\n",
    "torch.save(student_model.state_dict(), 'student_model_weights_ckd_prof.pth')\n",
    "torch.save(student_model, 'student_model_ckd_prof.pth')\n",
    "print('student weights and architecture saved and exported')\n",
    "\n",
    "torch.save(teacher_model.state_dict(), 'teacher_model_weights_ckd_prof.pth')\n",
    "torch.save(teacher_model, 'teacher_model_ckd_prof.pth')\n",
    "print('teacher weights and architecture saved and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995827c-1726-44e2-89aa-5f346a3d4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the comparison and plotting functions after training\n",
    "teacher_params, student_params = compare_model_size(teacher_model, student_model)\n",
    "teacher_time, student_time = compare_inference_time(teacher_model, student_model, testloader)\n",
    "performance_metrics = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "\n",
    "# Extracting the metric values for plotting\n",
    "performance_labels = ['accuracy', 'precision', 'recall', 'f1']\n",
    "teacher_performance_values = [performance_metrics['metrics'][metric][0] for metric in performance_labels]\n",
    "student_performance_values = [performance_metrics['metrics'][metric][1] for metric in performance_labels]\n",
    "\n",
    "# Plotting the comparison for performance metrics\n",
    "plot_comparison(performance_labels, teacher_performance_values, student_performance_values, 'Performance Comparison', 'Score')\n",
    "\n",
    "# Plotting the comparison for model size\n",
    "model_size_labels = ['Model Size']\n",
    "teacher_model_size_values = [teacher_params]\n",
    "student_model_size_values = [student_params]\n",
    "plot_comparison(model_size_labels, teacher_model_size_values, student_model_size_values, 'Model Size Comparison', 'Parameter Count (millions)')\n",
    "\n",
    "# Plotting the comparison for inference time\n",
    "inference_time_labels = ['Inference Time']\n",
    "teacher_inference_time_values = [teacher_time]\n",
    "student_inference_time_values = [student_time]\n",
    "plot_comparison(inference_time_labels, teacher_inference_time_values, student_inference_time_values, 'Inference Time Comparison', 'Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289f549-42fc-4c17-ad60-415775d9d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(preds, targets, condition):\n",
    "    \"\"\"\n",
    "    Calculate recall for a given condition in a multi-class setting.\n",
    "\n",
    "    :param preds: Predicted classes.\n",
    "    :param targets: True classes.\n",
    "    :param condition: Boolean tensor indicating the condition (subset) for which to calculate recall.\n",
    "    :return: Recall value.\n",
    "    \"\"\"\n",
    "    if condition.sum() == 0:  # No samples meet the condition\n",
    "        return 0.0\n",
    "\n",
    "    filtered_preds = preds[condition]\n",
    "    filtered_targets = targets[condition]\n",
    "\n",
    "    true_positive = (filtered_preds == filtered_targets).sum().float()\n",
    "    condition_positive = filtered_targets.size(0)\n",
    "\n",
    "    recall = true_positive / condition_positive if condition_positive > 0 else 0.0\n",
    "    return recall\n",
    "    \n",
    "def calculate_weighted_disparity(disparity_sums, counts):\n",
    "    \"\"\"\n",
    "    Calculate weighted disparity for each class-attribute pair.\n",
    "    \"\"\"\n",
    "    weighted_disparities = torch.zeros_like(disparity_sums)\n",
    "    for class_idx in range(disparity_sums.size(0)):\n",
    "        for attr_idx in range(disparity_sums.size(1)):\n",
    "            if counts[class_idx][attr_idx] > 0:\n",
    "                weighted_disparities[class_idx][attr_idx] = disparity_sums[class_idx][attr_idx] / counts[class_idx][attr_idx]\n",
    "            else:\n",
    "                weighted_disparities[class_idx][attr_idx] = 0.0\n",
    "    return weighted_disparities\n",
    "\n",
    "def evaluate_disparity(model, dataloader, num_classes, device):\n",
    "    \"\"\"\n",
    "    Evaluate the disparity on the test data with weighted consideration.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    disparity_sums = None\n",
    "    counts = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['img'].to(device)\n",
    "            targets = batch['label'].to(device)\n",
    "            attributes = batch['target'].to(device)\n",
    "\n",
    "            if disparity_sums is None:\n",
    "                disparity_sums = torch.zeros(num_classes, attributes.size(1), device=device)\n",
    "                counts = torch.zeros(num_classes, attributes.size(1), device=device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for class_idx in range(num_classes):\n",
    "                for attr_idx in range(attributes.size(1)):\n",
    "                    condition_present = (attributes[:, attr_idx] == 1) & (targets == class_idx)\n",
    "                    condition_absent = (attributes[:, attr_idx] == 0) & (targets == class_idx)\n",
    "\n",
    "                    if condition_present.sum() > 0 or condition_absent.sum() > 0:\n",
    "                        recall_present = calculate_recall(preds, targets, condition_present)\n",
    "                        recall_absent = calculate_recall(preds, targets, condition_absent)\n",
    "\n",
    "                        disparity = abs(recall_present - recall_absent)\n",
    "                        count = condition_present.sum() + condition_absent.sum()\n",
    "                        disparity_sums[class_idx][attr_idx] += disparity * count\n",
    "                        counts[class_idx][attr_idx] += count\n",
    "\n",
    "    weighted_disparities = calculate_weighted_disparity(disparity_sums, counts)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        for attr_idx in range(attributes.size(1)):\n",
    "            print(f\"Class: {class_idx}, Attr: {attr_idx}, Weighted Disparity: {weighted_disparities[class_idx][attr_idx]}\")\n",
    "\n",
    "    weighted_average = weighted_disparities.flatten()\n",
    "    weighted_average = weighted_average.sum()/weighted_average.numel()\n",
    "    return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3dd66-85c6-4a91-9c4d-f20fbc011342",
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity = evaluate_disparity(student_model, testloader, num_classes=num_classes, device=device)\n",
    "print(f'Average recall disparity across all attributes and classes: {disparity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa18cfd-8f83-4c3f-bbe9-c834403adf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_new = [f\"Class {label}\" for label in range(30)]\n",
    "def plot_prediction_distribution_and_confusion_matrix(labels, preds, class_names):\n",
    "    # Plotting the distribution of predictions\n",
    "    sns.countplot(x=preds)\n",
    "    plt.title('Distribution of Predictions')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Computing the confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names_new, columns=class_names_new)\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='g')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report for detailed metrics\n",
    "    print(classification_report(labels, preds, target_names=class_names, zero_division=0))\n",
    "\n",
    "performance_metrics_teacher = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "all_labels = performance_metrics_teacher['all_labels']\n",
    "all_teacher_preds = performance_metrics_teacher['all_teacher_preds']\n",
    "all_student_preds = performance_metrics_teacher['all_student_preds']\n",
    "\n",
    "# For the Teacher Model\n",
    "plot_prediction_distribution_and_confusion_matrix(all_labels, all_teacher_preds, class_names_new)\n",
    "\n",
    "# For the Student Model\n",
    "plot_prediction_distribution_and_confusion_matrix(all_labels, all_student_preds, class_names_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ccf26-26ea-468d-877a-d8c24d8b45a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf4430b-9213-444d-9288-b03b6011266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef97ca7-c21c-4463-8845-1d4cf2e45927",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules=list(student_model.children())[:-1]\n",
    "student_features=nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd9fea-f2c5-469c-aaf1-5a045dfb0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e634a-992d-4ae1-8883-fb8065970cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in student_model.named_parameters():\n",
    "    if \"weight\" in name:  # assuming the last layer has a weight parameter\n",
    "        last_layer_hidden_size = param.size(0)  # The first dimension is the hidden size\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05762185-9db6-46c5-93f1-3eb7e52402ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8f21a-ed8f-42bb-9b91-fce9fe3c1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d76a0f-a3e9-416d-861b-52c733cf9d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        return 100  # Some arbitrary number\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.zeros(1, 3, 224, 224), torch.zeros(1, dtype=torch.long), torch.zeros(1, 1)\n",
    "\n",
    "simple_dataset = SimpleDataset()\n",
    "simple_loader = DataLoader(simple_dataset, batch_size=10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f1b5e-7974-4794-b588-96ff91a1ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac346080-6245-44c5-9f1e-71fb6359cc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)  # Using the default collate function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7c57c-88e5-4508-8bca-4c5daff63934",
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_indices = [10, 20, 30]  # Replace with actual problematic indices if found\n",
    "for idx in problematic_indices:\n",
    "    try:\n",
    "        data = train_dataset[idx]\n",
    "        print(f\"Data at index {idx}: {data}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Exception for index {idx}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30025191-28e7-4d4c-9598-01cb15496d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in [10, 20, 30]:  # Same indices as before\n",
    "    data = train_dataset[idx]\n",
    "    print(f\"Data at index {idx}: {data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9bdb28-eacf-4bd5-a842-377fc3827657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
