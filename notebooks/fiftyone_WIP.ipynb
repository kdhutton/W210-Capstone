{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0a8b3b-5189-4857-887c-e93b637bf000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.28.68 requires botocore<1.32.0,>=1.31.68, but you have botocore 1.31.64 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.7.0 requires botocore<1.31.65,>=1.31.16, but you have botocore 1.31.73 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install for AWS\n",
    "!pip install torch --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install scikit-image --quiet\n",
    "!pip install scikit-learn --quiet\n",
    "!pip install matplotlib --quiet\n",
    "!pip install torchvision --quiet\n",
    "!pip install s3fs --quiet\n",
    "!pip install boto3 --quiet\n",
    "!pip install tqdm --quiet\n",
    "!pip install fiftyone --quiet\n",
    "!pip install pycocotools --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c928e2-c18b-457c-8640-f03b4b11b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tarfile\n",
    "import shutil\n",
    "import torchvision\n",
    "import random\n",
    "import warnings\n",
    "import boto3\n",
    "import s3fs\n",
    "import io\n",
    "import time\n",
    "import botocore.exceptions\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import getpass\n",
    "import json\n",
    "# import torch.jit as jit\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, utils, models, datasets\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f329f00-bc92-4a2c-9f76-9d7a8f3aff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your access:  ········\n",
      "Enter your secret:  ········\n"
     ]
    }
   ],
   "source": [
    "########## LM ##########\n",
    "\n",
    "access_key = getpass.getpass(\"Enter your access: \")\n",
    "\n",
    "secret_key = password = getpass.getpass(\"Enter your secret: \")\n",
    "\n",
    "bucket_name = 'w210facetdata'\n",
    "annotations_prefix = 'annotations/'\n",
    "images_prefix = '/home/ubuntu/W210-Capstone'\n",
    "\n",
    "s3 = s3fs.S3FileSystem(key=access_key, secret=secret_key)\n",
    "\n",
    "# Use s3.open to open the CSV file and read its content into a Pandas DataFrame\n",
    "with s3.open(f's3://{bucket_name}/{annotations_prefix}annotations.csv', 'rb') as file:\n",
    "    gt_df = pd.read_csv(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "344af206-7ef3-4ec9-b74c-f332ab043ff8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset 'IDP2' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Delete the datasets\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name \u001b[38;5;129;01min\u001b[39;00m datasets_to_delete:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/core/dataset.py:192\u001b[0m, in \u001b[0;36mdelete_dataset\u001b[0;34m(name, verbose)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelete_dataset\u001b[39m(name, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    186\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deletes the FiftyOne dataset with the given name.\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m        name: the name of the dataset\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m        verbose (False): whether to log the name of the deleted dataset\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mdelete()\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/core/dataset.py:131\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m(name):\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads the FiftyOne dataset with the given name.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    To create a new dataset, use the :class:`Dataset` constructor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m        a :class:`Dataset`\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/core/singletons.py:36\u001b[0m, in \u001b[0;36mDatasetSingleton.__call__\u001b[0;34m(cls, name, _create, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     _create\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mdeleted\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     34\u001b[0m ):\n\u001b[1;32m     35\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     name \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39mname  \u001b[38;5;66;03m# `__init__` may have changed `name`\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/core/dataset.py:274\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, name, persistent, overwrite, _create, _virtual, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m     doc, sample_doc_cls, frame_doc_cls \u001b[38;5;241m=\u001b[39m _create_dataset(\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28mself\u001b[39m, name, persistent\u001b[38;5;241m=\u001b[39mpersistent, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    272\u001b[0m     )\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     doc, sample_doc_cls, frame_doc_cls \u001b[38;5;241m=\u001b[39m \u001b[43m_load_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvirtual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_virtual\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_doc \u001b[38;5;241m=\u001b[39m doc\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_doc_cls \u001b[38;5;241m=\u001b[39m sample_doc_cls\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/core/dataset.py:6804\u001b[0m, in \u001b[0;36m_load_dataset\u001b[0;34m(obj, name, virtual)\u001b[0m\n\u001b[1;32m   6802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_load_dataset\u001b[39m(obj, name, virtual\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   6803\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m virtual:\n\u001b[0;32m-> 6804\u001b[0m         \u001b[43mfomi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmigrate_dataset_if_necessary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6806\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   6807\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _do_load_dataset(obj, name)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/migrations/runner.py:222\u001b[0m, in \u001b[0;36mmigrate_dataset_if_necessary\u001b[0;34m(name, destination, error_level, verbose)\u001b[0m\n\u001b[1;32m    220\u001b[0m     _migrate_dataset_if_necessary(name, destination, verbose)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 222\u001b[0m     \u001b[43mfou\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_level\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/core/utils.py:676\u001b[0m, in \u001b[0;36mhandle_error\u001b[0;34m(error, error_level, base_error)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandle_error\u001b[39m(error, error_level, base_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    664\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Handles the error at the specified error level.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \n\u001b[1;32m    666\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03m        base_error: (optional) a base Exception from which to raise ``error``\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 676\u001b[0m     \u001b[43metau\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_error\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/eta/core/utils.py:1004\u001b[0m, in \u001b[0;36mhandle_error\u001b[0;34m(error, error_level, base_error)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         six\u001b[38;5;241m.\u001b[39mraise_from(error, base_error)\n\u001b[1;32m   1003\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1004\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_level \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1007\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(error)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/migrations/runner.py:220\u001b[0m, in \u001b[0;36mmigrate_dataset_if_necessary\u001b[0;34m(name, destination, error_level, verbose)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Migrates the dataset from its current revision to the specified\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03mdestination revision.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    verbose (False): whether to log incremental migrations that are run\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 220\u001b[0m     \u001b[43m_migrate_dataset_if_necessary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdestination\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    222\u001b[0m     fou\u001b[38;5;241m.\u001b[39mhandle_error(e, error_level\u001b[38;5;241m=\u001b[39merror_level)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/migrations/runner.py:229\u001b[0m, in \u001b[0;36m_migrate_dataset_if_necessary\u001b[0;34m(name, destination, verbose)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _migrations_disabled():\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_revision\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m db_version \u001b[38;5;241m=\u001b[39m get_database_revision()\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/migrations/runner.py:62\u001b[0m, in \u001b[0;36mget_dataset_revision\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     60\u001b[0m dataset_doc \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mfind_one({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: name}, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m})\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dataset_doc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset_doc\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset 'IDP2' not found"
     ]
    }
   ],
   "source": [
    "datasets_to_delete = ['IDP',\n",
    " 'IDP2',\n",
    " 'IDP3',\n",
    " 'IDP4',\n",
    " 'IDP5',\n",
    " 'IDP6',\n",
    " 'IDP7',\n",
    " 'IDP8']\n",
    "\n",
    "# Delete the datasets\n",
    "for dataset_name in datasets_to_delete:\n",
    "    fo.delete_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f7c272-668d-4671-84b1-16e16b01439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 31702/31702 [4.9s elapsed, 0s remaining, 6.6K samples/s]      \n",
      "Computing metadata...\n",
      " 100% |█████████████| 31702/31702 [1.0m elapsed, 0s remaining, 593.7 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# use relative paths to your image dirs\n",
    "dataset = fo.Dataset(name = \"IDP\", persistent=True)\n",
    "# dataset = fo.load_dataset('IDP')\n",
    "dataset.add_images_dir(images_prefix)\n",
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7a9c86-d180-4065-b902-93d75cf2515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########## KH ##########\n",
    "\n",
    "# # Initialize S3 client\n",
    "# s3_client = boto3.client('s3', region_name='us-west-2')\n",
    "\n",
    "# # Define the S3 bucket name and prefixes\n",
    "# bucket_name = 'w210facetdata'\n",
    "# annotations_prefix = 'annotations/'\n",
    "# images_prefix = 'images/'\n",
    "\n",
    "# # Load CSV annotations from S3\n",
    "# annotations_s3_path = f's3://{bucket_name}/{annotations_prefix}'\n",
    "# gt_df = pd.read_csv(f'{annotations_s3_path}annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04df1e08-51f4-42ee-9459-9f3fa86b1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########## KH ##########\n",
    "# local_images_dir = 'local_images_dir'\n",
    "# os.makedirs(local_images_dir, exist_ok=True)\n",
    "\n",
    "# # Create a paginator to handle pagination of the results\n",
    "# paginator = s3_client.get_paginator('list_objects_v2')\n",
    "\n",
    "# # Use the paginator to retrieve all objects\n",
    "# for page in paginator.paginate(Bucket=bucket_name, Prefix=images_prefix):\n",
    "#     for obj in page.get('Contents', []):\n",
    "#         # Skip the prefix itself\n",
    "#         if obj['Key'] == images_prefix:\n",
    "#             continue\n",
    "#         local_file_path = os.path.join(local_images_dir, os.path.basename(obj['Key']))\n",
    "#         s3_client.download_file(bucket_name, obj['Key'], local_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f0cfd8a-0b73-4f7b-8b4a-33af7033b4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 31703/31703 [4.8s elapsed, 0s remaining, 6.8K samples/s]      \n",
      "Computing metadata...\n",
      " 100% |█████████████| 31703/31703 [59.6s elapsed, 0s remaining, 603.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# ########## KH ##########\n",
    "# # local_images_dir = 'local_images_dir'\n",
    "\n",
    "# # fo.delete_dataset('local_images_dir')\n",
    "# dataset = fo.Dataset(name='local_images_dir')\n",
    "\n",
    "# dataset.add_images_dir(local_images_dir)\n",
    "# dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dc58bbf-ed5e-4f3a-b86a-77e58abb1ad2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'local_images_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Count the number of files in the local_images_dir\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m num_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[43mlocal_images_dir\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(local_images_dir, f))])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(num_files)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'local_images_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# # Count the number of files in the local_images_dir\n",
    "# num_files = len([f for f in os.listdir(local_images_dir) if os.path.isfile(os.path.join(local_images_dir, f))])\n",
    "# print(num_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c721ef-146f-4f48-a411-7fb93bca9d84",
   "metadata": {},
   "source": [
    "# Object Detection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be99a8b0-a415-4560-a578-2b9f8bba27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOLEAN_PERSONAL_ATTRS = (\n",
    "    \"has_facial_hair\",\n",
    "    \"has_tattoo\",\n",
    "    \"has_cap\",\n",
    "    \"has_mask\",\n",
    "    \"has_headscarf\",\n",
    "    \"has_eyeware\",\n",
    ")\n",
    "def add_boolean_person_attributes(detection, row_index):\n",
    "    for attr in BOOLEAN_PERSONAL_ATTRS:\n",
    "        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "892ad9c7-cb94-49ee-bbe6-caf72e3e7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hairtype(row_index):\n",
    "    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hairtype')]\n",
    "    hairtype = hair_info[hair_info == 1]\n",
    "    if len(hairtype) == 0:\n",
    "        return None\n",
    "    return hairtype.index[0].split('_')[1]\n",
    "\n",
    "def get_haircolor(row_index):\n",
    "    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hair_color')]\n",
    "    haircolor = hair_info[hair_info == 1]\n",
    "    if len(haircolor) == 0:\n",
    "        return None\n",
    "    return haircolor.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8fc5510-2fd8-42f9-a58a-c301286b3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_person_attributes(detection, row_index):\n",
    "    detection[\"hairtype\"] = get_hairtype(row_index)\n",
    "    detection[\"haircolor\"] = get_haircolor(row_index)\n",
    "    add_boolean_person_attributes(detection, row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0091b7eb-d570-4b21-8290-17fee15d226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perceived_gender_presentation(row_index):\n",
    "    gender_info = gt_df.loc[row_index, gt_df.columns.str.startswith('gender')]\n",
    "    pgp = gender_info[gender_info == 1]\n",
    "    if len(pgp) == 0:\n",
    "        return None\n",
    "    return pgp.index[0].replace(\"gender_presentation_\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "def get_perceived_age_presentation(row_index):\n",
    "    age_info = gt_df.loc[row_index, gt_df.columns.str.startswith('age')]\n",
    "    pap = age_info[age_info == 1]\n",
    "    if len(pap) == 0:\n",
    "        return None\n",
    "    return pap.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c49856f0-9c40-44f1-b8dc-afc8a0c80786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skintone(row_index):\n",
    "    skin_info = gt_df.loc[row_index, gt_df.columns.str.startswith('skin_tone')]\n",
    "    return skin_info.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a702e8b-93a2-4e8b-879c-f82be93830ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_protected_attributes(detection, row_index):\n",
    "    detection[\"perceived_age_presentation\"] = get_perceived_age_presentation(row_index)\n",
    "    detection[\"perceived_gender_presentation\"] = get_perceived_gender_presentation(row_index)\n",
    "    detection[\"skin_tone\"] = get_skintone(row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01f3a95d-e491-4de4-be5a-d53de547f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISIBILITY_ATTRS = (\"visible_torso\", \"visible_face\", \"visible_minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc9ed1ed-c71d-4583-963f-bfa23563309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lighting(row_index):\n",
    "    lighting_info = gt_df.loc[row_index, gt_df.columns.str.startswith('lighting')]\n",
    "    lighting = lighting_info[lighting_info == 1]\n",
    "    if len(lighting) == 0:\n",
    "        return None\n",
    "    lighting = lighting.index[0].replace(\"lighting_\", \"\").replace(\"_\", \" \")\n",
    "    return lighting\n",
    "\n",
    "def add_other_attributes(detection, row_index):\n",
    "    detection[\"lighting\"] = get_lighting(row_index)\n",
    "    for attr in VISIBILITY_ATTRS:\n",
    "        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3de3c768-bfa3-4727-95ed-59346a5788a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detection(row_index, sample):\n",
    "    bbox_dict = json.loads(gt_df.loc[row_index, \"bounding_box\"])\n",
    "    x, y, w, h = bbox_dict[\"x\"], bbox_dict[\"y\"], bbox_dict[\"width\"], bbox_dict[\"height\"]\n",
    "    cat1, cat2 = bbox_dict[\"dict_attributes\"][\"cat1\"], bbox_dict[\"dict_attributes\"][\"cat2\"]\n",
    "\n",
    "    person_id = gt_df.loc[row_index, \"person_id\"]\n",
    "\n",
    "    img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "\n",
    "    bounding_box = [x/img_width, y/img_height, w/img_width, h/img_height]\n",
    "    detection = fo.Detection(\n",
    "        label=cat1, \n",
    "        bounding_box=bounding_box,\n",
    "        person_id=person_id,\n",
    "        )\n",
    "\n",
    "    detection[\"class2\"] = cat2\n",
    "\n",
    "    add_person_attributes(detection, row_index)\n",
    "    add_protected_attributes(detection, row_index)\n",
    "    add_other_attributes(detection, row_index)\n",
    "\n",
    "    return detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1869986-c9eb-454e-b164-cd5f8b598286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ground_truth_labels(dataset):\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        sample_annos = gt_df[gt_df['filename'] == sample.filename]\n",
    "        detections = []\n",
    "        for row in sample_annos.iterrows():\n",
    "            row_index = row[0]\n",
    "            detection = create_detection(row_index, sample)\n",
    "            detections.append(detection)\n",
    "        sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "    dataset.add_dynamic_sample_fields()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec896ec-9aa5-4033-839d-ced0b1ae54d6",
   "metadata": {},
   "source": [
    "# Add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d068d1d0-38e0-43b7-b827-5006503182d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 31702/31702 [5.8m elapsed, 0s remaining, 90.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "## add all of the ground truth labels\n",
    "add_ground_truth_labels(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e53d3-c90d-467e-83c2-6b1bc5717fdd",
   "metadata": {},
   "source": [
    "# Add Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b485560-887e-47f9-b980-a9460a481942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  36% |████---------| 11528/31703 [10.0m elapsed, 17.9m remaining, 18.2 samples/s]   "
     ]
    }
   ],
   "source": [
    "def add_coco_masks_to_dataset(dataset):\n",
    "    ########## LM ##########\n",
    "    # with s3.open(f's3://{bucket_name}/{annotations_prefix}coco_masks.json', 'rb') as file:\n",
    "    #     coco_masks = json.load(file)\n",
    "\n",
    "    ########## KH ##########\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket_name = 'w210facetdata'\n",
    "    object_key = 'annotations/coco_masks.json'\n",
    "    s3_object = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    s3_file_content = s3_object['Body'].read().decode('utf-8')\n",
    "    coco_masks = json.loads(s3_file_content)\n",
    "\n",
    "    \n",
    "    cmas = coco_masks[\"annotations\"]\n",
    "\n",
    "    FILENAME_TO_ID = {\n",
    "        img[\"file_name\"]: img[\"id\"]\n",
    "        for img in coco_masks[\"images\"]\n",
    "    }\n",
    "\n",
    "    CAT_TO_LABEL = {cat[\"id\"]: cat[\"name\"] for cat in coco_masks[\"categories\"]}\n",
    "\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        fn = sample.filename\n",
    "\n",
    "        if fn not in FILENAME_TO_ID:\n",
    "            continue\n",
    "\n",
    "        img_id = FILENAME_TO_ID[fn]\n",
    "        img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "        sample_annos = [a for a in cmas if a[\"image_id\"] == img_id]\n",
    "        if len(sample_annos) == 0:\n",
    "            continue\n",
    "\n",
    "        coco_detections = []\n",
    "        for ann in sample_annos:\n",
    "            label = CAT_TO_LABEL[ann[\"category_id\"]]\n",
    "            bbox = ann['bbox']\n",
    "            ann_id = ann['ann_id']\n",
    "            person_id = ann['facet_person_id']\n",
    "\n",
    "            mask = maskUtils.decode(ann[\"segmentation\"])\n",
    "            mask = Image.fromarray(255*mask)\n",
    "\n",
    "            ## Change bbox to be in the format [x, y, x, y]\n",
    "            bbox[2] = bbox[0] + bbox[2]\n",
    "            bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "            ## Get the cropped image\n",
    "            cropped_mask = np.array(mask.crop(bbox)).astype(bool)\n",
    "\n",
    "            ## Convert to relative [x, y, w, h] coordinates\n",
    "            bbox[2] = bbox[2] - bbox[0]\n",
    "            bbox[3] = bbox[3] - bbox[1]\n",
    "\n",
    "            bbox[0] = bbox[0]/img_width\n",
    "            bbox[1] = bbox[1]/img_height\n",
    "            bbox[2] = bbox[2]/img_width\n",
    "            bbox[3] = bbox[3]/img_height\n",
    "\n",
    "            new_detection = fo.Detection(\n",
    "                label=label, \n",
    "                bounding_box=bbox,\n",
    "                person_id=person_id,\n",
    "                ann_id=ann_id,\n",
    "                mask=cropped_mask,\n",
    "                )\n",
    "            coco_detections.append(new_detection)\n",
    "        sample[\"coco_masks\"] = fo.Detections(detections=coco_detections)\n",
    "\n",
    "## add the masks\n",
    "add_coco_masks_to_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a7cb6d-c307-434a-9500-15ae6a4ccadb",
   "metadata": {},
   "source": [
    "# Import Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed90e2-cd2c-4b69-a7a3-d63e9effea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5 = foz.load_zoo_model('yolov5m-coco-torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be206d9-d465-4094-8aff-7b8ede9ccf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(yolov5, label_field=\"yolov5m\")\n",
    "### Just retain the \"person\" detections\n",
    "people_view_values = dataset.filter_labels(\"yolov5m\", F(\"label\") == \"person\").values(\"yolov5m\")\n",
    "dataset.set_values(\"yolov5m\", people_view_values)\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22f25c-8c22-4c4a-ae09-05e07ec88e17",
   "metadata": {},
   "source": [
    "# Clip classification model --> Replace with teacher/student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880f2c8-3d3a-4744-89cb-b69ca608520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a list of all 52 classes\n",
    "facet_classes = dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "## instantiate a CLIP model with these classes\n",
    "clip = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    text_prompt=\"A photo of a\",\n",
    "    classes=facet_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a70100-8fa6-4b34-a754-f86b4b3a52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_view = dataset.to_patches(\"ground_truth\")\n",
    "patch_view.apply_model(clip, label_field=\"clip\")\n",
    "dataset.save_view(\"patch_view\", patch_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f2b7d-5058-4278-81be-32515887d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IOU_THRESHS = np.round(np.arange(0.5, 1.0, 0.05), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14507c5d-78b9-4895-9232-a277c0b61aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_detection_model(dataset, label_field):\n",
    "    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n",
    "    dataset.evaluate_detections(label_field, \"ground_truth\", eval_key=eval_key, classwise=False)\n",
    "    \n",
    "    for sample in dataset.iter_samples(progress=True):\n",
    "        for pred in sample[label_field].detections:\n",
    "            iou_field = f\"{eval_key}_iou\"\n",
    "            if iou_field not in pred:\n",
    "                continue\n",
    "\n",
    "            iou = pred[iou_field]\n",
    "            for it in IOU_THRESHS:\n",
    "                pred[f\"{iou_field}_{str(it).replace('.', '')}\"] = iou >= it\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4e194-37cc-4de9-a76b-51ed0de18ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_evaluate_detection_model(dataset, 'yolov5m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682ce1b-787e-47db-95b8-fb17ed5d9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_detection_mAR(sample_collection, label_field):\n",
    "    \"\"\"Computes the mean average recall of the specified detection field.\n",
    "    -- computed as the average over iou thresholds of the recall at\n",
    "    each threshold.\n",
    "    \"\"\"\n",
    "    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n",
    "    iou_recalls = []\n",
    "    for it in IOU_THRESHS:\n",
    "        field_str = f\"{label_field}.detections.{eval_key}_iou_{str(it).replace('.', '')}\"\n",
    "        counts = sample_collection.count_values(field_str)\n",
    "        tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "        recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "        iou_recalls.append(recall)\n",
    "\n",
    "    return np.mean(iou_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb73ee8-28ac-4416-8d7a-791959206e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_detection_mAR(dataset, label_field, concept, attributes):\n",
    "    sub_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_view = sub_view.filter_labels(\"ground_truth\", F(f\"skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_view = sub_view.filter_labels(\"ground_truth\", F(attribute[0]) == attribute[1])\n",
    "    return _compute_detection_mAR(sub_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651365b-d969-456b-aa02-78a47f6d4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = 'lawman'\n",
    "attributes = {\"hairtype\": \"straight\", \"haircolor\": \"brown\"}\n",
    "get_concept_attr_detection_mAR(dataset, \"yolov5m\", concept, attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320aa339-ddd0-4e4d-800e-aa6ac8d31b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_classification_model(dataset, prediction_field):\n",
    "    patch_view = dataset.load_saved_view(\"patch_view\")\n",
    "    eval_key = \"eval_\" + prediction_field\n",
    "    \n",
    "    for sample in patch_view.iter_samples(progress=True):\n",
    "        sample[eval_key] = (\n",
    "            sample.ground_truth.label == sample[prediction_field].label\n",
    "        )\n",
    "        sample.save()\n",
    "    dataset.save_view(\"patch_view\", patch_view, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffe5b3-dfa1-42ad-bb88-52b04622bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_evaluate_classification_model(dataset, 'clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830a851-7ff9-4ee1-9082-3810dc511cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_recall(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field.split(\"_\")[0]\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5f107-e21f-46de-a8d3-3a329aa88916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_classification_recall(dataset, label_field, concept, attributes):\n",
    "    patch_view = dataset.load_saved_view(\"patch_view\")\n",
    "    sub_patch_view = patch_view.match(F(\"ground_truth.label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_patch_view = sub_patch_view.match(F(f\"ground_truth.skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_patch_view = sub_patch_view.match(F(f\"ground_truth.{attribute[0]}\") == attribute[1])\n",
    "    return _compute_classification_recall(sub_patch_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf03f9-f46d-4ffd-bb0f-e868ad9116ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = {'hairtype': 'curly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60239f5d-239e-4b3d-af81-a81936cab0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_concept_attr_classification_recall(dataset, \"clip\", concept, attribute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c238e25-c163-4774-82dd-f9a96bb3e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_recall(dataset, label_field, concept, attribute):\n",
    "    if label_field in dataset.get_field_schema().keys():\n",
    "        return get_concept_attr_detection_mAR(dataset, label_field, concept, attribute)\n",
    "    else:\n",
    "        return get_concept_attr_classification_recall(dataset, label_field, concept, attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e153b95-8f68-4a3c-a562-f1eee4136004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_disparity(dataset, label_field, concept, attribute1, attribute2):\n",
    "#     recall1 = get_concept_attr_recall(dataset, label_field, concept, attribute1)\n",
    "#     recall2 = get_concept_attr_recall(dataset, label_field, concept, attribute2)\n",
    "#     return recall1 - recall2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55758f91-25ff-4b29-80b2-8e82878221af",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs1 = {\"perceived_gender_presentation\": \"fem\"}\n",
    "attrs2 = {\"hairtype\": \"straight\"}\n",
    "for concept in [\"astronaut\", \"singer\", \"judge\", \"student\"]:\n",
    "    disparity = compute_disparity(dataset, \"clip\", concept, attrs1, attrs2)     \n",
    "    print(f\"{concept}: {disparity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936a1d6-090a-4310-9001-de6b5b39c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_concept_attr_classification_recall(dataset, 'clip', 'singer', attrs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a7918-51d0-41f4-a53f-a2324a250437",
   "metadata": {},
   "source": [
    "# Experimenting with uploading custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9a1dc98-17cc-4059-ae0a-df8e32a22ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.torch as fout\n",
    "from torchvision.models import resnet50\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import fiftyone.core.expressions as foe\n",
    "from fiftyone import ViewField as VF\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from models_package import StudentModel, Student\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e20632e4-2b30-4c89-84a6-b4d0aa7e334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset('IDP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b1137d9-6b03-469e-bebf-6bd050112555",
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_mapping = {\n",
    "    'bartender': 'Chef',\n",
    "    'doctor': 'Doctor',\n",
    "    'carpenter': 'Engineer',\n",
    "    'computer_user': 'Engineer',\n",
    "    'electrician': 'Engineer',\n",
    "    'farmer': 'Farmer',\n",
    "    'gardener': 'Farmer',\n",
    "    'fireman': 'Firefighter',\n",
    "    'judge': 'Judge',\n",
    "    'laborer': 'Mechanic',\n",
    "    'machinist': 'Mechanic',\n",
    "    'astronaut': 'Pilot',\n",
    "    'lawman': 'Police',\n",
    "    'guard': 'Police',\n",
    "    'waiter': 'Waiter',\n",
    "    'soldier': 'Police'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "845a37c7-1e35-4963-a0fb-988e4558adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_label(detections):\n",
    "    for detection in detections.detections:\n",
    "        detection.label = reversed_mapping.get(detection.label, detection.label)\n",
    "        if detection.class2:\n",
    "            detection.class2 = reversed_mapping.get(detection.class2, detection.class2)\n",
    "    return detections\n",
    "\n",
    "# Iterate through the dataset and update the labels\n",
    "for sample in dataset:\n",
    "    sample['ground_truth'] = update_label(sample['ground_truth'])\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4fb3f810-0334-498d-8aea-01051532de8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the student model architecture and weights\n",
    "student_model = StudentModel(in_features=16, num_classes=10)\n",
    "student_model.load_state_dict(torch.load('student_model_weights3.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b2362-9193-45d1-bcd8-2aa80df017a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the number of new classes\n",
    "# num_new_classes = 10\n",
    "\n",
    "# # Modify the student model for the FACET dataset\n",
    "# class AdaptedStudentModel(nn.Module):\n",
    "#     def __init__(self, student_model, num_classes):\n",
    "#         super(AdaptedStudentModel, self).__init__()\n",
    "#         self.features = nn.Sequential(*list(student_model.children())[:-1])  # Preserve the features part of the student model\n",
    "#         self.fc1 = nn.Linear(512, num_classes)  # Adjust the linear layer for the new number of classes\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = x.view(-1, 512)\n",
    "#         x = self.fc1(x)\n",
    "#         return x\n",
    "\n",
    "# adapted_student_model = AdaptedStudentModel(student_model, num_classes=num_new_classes)\n",
    "# model = adapted_student_model\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f06058c4-f25f-4905-8c3f-e82e714e67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_loader(image_paths, sample_ids, batch_size):\n",
    "    mean = [0.5071, 0.4867, 0.4408]\n",
    "    std = [0.2675, 0.2565, 0.2761]\n",
    "    transforms = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize((256, 256)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = fout.TorchImageDataset(\n",
    "        image_paths, sample_ids=sample_ids, transform=transforms\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "model = student_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143c938-3207-4744-8349-183c319a5013",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# class PrototypicalNetwork(nn.Module):\n",
    "#     def __init__(self, backbone, num_classes):\n",
    "#         super(PrototypicalNetwork, self).__init__()\n",
    "#         self.backbone = backbone\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#     def forward(self, support_set, query_set):\n",
    "#         # Extract embeddings for support set and query set\n",
    "#         support_set = self.backbone(support_set) # For 5-shot, 5 images of each FACET class labeled\n",
    "#         query_set = self.backbone(query_set) # All the rest of the FACET images, unlabeled\n",
    "\n",
    "#         # Calculate class prototypes from the support set\n",
    "#         support_set = support_set.view(self.num_classes, -1, support_set.size(1))\n",
    "#         class_prototypes = support_set.mean(dim=1)\n",
    "\n",
    "#         # Calculate similarity scores between query set and class prototypes\n",
    "#         query_set = query_set.view(-1, 1, query_set.size(1))\n",
    "#         class_prototypes = class_prototypes.view(1, -1, class_prototypes.size(1))\n",
    "#         similarities = -((query_set - class_prototypes) ** 2).sum(dim=2)\n",
    "\n",
    "#         return similarities\n",
    "\n",
    "# # Create a few-shot learning model\n",
    "# resnet = nn.Sequential(*list(student.children())[:-1])\n",
    "# num_classes = 52  # Number of classes in the few-shot task\n",
    "# model = PrototypicalNetwork(resnet, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fff460c1-0bdf-46f4-949d-a620be5f2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, imgs):\n",
    "    logits = model(imgs).detach().cpu().numpy()\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    odds = np.exp(logits)\n",
    "    confidences = np.max(odds, axis=1) / np.sum(odds, axis=1)\n",
    "    return predictions, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d813418-51c8-442b-b2da-5e58d5d32886",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "view = dataset.match({\"ground_truth.detections.label\": {\"$in\": ['Police', 'Mechanic', 'Pilot', 'Firefighter', 'Doctor',\n",
    "       'Farmer', 'Engineer', 'Waiter', 'Judge', 'Chef']}})\n",
    "\n",
    "image_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view])\n",
    "data_loader = make_data_loader(image_paths, sample_ids, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "563f5bdc-622a-4109-8cbd-1d54d09f3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    \"9\" : \"Waiter\",\n",
    "    \"6\" : \"Mechanic\",\n",
    "    \"2\" : \"Engineer\",\n",
    "    \"4\" : \"Firefighter\",\n",
    "    \"0\" : \"Chef\",\n",
    "    \"5\" : \"Judge\",\n",
    "    \"7\" : \"Pilot\",\n",
    "    \"8\" : \"Police\",\n",
    "    \"3\" : \"Farmer\",\n",
    "    \"1\" : \"Doctor\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ecb3bdd-31ef-4a66-afee-f6ed696f08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Perform prediction and store results in dataset\n",
    "#\n",
    "\n",
    "\n",
    "for imgs, sample_ids in data_loader:\n",
    "    imgs = imgs.to(device)\n",
    "    predictions, confidences = predict(model, imgs)\n",
    "\n",
    "    # Add predictions to your FiftyOne dataset\n",
    "    for sample_id, prediction, confidence in zip(\n",
    "        sample_ids, predictions, confidences\n",
    "    ):\n",
    "        sample = view[sample_id]\n",
    "        sample[\"pred\"] = fo.Classification(\n",
    "            label=classes[str(prediction)],  # Use the mapping to get class labels\n",
    "            confidence=confidence,\n",
    "        )\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "364c1cc6-e500-4a32-9b5b-6425e21864db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_classification_modelr(dataset, prediction_field):\n",
    "    eval_key = \"eval_\" + prediction_field\n",
    "    \n",
    "    for sample in dataset.iter_samples(progress=True):\n",
    "        labels = []\n",
    "\n",
    "        # Iterate through the detections and extract the \"label\" field\n",
    "        for detection in sample.ground_truth.detections:\n",
    "            label = detection.label\n",
    "            label2 = detection.class2\n",
    "            if label is not None:  # Check if \"label\" exists in the dictionary\n",
    "                labels.append(label)\n",
    "            if label2 is not None: \n",
    "                labels.append(label2)\n",
    "                \n",
    "        if sample[prediction_field].label in labels:\n",
    "            sample[eval_key] = True\n",
    "        else:\n",
    "            sample[eval_key] = False\n",
    "        sample.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1067a92a-6d53-4d62-b512-cb43083d3777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 10510/10510 [39.2s elapsed, 0s remaining, 352.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "_evaluate_classification_modelr(view, 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f7f87bbe-6a3c-40da-86b3-0baee1ba3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_recall(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp > 0 else 1e-6\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "407b491e-e3d7-4d97-9161-554390b1af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_results(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp > 0 else 1e-6\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "204e607b-164f-4326-9971-f09bbd4ce18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_classification_recall(dataset, label_field, concept, attributes):\n",
    "    sub_patch_view_primary = dataset.filter_labels(\"ground_truth\", VF(\"label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_patch_view_primary = sub_patch_view_primary.filter_labels('ground_truth', VF(f\"skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_patch_view_primary = sub_patch_view_primary.filter_labels('ground_truth', VF(f\"{attribute[0]}\") == attribute[1])\n",
    "    primary = _compute_classification_recall(sub_patch_view_primary, label_field)\n",
    "    return primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bff09138-a09c-4f10-9e3f-2ebf3de961f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_disparity(dataset, label_field, concept, attribute1, attribute2):\n",
    "    recall1 = get_concept_attr_classification_recall(dataset, label_field, concept, attribute1)\n",
    "    recall2 = get_concept_attr_classification_recall(dataset, label_field, concept, attribute2)\n",
    "    return (recall1 - recall2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16708d22-e7e6-4ec8-b304-5f1747b042ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Farmer: -0.05137944822071172\n",
      "Police: 0.052292348752525725\n",
      "Engineer: -0.17633865993832354\n",
      "Firefighter: -0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "attrs1 = {\"skin_tone_1\": \"2\"}\n",
    "attrs2 = {\"skin_tone_9\": \"2\"}\n",
    "attrs3 = {\"perceived_gender_presentation\": \"masc\"}\n",
    "attrs4 = {\"perceived_gender_presentation\": \"fem\"}\n",
    "for concept in [\"Farmer\", \"Police\", \"Engineer\", \"Firefighter\"]:\n",
    "    disparity = compute_disparity(view, \"pred\", concept, attrs1, attrs2)     \n",
    "    print(f\"{concept}: {disparity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9fee5b8d-3dd1-4841-94e5-b688c378082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Police:0.21349963316214232\n",
      "Mechanic:0.2577903682719547\n",
      "Pilot:0.0196078431372549\n",
      "Firefighter:0.16733067729083664\n",
      "Doctor:0.1712\n",
      "Farmer:0.8025435073627845\n",
      "Engineer:0.17090216010165185\n",
      "Waiter:0.08888888888888889\n",
      "Judge:0.125\n",
      "Chef:0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "concepts = ['Police', 'Mechanic', 'Pilot', 'Firefighter', 'Doctor',\n",
    "       'Farmer', 'Engineer', 'Waiter', 'Judge', 'Chef']\n",
    "\n",
    "for i in concepts: \n",
    "    print(f'{i}:{_compute_classification_recall(view.filter_labels(\"ground_truth\", VF(\"label\") == i), \"pred\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c7f5d973-7bfb-4288-afe4-d8b6aca159d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Police:{False: 3216, True: 873}\n",
      "Mechanic:{False: 1310, True: 455}\n",
      "Pilot:{False: 200, True: 4}\n",
      "Firefighter:{False: 418, True: 84}\n",
      "Doctor:{False: 518, True: 107}\n",
      "Farmer:{False: 295, True: 1199}\n",
      "Engineer:{False: 1305, True: 269}\n",
      "Waiter:{False: 246, True: 24}\n",
      "Judge:{False: 42, True: 6}\n",
      "Chef:{False: 40, True: 4}\n"
     ]
    }
   ],
   "source": [
    "for i in concepts: \n",
    "    print(f'{i}:{_compute_classification_results(view.filter_labels(\"ground_truth\", VF(\"label\") == i), \"pred\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b583c86b-eeb4-46cf-8ed8-2e1e4a652f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for i in view.iter_samples(): \n",
    "    preds.append(i.pred.label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8fcac311-f6a8-49f4-bca7-9edf63851a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample list of data\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(preds, bins=10, edgecolor='k')  # 'bins' determines the number of bins or bars\n",
    "\n",
    "# Add labels and a title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram Example')\n",
    "\n",
    "# Show the histogram\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c95cf-5315-4116-89a6-7c63d88521ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
