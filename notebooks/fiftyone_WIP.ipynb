{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0a8b3b-5189-4857-887c-e93b637bf000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.28.68 requires botocore<1.32.0,>=1.31.68, but you have botocore 1.31.64 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.7.0 requires botocore<1.31.65,>=1.31.16, but you have botocore 1.31.73 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install for AWS\n",
    "!pip install torch --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install scikit-image --quiet\n",
    "!pip install scikit-learn --quiet\n",
    "!pip install matplotlib --quiet\n",
    "!pip install torchvision --quiet\n",
    "!pip install s3fs --quiet\n",
    "!pip install boto3 --quiet\n",
    "!pip install tqdm --quiet\n",
    "!pip install fiftyone --quiet\n",
    "!pip install pycocotools --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c928e2-c18b-457c-8640-f03b4b11b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tarfile\n",
    "import shutil\n",
    "import torchvision\n",
    "import random\n",
    "import warnings\n",
    "import boto3\n",
    "import s3fs\n",
    "import io\n",
    "import time\n",
    "import botocore.exceptions\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import getpass\n",
    "import json\n",
    "# import torch.jit as jit\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, utils, models, datasets\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f329f00-bc92-4a2c-9f76-9d7a8f3aff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LM ##########\n",
    "\n",
    "# access_key = getpass.getpass(\"Enter your access: \")\n",
    "\n",
    "# secret_key = password = getpass.getpass(\"Enter your secret: \")\n",
    "\n",
    "# bucket_name = 'w210facetdata'\n",
    "# annotations_prefix = 'annotations/'\n",
    "# images_prefix = '/home/ubuntu/W210-Capstone'\n",
    "\n",
    "# s3 = s3fs.S3FileSystem(key=access_key, secret=secret_key)\n",
    "\n",
    "# # Use s3.open to open the CSV file and read its content into a Pandas DataFrame\n",
    "# with s3.open(f's3://{bucket_name}/{annotations_prefix}annotations.csv', 'rb') as file:\n",
    "#     gt_df = pd.read_csv(file)\n",
    "\n",
    "# ## use relative paths to your image dirs\n",
    "# # dataset = fo.Dataset(name = \"FACET14\", persistent=True)\n",
    "# dataset = fo.load_dataset('FACET14')\n",
    "# # dataset.add_images_dir(images_prefix)\n",
    "# dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7a9c86-d180-4065-b902-93d75cf2515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## KH ##########\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3', region_name='us-west-2')\n",
    "\n",
    "# Define the S3 bucket name and prefixes\n",
    "bucket_name = 'w210facetdata'\n",
    "annotations_prefix = 'annotations/'\n",
    "images_prefix = 'images/'\n",
    "\n",
    "# Load CSV annotations from S3\n",
    "annotations_s3_path = f's3://{bucket_name}/{annotations_prefix}'\n",
    "gt_df = pd.read_csv(f'{annotations_s3_path}annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04df1e08-51f4-42ee-9459-9f3fa86b1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## KH ##########\n",
    "local_images_dir = 'local_images_dir'\n",
    "os.makedirs(local_images_dir, exist_ok=True)\n",
    "\n",
    "# Create a paginator to handle pagination of the results\n",
    "paginator = s3_client.get_paginator('list_objects_v2')\n",
    "\n",
    "# Use the paginator to retrieve all objects\n",
    "for page in paginator.paginate(Bucket=bucket_name, Prefix=images_prefix):\n",
    "    for obj in page.get('Contents', []):\n",
    "        # Skip the prefix itself\n",
    "        if obj['Key'] == images_prefix:\n",
    "            continue\n",
    "        local_file_path = os.path.join(local_images_dir, os.path.basename(obj['Key']))\n",
    "        s3_client.download_file(bucket_name, obj['Key'], local_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f0cfd8a-0b73-4f7b-8b4a-33af7033b4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 31703/31703 [4.8s elapsed, 0s remaining, 6.8K samples/s]      \n",
      "Computing metadata...\n",
      " 100% |█████████████| 31703/31703 [59.6s elapsed, 0s remaining, 603.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "########## KH ##########\n",
    "# local_images_dir = 'local_images_dir'\n",
    "\n",
    "# fo.delete_dataset('local_images_dir')\n",
    "dataset = fo.Dataset(name='local_images_dir')\n",
    "\n",
    "dataset.add_images_dir(local_images_dir)\n",
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dc58bbf-ed5e-4f3a-b86a-77e58abb1ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31702\n"
     ]
    }
   ],
   "source": [
    "# Count the number of files in the local_images_dir\n",
    "num_files = len([f for f in os.listdir(local_images_dir) if os.path.isfile(os.path.join(local_images_dir, f))])\n",
    "print(num_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c721ef-146f-4f48-a411-7fb93bca9d84",
   "metadata": {},
   "source": [
    "# Object Detection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be99a8b0-a415-4560-a578-2b9f8bba27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOLEAN_PERSONAL_ATTRS = (\n",
    "    \"has_facial_hair\",\n",
    "    \"has_tattoo\",\n",
    "    \"has_cap\",\n",
    "    \"has_mask\",\n",
    "    \"has_headscarf\",\n",
    "    \"has_eyeware\",\n",
    ")\n",
    "def add_boolean_person_attributes(detection, row_index):\n",
    "    for attr in BOOLEAN_PERSONAL_ATTRS:\n",
    "        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "892ad9c7-cb94-49ee-bbe6-caf72e3e7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hairtype(row_index):\n",
    "    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hairtype')]\n",
    "    hairtype = hair_info[hair_info == 1]\n",
    "    if len(hairtype) == 0:\n",
    "        return None\n",
    "    return hairtype.index[0].split('_')[1]\n",
    "\n",
    "def get_haircolor(row_index):\n",
    "    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hair_color')]\n",
    "    haircolor = hair_info[hair_info == 1]\n",
    "    if len(haircolor) == 0:\n",
    "        return None\n",
    "    return haircolor.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8fc5510-2fd8-42f9-a58a-c301286b3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_person_attributes(detection, row_index):\n",
    "    detection[\"hairtype\"] = get_hairtype(row_index)\n",
    "    detection[\"haircolor\"] = get_haircolor(row_index)\n",
    "    add_boolean_person_attributes(detection, row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0091b7eb-d570-4b21-8290-17fee15d226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perceived_gender_presentation(row_index):\n",
    "    gender_info = gt_df.loc[row_index, gt_df.columns.str.startswith('gender')]\n",
    "    pgp = gender_info[gender_info == 1]\n",
    "    if len(pgp) == 0:\n",
    "        return None\n",
    "    return pgp.index[0].replace(\"gender_presentation_\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "def get_perceived_age_presentation(row_index):\n",
    "    age_info = gt_df.loc[row_index, gt_df.columns.str.startswith('age')]\n",
    "    pap = age_info[age_info == 1]\n",
    "    if len(pap) == 0:\n",
    "        return None\n",
    "    return pap.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c49856f0-9c40-44f1-b8dc-afc8a0c80786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skintone(row_index):\n",
    "    skin_info = gt_df.loc[row_index, gt_df.columns.str.startswith('skin_tone')]\n",
    "    return skin_info.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a702e8b-93a2-4e8b-879c-f82be93830ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_protected_attributes(detection, row_index):\n",
    "    detection[\"perceived_age_presentation\"] = get_perceived_age_presentation(row_index)\n",
    "    detection[\"perceived_gender_presentation\"] = get_perceived_gender_presentation(row_index)\n",
    "    detection[\"skin_tone\"] = get_skintone(row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01f3a95d-e491-4de4-be5a-d53de547f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISIBILITY_ATTRS = (\"visible_torso\", \"visible_face\", \"visible_minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc9ed1ed-c71d-4583-963f-bfa23563309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lighting(row_index):\n",
    "    lighting_info = gt_df.loc[row_index, gt_df.columns.str.startswith('lighting')]\n",
    "    lighting = lighting_info[lighting_info == 1]\n",
    "    if len(lighting) == 0:\n",
    "        return None\n",
    "    lighting = lighting.index[0].replace(\"lighting_\", \"\").replace(\"_\", \" \")\n",
    "    return lighting\n",
    "\n",
    "def add_other_attributes(detection, row_index):\n",
    "    detection[\"lighting\"] = get_lighting(row_index)\n",
    "    for attr in VISIBILITY_ATTRS:\n",
    "        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3de3c768-bfa3-4727-95ed-59346a5788a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detection(row_index, sample):\n",
    "    bbox_dict = json.loads(gt_df.loc[row_index, \"bounding_box\"])\n",
    "    x, y, w, h = bbox_dict[\"x\"], bbox_dict[\"y\"], bbox_dict[\"width\"], bbox_dict[\"height\"]\n",
    "    cat1, cat2 = bbox_dict[\"dict_attributes\"][\"cat1\"], bbox_dict[\"dict_attributes\"][\"cat2\"]\n",
    "\n",
    "    person_id = gt_df.loc[row_index, \"person_id\"]\n",
    "\n",
    "    img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "\n",
    "    bounding_box = [x/img_width, y/img_height, w/img_width, h/img_height]\n",
    "    detection = fo.Detection(\n",
    "        label=cat1, \n",
    "        bounding_box=bounding_box,\n",
    "        person_id=person_id,\n",
    "        )\n",
    "    if cat2 != 'none':\n",
    "        detection[\"class2\"] = cat2\n",
    "\n",
    "    add_person_attributes(detection, row_index)\n",
    "    add_protected_attributes(detection, row_index)\n",
    "    add_other_attributes(detection, row_index)\n",
    "\n",
    "    return detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1869986-c9eb-454e-b164-cd5f8b598286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ground_truth_labels(dataset):\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        sample_annos = gt_df[gt_df['filename'] == sample.filename]\n",
    "        detections = []\n",
    "        for row in sample_annos.iterrows():\n",
    "            row_index = row[0]\n",
    "            detection = create_detection(row_index, sample)\n",
    "            detections.append(detection)\n",
    "        sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "    dataset.add_dynamic_sample_fields()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec896ec-9aa5-4033-839d-ced0b1ae54d6",
   "metadata": {},
   "source": [
    "# Add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d068d1d0-38e0-43b7-b827-5006503182d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 31703/31703 [5.7m elapsed, 0s remaining, 91.1 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "## add all of the ground truth labels\n",
    "add_ground_truth_labels(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e53d3-c90d-467e-83c2-6b1bc5717fdd",
   "metadata": {},
   "source": [
    "# Add Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b485560-887e-47f9-b980-a9460a481942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  36% |████---------| 11528/31703 [10.0m elapsed, 17.9m remaining, 18.2 samples/s]   "
     ]
    }
   ],
   "source": [
    "def add_coco_masks_to_dataset(dataset):\n",
    "    ########## LM ##########\n",
    "    # with s3.open(f's3://{bucket_name}/{annotations_prefix}coco_masks.json', 'rb') as file:\n",
    "    #     coco_masks = json.load(file)\n",
    "\n",
    "    ########## KH ##########\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket_name = 'w210facetdata'\n",
    "    object_key = 'annotations/coco_masks.json'\n",
    "    s3_object = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    s3_file_content = s3_object['Body'].read().decode('utf-8')\n",
    "    coco_masks = json.loads(s3_file_content)\n",
    "\n",
    "    \n",
    "    cmas = coco_masks[\"annotations\"]\n",
    "\n",
    "    FILENAME_TO_ID = {\n",
    "        img[\"file_name\"]: img[\"id\"]\n",
    "        for img in coco_masks[\"images\"]\n",
    "    }\n",
    "\n",
    "    CAT_TO_LABEL = {cat[\"id\"]: cat[\"name\"] for cat in coco_masks[\"categories\"]}\n",
    "\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        fn = sample.filename\n",
    "\n",
    "        if fn not in FILENAME_TO_ID:\n",
    "            continue\n",
    "\n",
    "        img_id = FILENAME_TO_ID[fn]\n",
    "        img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "        sample_annos = [a for a in cmas if a[\"image_id\"] == img_id]\n",
    "        if len(sample_annos) == 0:\n",
    "            continue\n",
    "\n",
    "        coco_detections = []\n",
    "        for ann in sample_annos:\n",
    "            label = CAT_TO_LABEL[ann[\"category_id\"]]\n",
    "            bbox = ann['bbox']\n",
    "            ann_id = ann['ann_id']\n",
    "            person_id = ann['facet_person_id']\n",
    "\n",
    "            mask = maskUtils.decode(ann[\"segmentation\"])\n",
    "            mask = Image.fromarray(255*mask)\n",
    "\n",
    "            ## Change bbox to be in the format [x, y, x, y]\n",
    "            bbox[2] = bbox[0] + bbox[2]\n",
    "            bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "            ## Get the cropped image\n",
    "            cropped_mask = np.array(mask.crop(bbox)).astype(bool)\n",
    "\n",
    "            ## Convert to relative [x, y, w, h] coordinates\n",
    "            bbox[2] = bbox[2] - bbox[0]\n",
    "            bbox[3] = bbox[3] - bbox[1]\n",
    "\n",
    "            bbox[0] = bbox[0]/img_width\n",
    "            bbox[1] = bbox[1]/img_height\n",
    "            bbox[2] = bbox[2]/img_width\n",
    "            bbox[3] = bbox[3]/img_height\n",
    "\n",
    "            new_detection = fo.Detection(\n",
    "                label=label, \n",
    "                bounding_box=bbox,\n",
    "                person_id=person_id,\n",
    "                ann_id=ann_id,\n",
    "                mask=cropped_mask,\n",
    "                )\n",
    "            coco_detections.append(new_detection)\n",
    "        sample[\"coco_masks\"] = fo.Detections(detections=coco_detections)\n",
    "\n",
    "## add the masks\n",
    "add_coco_masks_to_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a7cb6d-c307-434a-9500-15ae6a4ccadb",
   "metadata": {},
   "source": [
    "# Import Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed90e2-cd2c-4b69-a7a3-d63e9effea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5 = foz.load_zoo_model('yolov5m-coco-torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be206d9-d465-4094-8aff-7b8ede9ccf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(yolov5, label_field=\"yolov5m\")\n",
    "### Just retain the \"person\" detections\n",
    "people_view_values = dataset.filter_labels(\"yolov5m\", F(\"label\") == \"person\").values(\"yolov5m\")\n",
    "dataset.set_values(\"yolov5m\", people_view_values)\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22f25c-8c22-4c4a-ae09-05e07ec88e17",
   "metadata": {},
   "source": [
    "# Clip classification model --> Replace with teacher/student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880f2c8-3d3a-4744-89cb-b69ca608520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a list of all 52 classes\n",
    "facet_classes = dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "## instantiate a CLIP model with these classes\n",
    "clip = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    text_prompt=\"A photo of a\",\n",
    "    classes=facet_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a70100-8fa6-4b34-a754-f86b4b3a52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_view = dataset.to_patches(\"ground_truth\")\n",
    "patch_view.apply_model(clip, label_field=\"clip\")\n",
    "dataset.save_view(\"patch_view\", patch_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f2b7d-5058-4278-81be-32515887d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IOU_THRESHS = np.round(np.arange(0.5, 1.0, 0.05), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14507c5d-78b9-4895-9232-a277c0b61aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_detection_model(dataset, label_field):\n",
    "    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n",
    "    dataset.evaluate_detections(label_field, \"ground_truth\", eval_key=eval_key, classwise=False)\n",
    "    \n",
    "    for sample in dataset.iter_samples(progress=True):\n",
    "        for pred in sample[label_field].detections:\n",
    "            iou_field = f\"{eval_key}_iou\"\n",
    "            if iou_field not in pred:\n",
    "                continue\n",
    "\n",
    "            iou = pred[iou_field]\n",
    "            for it in IOU_THRESHS:\n",
    "                pred[f\"{iou_field}_{str(it).replace('.', '')}\"] = iou >= it\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4e194-37cc-4de9-a76b-51ed0de18ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_evaluate_detection_model(dataset, 'yolov5m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682ce1b-787e-47db-95b8-fb17ed5d9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_detection_mAR(sample_collection, label_field):\n",
    "    \"\"\"Computes the mean average recall of the specified detection field.\n",
    "    -- computed as the average over iou thresholds of the recall at\n",
    "    each threshold.\n",
    "    \"\"\"\n",
    "    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n",
    "    iou_recalls = []\n",
    "    for it in IOU_THRESHS:\n",
    "        field_str = f\"{label_field}.detections.{eval_key}_iou_{str(it).replace('.', '')}\"\n",
    "        counts = sample_collection.count_values(field_str)\n",
    "        tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "        recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "        iou_recalls.append(recall)\n",
    "\n",
    "    return np.mean(iou_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb73ee8-28ac-4416-8d7a-791959206e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_detection_mAR(dataset, label_field, concept, attributes):\n",
    "    sub_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_view = sub_view.filter_labels(\"ground_truth\", F(f\"skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_view = sub_view.filter_labels(\"ground_truth\", F(attribute[0]) == attribute[1])\n",
    "    return _compute_detection_mAR(sub_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651365b-d969-456b-aa02-78a47f6d4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = 'lawman'\n",
    "attributes = {\"hairtype\": \"straight\", \"haircolor\": \"brown\"}\n",
    "get_concept_attr_detection_mAR(dataset, \"yolov5m\", concept, attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320aa339-ddd0-4e4d-800e-aa6ac8d31b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_classification_model(dataset, prediction_field):\n",
    "    patch_view = dataset.load_saved_view(\"patch_view\")\n",
    "    eval_key = \"eval_\" + prediction_field\n",
    "    \n",
    "    for sample in patch_view.iter_samples(progress=True):\n",
    "        sample[eval_key] = (\n",
    "            sample.ground_truth.label == sample[prediction_field].label\n",
    "        )\n",
    "        sample.save()\n",
    "    dataset.save_view(\"patch_view\", patch_view, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffe5b3-dfa1-42ad-bb88-52b04622bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_evaluate_classification_model(dataset, 'clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830a851-7ff9-4ee1-9082-3810dc511cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_recall(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field.split(\"_\")[0]\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5f107-e21f-46de-a8d3-3a329aa88916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_classification_recall(dataset, label_field, concept, attributes):\n",
    "    patch_view = dataset.load_saved_view(\"patch_view\")\n",
    "    sub_patch_view = patch_view.match(F(\"ground_truth.label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_patch_view = sub_patch_view.match(F(f\"ground_truth.skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_patch_view = sub_patch_view.match(F(f\"ground_truth.{attribute[0]}\") == attribute[1])\n",
    "    return _compute_classification_recall(sub_patch_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf03f9-f46d-4ffd-bb0f-e868ad9116ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = {'hairtype': 'curly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60239f5d-239e-4b3d-af81-a81936cab0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_concept_attr_classification_recall(dataset, \"clip\", concept, attribute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c238e25-c163-4774-82dd-f9a96bb3e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_recall(dataset, label_field, concept, attribute):\n",
    "    if label_field in dataset.get_field_schema().keys():\n",
    "        return get_concept_attr_detection_mAR(dataset, label_field, concept, attribute)\n",
    "    else:\n",
    "        return get_concept_attr_classification_recall(dataset, label_field, concept, attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e153b95-8f68-4a3c-a562-f1eee4136004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_disparity(dataset, label_field, concept, attribute1, attribute2):\n",
    "    recall1 = get_concept_attr_recall(dataset, label_field, concept, attribute1)\n",
    "    recall2 = get_concept_attr_recall(dataset, label_field, concept, attribute2)\n",
    "    return recall1 - recall2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55758f91-25ff-4b29-80b2-8e82878221af",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs1 = {\"perceived_gender_presentation\": \"fem\"}\n",
    "attrs2 = {\"hairtype\": \"straight\"}\n",
    "for concept in [\"astronaut\", \"singer\", \"judge\", \"student\"]:\n",
    "    disparity = compute_disparity(dataset, \"clip\", concept, attrs1, attrs2)     \n",
    "    print(f\"{concept}: {disparity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936a1d6-090a-4310-9001-de6b5b39c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_concept_attr_classification_recall(dataset, 'clip', 'singer', attrs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a7918-51d0-41f4-a53f-a2324a250437",
   "metadata": {},
   "source": [
    "# Experimenting with uploading custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9a1dc98-17cc-4059-ae0a-df8e32a22ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.torch as fout\n",
    "from torchvision.models import resnet50\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import fiftyone.core.expressions as foe\n",
    "from fiftyone import ViewField as VF\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from models_package import StudentModel, Student\n",
    "import numpy as np\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e20632e4-2b30-4c89-84a6-b4d0aa7e334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.load_dataset('FACET14')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fb3f810-0334-498d-8aea-01051532de8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the student model architecture and weights\n",
    "student_model = torchvision.models.resnet18(weights=None)\n",
    "student_model.load_state_dict(torch.load('student_model_weights_ckd_1.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab9b2362-9193-45d1-bcd8-2aa80df017a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaptedStudentModel(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (fc1): Linear(in_features=512, out_features=52, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the number of new classes\n",
    "num_new_classes = 52\n",
    "\n",
    "# Modify the student model for the FACET dataset\n",
    "class AdaptedStudentModel(nn.Module):\n",
    "    def __init__(self, student_model, num_classes):\n",
    "        super(AdaptedStudentModel, self).__init__()\n",
    "        self.features = nn.Sequential(*list(student_model.children())[:-1])  # Preserve the features part of the student model\n",
    "        self.fc1 = nn.Linear(512, num_classes)  # Adjust the linear layer for the new number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 512)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "adapted_student_model = AdaptedStudentModel(student_model, num_classes=num_new_classes)\n",
    "model = adapted_student_model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f06058c4-f25f-4905-8c3f-e82e714e67e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrototypicalNetwork(\n",
       "  (backbone): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_data_loader(image_paths, sample_ids, batch_size):\n",
    "    mean = [0.5071, 0.4867, 0.4408]\n",
    "    std = [0.2675, 0.2565, 0.2761]\n",
    "    transforms = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize((256, 256)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = fout.TorchImageDataset(\n",
    "        image_paths, sample_ids=sample_ids, transform=transforms\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5143c938-3207-4744-8349-183c319a5013",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, support_set, query_set):\n",
    "        # Extract embeddings for support set and query set\n",
    "        support_set = self.backbone(support_set)\n",
    "        query_set = self.backbone(query_set)\n",
    "\n",
    "        # Calculate class prototypes from the support set\n",
    "        support_set = support_set.view(self.num_classes, -1, support_set.size(1))\n",
    "        class_prototypes = support_set.mean(dim=1)\n",
    "\n",
    "        # Calculate similarity scores between query set and class prototypes\n",
    "        query_set = query_set.view(-1, 1, query_set.size(1))\n",
    "        class_prototypes = class_prototypes.view(1, -1, class_prototypes.size(1))\n",
    "        similarities = -((query_set - class_prototypes) ** 2).sum(dim=2)\n",
    "\n",
    "        return similarities\n",
    "\n",
    "# Create a few-shot learning model\n",
    "resnet = nn.Sequential(*list(student.children())[:-1])\n",
    "num_classes = 52  # Number of classes in the few-shot task\n",
    "model = PrototypicalNetwork(resnet, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fff460c1-0bdf-46f4-949d-a620be5f2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, imgs):\n",
    "    logits = model(imgs).detach().cpu().numpy()\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    odds = np.exp(logits)\n",
    "    confidences = np.max(odds, axis=1) / np.sum(odds, axis=1)\n",
    "    return predictions, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d813418-51c8-442b-b2da-5e58d5d32886",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "view = dataset.take(num_samples, seed=51)\n",
    "classes = []\n",
    "for i in view.iter_samples():\n",
    "    classes.append(i.ground_truth.detections[0].label)\n",
    "\n",
    "image_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view])\n",
    "data_loader = make_data_loader(image_paths, sample_ids, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0ecb3bdd-31ef-4a66-afee-f6ed696f08cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[98], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m---> 11\u001b[0m     support_set, query_set, labels \u001b[38;5;241m=\u001b[39m batch  \u001b[38;5;66;03m# Use the DataLoader to load batches of query set examples\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Calculate similarities\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m model(support_set, query_set)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Perform prediction and store results in dataset\n",
    "#\n",
    "\n",
    "\n",
    "for imgs, sample_ids in data_loader:\n",
    "    imgs = imgs.to(device)\n",
    "    predictions, confidences = predict(model, imgs)\n",
    "\n",
    "    # Add predictions to your FiftyOne dataset\n",
    "    for sample_id, prediction, confidence in zip(\n",
    "        sample_ids, predictions, confidences\n",
    "    ):\n",
    "        sample = dataset[sample_id]\n",
    "        sample[\"pred\"] = fo.Classification(\n",
    "            label=classes[prediction],  # Use the mapping to get class labels\n",
    "            confidence=confidence,\n",
    "        )\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "364c1cc6-e500-4a32-9b5b-6425e21864db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_classification_modelr(dataset, prediction_field):\n",
    "    eval_key = \"eval_\" + prediction_field\n",
    "    \n",
    "    for sample in dataset.iter_samples(progress=True):\n",
    "        sample[eval_key] = (\n",
    "            sample.ground_truth.detections[0].label == sample[prediction_field].label\n",
    "        )\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6e5764e5-4859-4b2b-8e04-68224437e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _evaluate_classification_modelr(dataset, prediction_field):\n",
    "#     eval_key = \"eval_\" + prediction_field\n",
    "    \n",
    "#     for sample in dataset.iter_samples(progress=True):\n",
    "#         # Check if ground_truth.detections is not empty\n",
    "#         if sample.ground_truth.detections:\n",
    "#             sample[eval_key] = (\n",
    "#                 sample.ground_truth.detections[0].label == sample[prediction_field].label\n",
    "#             )\n",
    "#             sample.save()\n",
    "#         else:\n",
    "#             print(f\"Sample {sample.id} has no detections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1067a92a-6d53-4d62-b512-cb43083d3777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 10000/10000 [2.7m elapsed, 0s remaining, 67.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "_evaluate_classification_modelr(view, 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7f87bbe-6a3c-40da-86b3-0baee1ba3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_recall(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "204e607b-164f-4326-9971-f09bbd4ce18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_classification_recall(dataset, label_field, concept, attributes):\n",
    "    sub_patch_view = dataset.filter_labels(\"ground_truth\", VF(\"label\") == concept)  # Use foe instead of F\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_patch_view = sub_patch_view.filter_labels('ground_truth', VF(f\"skin_tone.{attribute[0]}\") != 0)  # Use foe instead of F\n",
    "        else:\n",
    "            sub_patch_view = sub_patch_view.filter_labels('ground_truth', VF(f\"{attribute[0]}\") == attribute[1])  # Use foe instead of F\n",
    "    return _compute_classification_recall(sub_patch_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c2980f61-f140-4a86-9e41-fee7161fb3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01282051282051282"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept = 'hairdresser'\n",
    "attributes = {\"hairtype\": \"straight\"}\n",
    "\n",
    "get_concept_attr_classification_recall(view, 'pred', concept, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8fd16a3a-4212-486d-9107-5011a92feae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 10000/10000 [43.9s elapsed, 0s remaining, 224.7 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "labels = set()\n",
    "for sample in view.iter_samples(progress=True):\n",
    "    labels.add(sample.ground_truth.detections[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1e3fc57c-4357-407f-b77e-6bedeea66241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'astronaut',\n",
       " 'backpacker',\n",
       " 'ballplayer',\n",
       " 'bartender',\n",
       " 'basketball_player',\n",
       " 'boatman',\n",
       " 'carpenter',\n",
       " 'cheerleader',\n",
       " 'climber',\n",
       " 'computer_user',\n",
       " 'craftsman',\n",
       " 'dancer',\n",
       " 'disk_jockey',\n",
       " 'doctor',\n",
       " 'drummer',\n",
       " 'electrician',\n",
       " 'farmer',\n",
       " 'fireman',\n",
       " 'flutist',\n",
       " 'gardener',\n",
       " 'guard',\n",
       " 'guitarist',\n",
       " 'gymnast',\n",
       " 'hairdresser',\n",
       " 'horseman',\n",
       " 'judge',\n",
       " 'laborer',\n",
       " 'lawman',\n",
       " 'lifeguard',\n",
       " 'machinist',\n",
       " 'motorcyclist',\n",
       " 'nurse',\n",
       " 'painter',\n",
       " 'patient',\n",
       " 'prayer',\n",
       " 'referee',\n",
       " 'repairman',\n",
       " 'reporter',\n",
       " 'retailer',\n",
       " 'runner',\n",
       " 'sculptor',\n",
       " 'seller',\n",
       " 'singer',\n",
       " 'skateboarder',\n",
       " 'soccer_player',\n",
       " 'soldier',\n",
       " 'speaker',\n",
       " 'student',\n",
       " 'teacher',\n",
       " 'tennis_player',\n",
       " 'trumpeter',\n",
       " 'waiter'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9b2d41d-ae7a-4c9a-bd2e-9c4bd988babd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "640.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2560/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faead1e-1785-4b48-82e2-f9201edc0657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b653ec-6918-4079-acc1-e4b20cf77825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
