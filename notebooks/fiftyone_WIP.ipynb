{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0a8b3b-5189-4857-887c-e93b637bf000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.28.68 requires botocore<1.32.0,>=1.31.68, but you have botocore 1.31.64 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.7.0 requires botocore<1.31.65,>=1.31.16, but you have botocore 1.31.73 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install for AWS\n",
    "!pip install torch --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install scikit-image --quiet\n",
    "!pip install scikit-learn --quiet\n",
    "!pip install matplotlib --quiet\n",
    "!pip install torchvision --quiet\n",
    "!pip install s3fs --quiet\n",
    "!pip install boto3 --quiet\n",
    "!pip install tqdm --quiet\n",
    "!pip install fiftyone --quiet\n",
    "!pip install pycocotools --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c928e2-c18b-457c-8640-f03b4b11b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tarfile\n",
    "import shutil\n",
    "import torchvision\n",
    "import random\n",
    "import warnings\n",
    "import boto3\n",
    "import s3fs\n",
    "import io\n",
    "import time\n",
    "import botocore.exceptions\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import getpass\n",
    "import json\n",
    "# import torch.jit as jit\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, utils, models, datasets\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f329f00-bc92-4a2c-9f76-9d7a8f3aff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## LM ##########\n",
    "\n",
    "# access_key = getpass.getpass(\"Enter your access: \")\n",
    "\n",
    "# secret_key = password = getpass.getpass(\"Enter your secret: \")\n",
    "\n",
    "# bucket_name = 'w210facetdata'\n",
    "# annotations_prefix = 'annotations/'\n",
    "# images_prefix = '/home/ubuntu/W210-Capstone'\n",
    "\n",
    "# s3 = s3fs.S3FileSystem(key=access_key, secret=secret_key)\n",
    "\n",
    "# # Use s3.open to open the CSV file and read its content into a Pandas DataFrame\n",
    "# with s3.open(f's3://{bucket_name}/{annotations_prefix}annotations.csv', 'rb') as file:\n",
    "#     gt_df = pd.read_csv(file)\n",
    "\n",
    "# ## use relative paths to your image dirs\n",
    "# # dataset = fo.Dataset(name = \"FACET14\", persistent=True)\n",
    "# dataset = fo.load_dataset('FACET14')\n",
    "# # dataset.add_images_dir(images_prefix)\n",
    "# dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7a9c86-d180-4065-b902-93d75cf2515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## KH ##########\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3', region_name='us-west-2')\n",
    "\n",
    "# Define the S3 bucket name and prefixes\n",
    "bucket_name = 'w210facetdata'\n",
    "annotations_prefix = 'annotations/'\n",
    "images_prefix = 'images/'\n",
    "\n",
    "# Load CSV annotations from S3\n",
    "annotations_s3_path = f's3://{bucket_name}/{annotations_prefix}'\n",
    "gt_df = pd.read_csv(f'{annotations_s3_path}annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df1e08-51f4-42ee-9459-9f3fa86b1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## KH ##########\n",
    "local_images_dir = 'local_images_dir'\n",
    "os.makedirs(local_images_dir, exist_ok=True)\n",
    "\n",
    "# Create a paginator to handle pagination of the results\n",
    "paginator = s3_client.get_paginator('list_objects_v2')\n",
    "\n",
    "# Use the paginator to retrieve all objects\n",
    "for page in paginator.paginate(Bucket=bucket_name, Prefix=images_prefix):\n",
    "    for obj in page.get('Contents', []):\n",
    "        # Skip the prefix itself\n",
    "        if obj['Key'] == images_prefix:\n",
    "            continue\n",
    "        local_file_path = os.path.join(local_images_dir, os.path.basename(obj['Key']))\n",
    "        s3_client.download_file(bucket_name, obj['Key'], local_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0cfd8a-0b73-4f7b-8b4a-33af7033b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## KH ##########\n",
    "local_images_dir = 'local_images_dir'\n",
    "\n",
    "fo.delete_dataset('local_images_dir')\n",
    "dataset = fo.Dataset(name='local_images_dir')\n",
    "\n",
    "dataset.add_images_dir(local_images_dir)\n",
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc58bbf-ed5e-4f3a-b86a-77e58abb1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of files in the local_images_dir\n",
    "num_files = len([f for f in os.listdir(local_images_dir) if os.path.isfile(os.path.join(local_images_dir, f))])\n",
    "print(num_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c721ef-146f-4f48-a411-7fb93bca9d84",
   "metadata": {},
   "source": [
    "# Object Detection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99a8b0-a415-4560-a578-2b9f8bba27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOLEAN_PERSONAL_ATTRS = (\n",
    "    \"has_facial_hair\",\n",
    "    \"has_tattoo\",\n",
    "    \"has_cap\",\n",
    "    \"has_mask\",\n",
    "    \"has_headscarf\",\n",
    "    \"has_eyeware\",\n",
    ")\n",
    "def add_boolean_person_attributes(detection, row_index):\n",
    "    for attr in BOOLEAN_PERSONAL_ATTRS:\n",
    "        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ad9c7-cb94-49ee-bbe6-caf72e3e7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hairtype(row_index):\n",
    "    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hairtype')]\n",
    "    hairtype = hair_info[hair_info == 1]\n",
    "    if len(hairtype) == 0:\n",
    "        return None\n",
    "    return hairtype.index[0].split('_')[1]\n",
    "\n",
    "def get_haircolor(row_index):\n",
    "    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hair_color')]\n",
    "    haircolor = hair_info[hair_info == 1]\n",
    "    if len(haircolor) == 0:\n",
    "        return None\n",
    "    return haircolor.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc5510-2fd8-42f9-a58a-c301286b3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_person_attributes(detection, row_index):\n",
    "    detection[\"hairtype\"] = get_hairtype(row_index)\n",
    "    detection[\"haircolor\"] = get_haircolor(row_index)\n",
    "    add_boolean_person_attributes(detection, row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091b7eb-d570-4b21-8290-17fee15d226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perceived_gender_presentation(row_index):\n",
    "    gender_info = gt_df.loc[row_index, gt_df.columns.str.startswith('gender')]\n",
    "    pgp = gender_info[gender_info == 1]\n",
    "    if len(pgp) == 0:\n",
    "        return None\n",
    "    return pgp.index[0].replace(\"gender_presentation_\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "def get_perceived_age_presentation(row_index):\n",
    "    age_info = gt_df.loc[row_index, gt_df.columns.str.startswith('age')]\n",
    "    pap = age_info[age_info == 1]\n",
    "    if len(pap) == 0:\n",
    "        return None\n",
    "    return pap.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49856f0-9c40-44f1-b8dc-afc8a0c80786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skintone(row_index):\n",
    "    skin_info = gt_df.loc[row_index, gt_df.columns.str.startswith('skin_tone')]\n",
    "    return skin_info.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a702e8b-93a2-4e8b-879c-f82be93830ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_protected_attributes(detection, row_index):\n",
    "    detection[\"perceived_age_presentation\"] = get_perceived_age_presentation(row_index)\n",
    "    detection[\"perceived_gender_presentation\"] = get_perceived_gender_presentation(row_index)\n",
    "    detection[\"skin_tone\"] = get_skintone(row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f3a95d-e491-4de4-be5a-d53de547f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISIBILITY_ATTRS = (\"visible_torso\", \"visible_face\", \"visible_minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9ed1ed-c71d-4583-963f-bfa23563309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lighting(row_index):\n",
    "    lighting_info = gt_df.loc[row_index, gt_df.columns.str.startswith('lighting')]\n",
    "    lighting = lighting_info[lighting_info == 1]\n",
    "    if len(lighting) == 0:\n",
    "        return None\n",
    "    lighting = lighting.index[0].replace(\"lighting_\", \"\").replace(\"_\", \" \")\n",
    "    return lighting\n",
    "\n",
    "def add_other_attributes(detection, row_index):\n",
    "    detection[\"lighting\"] = get_lighting(row_index)\n",
    "    for attr in VISIBILITY_ATTRS:\n",
    "        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de3c768-bfa3-4727-95ed-59346a5788a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detection(row_index, sample):\n",
    "    bbox_dict = json.loads(gt_df.loc[row_index, \"bounding_box\"])\n",
    "    x, y, w, h = bbox_dict[\"x\"], bbox_dict[\"y\"], bbox_dict[\"width\"], bbox_dict[\"height\"]\n",
    "    cat1, cat2 = bbox_dict[\"dict_attributes\"][\"cat1\"], bbox_dict[\"dict_attributes\"][\"cat2\"]\n",
    "\n",
    "    person_id = gt_df.loc[row_index, \"person_id\"]\n",
    "\n",
    "    img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "\n",
    "    bounding_box = [x/img_width, y/img_height, w/img_width, h/img_height]\n",
    "    detection = fo.Detection(\n",
    "        label=cat1, \n",
    "        bounding_box=bounding_box,\n",
    "        person_id=person_id,\n",
    "        )\n",
    "    if cat2 != 'none':\n",
    "        detection[\"class2\"] = cat2\n",
    "\n",
    "    add_person_attributes(detection, row_index)\n",
    "    add_protected_attributes(detection, row_index)\n",
    "    add_other_attributes(detection, row_index)\n",
    "\n",
    "    return detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1869986-c9eb-454e-b164-cd5f8b598286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ground_truth_labels(dataset):\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        sample_annos = gt_df[gt_df['filename'] == sample.filename]\n",
    "        detections = []\n",
    "        for row in sample_annos.iterrows():\n",
    "            row_index = row[0]\n",
    "            detection = create_detection(row_index, sample)\n",
    "            detections.append(detection)\n",
    "        sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "    dataset.add_dynamic_sample_fields()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec896ec-9aa5-4033-839d-ced0b1ae54d6",
   "metadata": {},
   "source": [
    "# Add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068d1d0-38e0-43b7-b827-5006503182d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## add all of the ground truth labels\n",
    "add_ground_truth_labels(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e53d3-c90d-467e-83c2-6b1bc5717fdd",
   "metadata": {},
   "source": [
    "# Add Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b485560-887e-47f9-b980-a9460a481942",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coco_masks_to_dataset(dataset):\n",
    "    ########## LM ##########\n",
    "    # with s3.open(f's3://{bucket_name}/{annotations_prefix}coco_masks.json', 'rb') as file:\n",
    "    #     coco_masks = json.load(file)\n",
    "\n",
    "    ########## KH ##########\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket_name = 'w210facetdata'\n",
    "    object_key = 'annotations/coco_masks.json'\n",
    "    s3_object = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "    s3_file_content = s3_object['Body'].read().decode('utf-8')\n",
    "    coco_masks = json.loads(s3_file_content)\n",
    "\n",
    "    \n",
    "    cmas = coco_masks[\"annotations\"]\n",
    "\n",
    "    FILENAME_TO_ID = {\n",
    "        img[\"file_name\"]: img[\"id\"]\n",
    "        for img in coco_masks[\"images\"]\n",
    "    }\n",
    "\n",
    "    CAT_TO_LABEL = {cat[\"id\"]: cat[\"name\"] for cat in coco_masks[\"categories\"]}\n",
    "\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        fn = sample.filename\n",
    "\n",
    "        if fn not in FILENAME_TO_ID:\n",
    "            continue\n",
    "\n",
    "        img_id = FILENAME_TO_ID[fn]\n",
    "        img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "        sample_annos = [a for a in cmas if a[\"image_id\"] == img_id]\n",
    "        if len(sample_annos) == 0:\n",
    "            continue\n",
    "\n",
    "        coco_detections = []\n",
    "        for ann in sample_annos:\n",
    "            label = CAT_TO_LABEL[ann[\"category_id\"]]\n",
    "            bbox = ann['bbox']\n",
    "            ann_id = ann['ann_id']\n",
    "            person_id = ann['facet_person_id']\n",
    "\n",
    "            mask = maskUtils.decode(ann[\"segmentation\"])\n",
    "            mask = Image.fromarray(255*mask)\n",
    "\n",
    "            ## Change bbox to be in the format [x, y, x, y]\n",
    "            bbox[2] = bbox[0] + bbox[2]\n",
    "            bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "            ## Get the cropped image\n",
    "            cropped_mask = np.array(mask.crop(bbox)).astype(bool)\n",
    "\n",
    "            ## Convert to relative [x, y, w, h] coordinates\n",
    "            bbox[2] = bbox[2] - bbox[0]\n",
    "            bbox[3] = bbox[3] - bbox[1]\n",
    "\n",
    "            bbox[0] = bbox[0]/img_width\n",
    "            bbox[1] = bbox[1]/img_height\n",
    "            bbox[2] = bbox[2]/img_width\n",
    "            bbox[3] = bbox[3]/img_height\n",
    "\n",
    "            new_detection = fo.Detection(\n",
    "                label=label, \n",
    "                bounding_box=bbox,\n",
    "                person_id=person_id,\n",
    "                ann_id=ann_id,\n",
    "                mask=cropped_mask,\n",
    "                )\n",
    "            coco_detections.append(new_detection)\n",
    "        sample[\"coco_masks\"] = fo.Detections(detections=coco_detections)\n",
    "\n",
    "## add the masks\n",
    "add_coco_masks_to_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a7cb6d-c307-434a-9500-15ae6a4ccadb",
   "metadata": {},
   "source": [
    "# Import Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed90e2-cd2c-4b69-a7a3-d63e9effea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5 = foz.load_zoo_model('yolov5m-coco-torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be206d9-d465-4094-8aff-7b8ede9ccf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(yolov5, label_field=\"yolov5m\")\n",
    "### Just retain the \"person\" detections\n",
    "people_view_values = dataset.filter_labels(\"yolov5m\", F(\"label\") == \"person\").values(\"yolov5m\")\n",
    "dataset.set_values(\"yolov5m\", people_view_values)\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22f25c-8c22-4c4a-ae09-05e07ec88e17",
   "metadata": {},
   "source": [
    "# Clip classification model --> Replace with teacher/student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880f2c8-3d3a-4744-89cb-b69ca608520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a list of all 52 classes\n",
    "facet_classes = dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "## instantiate a CLIP model with these classes\n",
    "clip = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    text_prompt=\"A photo of a\",\n",
    "    classes=facet_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a70100-8fa6-4b34-a754-f86b4b3a52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_view = dataset.to_patches(\"ground_truth\")\n",
    "patch_view.apply_model(clip, label_field=\"clip\")\n",
    "dataset.save_view(\"patch_view\", patch_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339f2b7d-5058-4278-81be-32515887d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IOU_THRESHS = np.round(np.arange(0.5, 1.0, 0.05), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14507c5d-78b9-4895-9232-a277c0b61aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_detection_model(dataset, label_field):\n",
    "    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n",
    "    dataset.evaluate_detections(label_field, \"ground_truth\", eval_key=eval_key, classwise=False)\n",
    "    \n",
    "    for sample in dataset.iter_samples(progress=True):\n",
    "        for pred in sample[label_field].detections:\n",
    "            iou_field = f\"{eval_key}_iou\"\n",
    "            if iou_field not in pred:\n",
    "                continue\n",
    "\n",
    "            iou = pred[iou_field]\n",
    "            for it in IOU_THRESHS:\n",
    "                pred[f\"{iou_field}_{str(it).replace('.', '')}\"] = iou >= it\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4e194-37cc-4de9-a76b-51ed0de18ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_evaluate_detection_model(dataset, 'yolov5m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e682ce1b-787e-47db-95b8-fb17ed5d9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_detection_mAR(sample_collection, label_field):\n",
    "    \"\"\"Computes the mean average recall of the specified detection field.\n",
    "    -- computed as the average over iou thresholds of the recall at\n",
    "    each threshold.\n",
    "    \"\"\"\n",
    "    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n",
    "    iou_recalls = []\n",
    "    for it in IOU_THRESHS:\n",
    "        field_str = f\"{label_field}.detections.{eval_key}_iou_{str(it).replace('.', '')}\"\n",
    "        counts = sample_collection.count_values(field_str)\n",
    "        tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "        recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "        iou_recalls.append(recall)\n",
    "\n",
    "    return np.mean(iou_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb73ee8-28ac-4416-8d7a-791959206e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_detection_mAR(dataset, label_field, concept, attributes):\n",
    "    sub_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_view = sub_view.filter_labels(\"ground_truth\", F(f\"skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_view = sub_view.filter_labels(\"ground_truth\", F(attribute[0]) == attribute[1])\n",
    "    return _compute_detection_mAR(sub_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651365b-d969-456b-aa02-78a47f6d4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = 'lawman'\n",
    "attributes = {\"hairtype\": \"straight\", \"haircolor\": \"brown\"}\n",
    "get_concept_attr_detection_mAR(dataset, \"yolov5m\", concept, attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320aa339-ddd0-4e4d-800e-aa6ac8d31b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_classification_model(dataset, prediction_field):\n",
    "    patch_view = dataset.load_saved_view(\"patch_view\")\n",
    "    eval_key = \"eval_\" + prediction_field\n",
    "    \n",
    "    for sample in patch_view.iter_samples(progress=True):\n",
    "        sample[eval_key] = (\n",
    "            sample.ground_truth.label == sample[prediction_field].label\n",
    "        )\n",
    "        sample.save()\n",
    "    dataset.save_view(\"patch_view\", patch_view, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffe5b3-dfa1-42ad-bb88-52b04622bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_evaluate_classification_model(dataset, 'clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830a851-7ff9-4ee1-9082-3810dc511cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_recall(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field.split(\"_\")[0]\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae5f107-e21f-46de-a8d3-3a329aa88916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_classification_recall(dataset, label_field, concept, attributes):\n",
    "    patch_view = dataset.load_saved_view(\"patch_view\")\n",
    "    sub_patch_view = patch_view.match(F(\"ground_truth.label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_patch_view = sub_patch_view.match(F(f\"ground_truth.skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_patch_view = sub_patch_view.match(F(f\"ground_truth.{attribute[0]}\") == attribute[1])\n",
    "    return _compute_classification_recall(sub_patch_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf03f9-f46d-4ffd-bb0f-e868ad9116ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = {'hairtype': 'curly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60239f5d-239e-4b3d-af81-a81936cab0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_concept_attr_classification_recall(dataset, \"clip\", concept, attribute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c238e25-c163-4774-82dd-f9a96bb3e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_recall(dataset, label_field, concept, attribute):\n",
    "    if label_field in dataset.get_field_schema().keys():\n",
    "        return get_concept_attr_detection_mAR(dataset, label_field, concept, attribute)\n",
    "    else:\n",
    "        return get_concept_attr_classification_recall(dataset, label_field, concept, attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e153b95-8f68-4a3c-a562-f1eee4136004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_disparity(dataset, label_field, concept, attribute1, attribute2):\n",
    "    recall1 = get_concept_attr_recall(dataset, label_field, concept, attribute1)\n",
    "    recall2 = get_concept_attr_recall(dataset, label_field, concept, attribute2)\n",
    "    return recall1 - recall2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55758f91-25ff-4b29-80b2-8e82878221af",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs1 = {\"perceived_gender_presentation\": \"fem\"}\n",
    "attrs2 = {\"hairtype\": \"straight\"}\n",
    "for concept in [\"astronaut\", \"singer\", \"judge\", \"student\"]:\n",
    "    disparity = compute_disparity(dataset, \"clip\", concept, attrs1, attrs2)     \n",
    "    print(f\"{concept}: {disparity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9936a1d6-090a-4310-9001-de6b5b39c2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_concept_attr_classification_recall(dataset, 'clip', 'singer', attrs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a7918-51d0-41f4-a53f-a2324a250437",
   "metadata": {},
   "source": [
    "# Experimenting with uploading custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a1dc98-17cc-4059-ae0a-df8e32a22ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.torch as fout\n",
    "from torchvision.models import resnet50\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import fiftyone.core.expressions as foe\n",
    "from fiftyone import ViewField as VF\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb3f810-0334-498d-8aea-01051532de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the student model architecture and weights\n",
    "student_model = torch.load('student_model.pth')\n",
    "student_model.load_state_dict(torch.load('student_model_weights.pth'))\n",
    "\n",
    "\n",
    "# Set the number of new classes\n",
    "num_new_classes = 52\n",
    "\n",
    "# Modify the student model for the FACET dataset\n",
    "class AdaptedStudentModel(nn.Module):\n",
    "    def __init__(self, student_model, num_classes):\n",
    "        super(AdaptedStudentModel, self).__init__()\n",
    "        self.features = nn.Sequential(*list(student_model.children())[:-1])  # Preserve the features part of the student model\n",
    "        self.fc1 = nn.Linear(16 * 15 * 15, num_classes)  # Adjust the linear layer for the new number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, 16 * 15 * 15)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "adapted_student_model = AdaptedStudentModel(student_model, num_classes=N)\n",
    "model = adapted_student_model\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06058c4-f25f-4905-8c3f-e82e714e67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_loader(image_paths, sample_ids, batch_size):\n",
    "    mean = [0.4914, 0.4822, 0.4465]\n",
    "    std = [0.2023, 0.1994, 0.2010]\n",
    "    transforms = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize((256, 256)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = fout.TorchImageDataset(\n",
    "        image_paths, sample_ids=sample_ids, transform=transforms\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "student = model.to(device)\n",
    "student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e09e7-065a-4a02-9d35-b2ea2eba1dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved architecture\n",
    "# with open('student_architecture.json', 'r') as f:\n",
    "#     student_architecture = json.load(f)\n",
    "\n",
    "# # Define the Student model based on the loaded architecture\n",
    "# class Student(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(Student, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(*student_architecture['conv1'])\n",
    "#         self.pool = nn.MaxPool2d(*student_architecture['pool'])\n",
    "#         self.fc1 = nn.Linear(*student_architecture['fc1'])\n",
    "#         self.fc2 = nn.Linear(student_architecture['fc2'][0], num_classes)  # new number of classes\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = x.view(-1, self.fc1.in_features)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Create a Student model instance with the new number of classes\n",
    "# num_new_classes = 1000  # replace with the actual number of new classes\n",
    "# student = Student(num_new_classes)\n",
    "\n",
    "# # Load the model weights\n",
    "# state_dict = torch.load('student_weights.pth')\n",
    "\n",
    "# # Remove the weights of the fc2 layer from the saved state_dict as we are going to initialize it randomly\n",
    "# state_dict = {k: v for k, v in state_dict.items() if \"fc2\" not in k}\n",
    "\n",
    "# # Load the state_dict into the student model\n",
    "# student.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# # Move the model to the appropriate device (e.g., GPU if available)\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# student = student.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454b58a-fc55-40c1-a873-1464f6d3c57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the Student model\n",
    "# class Student(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Student, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "#         self.pool = nn.MaxPool2d(2, 2)\n",
    "#         self.fc1 = nn.Linear(16*14*14, 120)\n",
    "#         self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pool(F.relu(self.conv1(x)))\n",
    "#         x = x.view(-1, 16*14*14)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "\n",
    "# # Create a new instance of the Student model and load the pretrained weights\n",
    "# model = Student()\n",
    "# model.load_state_dict(torch.load('student.pth'))\n",
    "# model.eval()\n",
    "\n",
    "# # Set the number of new classes\n",
    "# num_new_classes = 52\n",
    "\n",
    "# # Modify the final fully connected (fc) layer\n",
    "# # Get the number of input features to the fc2 layer\n",
    "# in_features = model.fc2.in_features\n",
    "\n",
    "# # Replace the fc2 layer with a new one for the desired number of classes\n",
    "# model.fc2 = nn.Linear(in_features, num_new_classes)\n",
    "\n",
    "# def make_data_loader(image_paths, sample_ids, batch_size):\n",
    "#     mean = [0.4914, 0.4822, 0.4465]\n",
    "#     std = [0.2023, 0.1994, 0.2010]\n",
    "#     transforms = torchvision.transforms.Compose(\n",
    "#         [\n",
    "#             torchvision.transforms.Resize((256, 256)),\n",
    "#             torchvision.transforms.ToTensor(),\n",
    "#             torchvision.transforms.Normalize(mean, std),\n",
    "#         ]\n",
    "#     )\n",
    "#     dataset = fout.TorchImageDataset(\n",
    "#         image_paths, sample_ids=sample_ids, transform=transforms\n",
    "#     )\n",
    "#     return DataLoader(dataset, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebd47cf-baf9-45d5-8f69-a4fd3f0fbb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = resnet50(pretrained=True)\n",
    "# num_new_classes = 52\n",
    "\n",
    "# # Modify the final fully connected (fc) layer\n",
    "# # Get the number of input features to the fc layer\n",
    "# in_features = model.fc.in_features\n",
    "\n",
    "# # Replace the fc layer with a new one for the desired number of classes\n",
    "# model.fc = nn.Linear(in_features, num_new_classes)\n",
    "\n",
    "# def make_data_loader(image_paths, sample_ids, batch_size):\n",
    "#     mean = [0.4914, 0.4822, 0.4465]\n",
    "#     std = [0.2023, 0.1994, 0.2010]\n",
    "#     transforms = torchvision.transforms.Compose(\n",
    "#         [\n",
    "#             torchvision.transforms.Resize((256, 256)),\n",
    "#             torchvision.transforms.ToTensor(),\n",
    "#             torchvision.transforms.Normalize(mean, std),\n",
    "#         ]\n",
    "#     )\n",
    "#     dataset = fout.TorchImageDataset(\n",
    "#         image_paths, sample_ids=sample_ids, transform=transforms\n",
    "#     )\n",
    "#     return DataLoader(dataset, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff460c1-0bdf-46f4-949d-a620be5f2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, imgs):\n",
    "    logits = model(imgs).detach().cpu().numpy()\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    # predictions = np.argmax(logits, axis=1) - 1\n",
    "    # print(f'Max prediction: {np.max(predictions)}, Min prediction: {np.min(predictions)}')\n",
    "    odds = np.exp(logits)\n",
    "    confidences = np.max(odds, axis=1) / np.sum(odds, axis=1)\n",
    "    return predictions, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d813418-51c8-442b-b2da-5e58d5d32886",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "batch_size = 5\n",
    "\n",
    "view = dataset.take(num_samples, seed=51)\n",
    "classes = []\n",
    "for i in view.iter_samples():\n",
    "    classes.append(i.ground_truth.detections[0].label)\n",
    "\n",
    "image_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view])\n",
    "data_loader = make_data_loader(image_paths, sample_ids, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b6016d-aa18-4fc5-832b-f71909a6184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_samples = 1000\n",
    "# batch_size = 5\n",
    "\n",
    "# view = dataset.take(num_samples, seed=51)\n",
    "# classes = set()  # Use a set to collect unique classes\n",
    "# for i in view.iter_samples():\n",
    "#     if i.ground_truth.detections:  # Check if detections is not empty\n",
    "#         classes.add(i.ground_truth.detections[0].label)\n",
    "#     else:\n",
    "#         print(f\"Sample {i.id} has no detections\")\n",
    "\n",
    "# image_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view])\n",
    "# data_loader = make_data_loader(image_paths, sample_ids, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af312860-74ff-4f0d-abb1-d05ffa7dbb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert classes set to a list and create a mapping from class labels to indices\n",
    "# classes = list(classes)\n",
    "# class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "\n",
    "# image_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view])\n",
    "# data_loader = make_data_loader(image_paths, sample_ids, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb3bdd-31ef-4a66-afee-f6ed696f08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Perform prediction and store results in dataset\n",
    "#\n",
    "\n",
    "for imgs, sample_ids in data_loader:\n",
    "    imgs = imgs.to(device)\n",
    "    predictions, confidences = predict(model, imgs)\n",
    "\n",
    "    # Add predictions to your FiftyOne dataset\n",
    "    for sample_id, prediction, confidence in zip(\n",
    "        sample_ids, predictions, confidences\n",
    "    ):\n",
    "        sample = dataset[sample_id]\n",
    "        sample[\"pred\"] = fo.Classification(\n",
    "            label=classes[prediction],  # Use the mapping to get class labels\n",
    "            confidence=confidence,\n",
    "        )\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c1cc6-e500-4a32-9b5b-6425e21864db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_classification_modelr(dataset, prediction_field):\n",
    "    eval_key = \"eval_\" + prediction_field\n",
    "    \n",
    "    for sample in dataset.iter_samples(progress=True):\n",
    "        sample[eval_key] = (\n",
    "            sample.ground_truth.detections[0].label == sample[prediction_field].label\n",
    "        )\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5764e5-4859-4b2b-8e04-68224437e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _evaluate_classification_modelr(dataset, prediction_field):\n",
    "#     eval_key = \"eval_\" + prediction_field\n",
    "    \n",
    "#     for sample in dataset.iter_samples(progress=True):\n",
    "#         # Check if ground_truth.detections is not empty\n",
    "#         if sample.ground_truth.detections:\n",
    "#             sample[eval_key] = (\n",
    "#                 sample.ground_truth.detections[0].label == sample[prediction_field].label\n",
    "#             )\n",
    "#             sample.save()\n",
    "#         else:\n",
    "#             print(f\"Sample {sample.id} has no detections\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1067a92a-6d53-4d62-b512-cb43083d3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "_evaluate_classification_modelr(view, 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f87bbe-6a3c-40da-86b3-0baee1ba3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_recall(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e607b-164f-4326-9971-f09bbd4ce18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_classification_recall(dataset, label_field, concept, attributes):\n",
    "    sub_patch_view = dataset.filter_labels(\"ground_truth\", VF(\"label\") == concept)  # Use foe instead of F\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_patch_view = sub_patch_view.filter_labels('ground_truth', VF(f\"skin_tone.{attribute[0]}\") != 0)  # Use foe instead of F\n",
    "        else:\n",
    "            sub_patch_view = sub_patch_view.filter_labels('ground_truth', VF(f\"{attribute[0]}\") == attribute[1])  # Use foe instead of F\n",
    "    return _compute_classification_recall(sub_patch_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2980f61-f140-4a86-9e41-fee7161fb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = 'lawman'\n",
    "attributes = {\"perceived_gender_presentation\": \"masc\"}\n",
    "\n",
    "get_concept_attr_classification_recall(view, 'pred', concept, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd16a3a-4212-486d-9107-5011a92feae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3fc57c-4357-407f-b77e-6bedeea66241",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2d41d-ae7a-4c9a-bd2e-9c4bd988babd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faead1e-1785-4b48-82e2-f9201edc0657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b653ec-6918-4079-acc1-e4b20cf77825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
