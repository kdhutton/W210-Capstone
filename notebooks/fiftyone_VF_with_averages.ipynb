{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0a8b3b-5189-4857-887c-e93b637bf000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "boto3 1.28.68 requires botocore<1.32.0,>=1.31.68, but you have botocore 1.31.64 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.7.0 requires botocore<1.31.65,>=1.31.16, but you have botocore 1.31.74 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install for AWS\n",
    "!pip install torch --quiet\n",
    "!pip install matplotlib --quiet\n",
    "!pip install torchvision --quiet\n",
    "!pip install s3fs --quiet\n",
    "!pip install boto3 --quiet\n",
    "!pip install fiftyone --quiet\n",
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c928e2-c18b-457c-8640-f03b4b11b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import numpy as np\n",
    "import boto3\n",
    "import s3fs\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import fiftyone.utils.torch as fout\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "import fiftyone.core.expressions as foe\n",
    "\n",
    "from models_package import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f329f00-bc92-4a2c-9f76-9d7a8f3aff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########## LM ##########\n",
    "\n",
    "# access_key = getpass.getpass(\"Enter your access: \")\n",
    "\n",
    "# secret_key = password = getpass.getpass(\"Enter your secret: \")\n",
    "\n",
    "# bucket_name = 'w210facetdata'\n",
    "# annotations_prefix = 'annotations/'\n",
    "# images_prefix = '/home/ubuntu/W210-Capstone'\n",
    "\n",
    "# s3 = s3fs.S3FileSystem(key=access_key, secret=secret_key)\n",
    "\n",
    "# # Use s3.open to open the CSV file and read its content into a Pandas DataFrame\n",
    "# with s3.open(f's3://{bucket_name}/{annotations_prefix}annotations.csv', 'rb') as file:\n",
    "#     gt_df = pd.read_csv(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f7c272-668d-4671-84b1-16e16b01439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########## LM ##########\n",
    "\n",
    "# # use relative paths to your image dirs\n",
    "# dataset = fo.Dataset(name = \"IDP\", persistent=True)\n",
    "# # dataset = fo.load_dataset('IDP')\n",
    "# dataset.add_images_dir(images_prefix)\n",
    "# dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7a9c86-d180-4065-b902-93d75cf2515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## KH ##########\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3', region_name='us-west-2')\n",
    "\n",
    "# Define the S3 bucket name and prefixes\n",
    "bucket_name = 'w210facetdata'\n",
    "annotations_prefix = 'annotations/'\n",
    "images_prefix = 'images/'\n",
    "\n",
    "# Load CSV annotations from S3\n",
    "annotations_s3_path = f's3://{bucket_name}/{annotations_prefix}'\n",
    "gt_df = pd.read_csv(f'{annotations_s3_path}annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04df1e08-51f4-42ee-9459-9f3fa86b1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########## KH ##########\n",
    "\n",
    "# UNCOMMENT THIS BLOCK TO PULL FACET DATA FROM S3\n",
    "\n",
    "# local_images_dir = 'local_images_dir'\n",
    "# os.makedirs(local_images_dir, exist_ok=True)\n",
    "\n",
    "# # Create a paginator to handle pagination of the results\n",
    "# paginator = s3_client.get_paginator('list_objects_v2')\n",
    "\n",
    "# # Use the paginator to retrieve all objects\n",
    "# for page in paginator.paginate(Bucket=bucket_name, Prefix=images_prefix):\n",
    "#     for obj in page.get('Contents', []):\n",
    "#         # Skip the prefix itself\n",
    "#         if obj['Key'] == images_prefix:\n",
    "#             continue\n",
    "#         local_file_path = os.path.join(local_images_dir, os.path.basename(obj['Key']))\n",
    "#         s3_client.download_file(bucket_name, obj['Key'], local_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0cfd8a-0b73-4f7b-8b4a-33af7033b4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 31703/31703 [5.0s elapsed, 0s remaining, 6.0K samples/s]      \n",
      "Computing metadata...\n",
      " 100% |█████████████| 31703/31703 [1.0m elapsed, 0s remaining, 589.4 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "########## KH ##########\n",
    "local_images_dir = 'local_images_dir'\n",
    "fo.delete_dataset('local_images_dir')\n",
    "dataset = fo.Dataset(name='local_images_dir', persistent=True)\n",
    "dataset.add_images_dir(local_images_dir)\n",
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc58bbf-ed5e-4f3a-b86a-77e58abb1ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31702\n"
     ]
    }
   ],
   "source": [
    "# Count the number of files in the local_images_dir\n",
    "num_files = len([f for f in os.listdir(local_images_dir) if os.path.isfile(os.path.join(local_images_dir, f))])\n",
    "print(num_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c721ef-146f-4f48-a411-7fb93bca9d84",
   "metadata": {},
   "source": [
    "# Object Detection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be99a8b0-a415-4560-a578-2b9f8bba27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOLEAN_PERSONAL_ATTRS = (\n",
    "    \"has_facial_hair\",\n",
    "    \"has_tattoo\",\n",
    "    \"has_cap\",\n",
    "    \"has_mask\",\n",
    "    \"has_headscarf\",\n",
    "    \"has_eyeware\",\n",
    ")\n",
    "def add_boolean_person_attributes(detection, row_index):\n",
    "    for attr in BOOLEAN_PERSONAL_ATTRS:\n",
    "        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "892ad9c7-cb94-49ee-bbe6-caf72e3e7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hairtype(row_index):\n",
    "    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hairtype')]\n",
    "    hairtype = hair_info[hair_info == 1]\n",
    "    if len(hairtype) == 0:\n",
    "        return None\n",
    "    return hairtype.index[0].split('_')[1]\n",
    "\n",
    "def get_haircolor(row_index):\n",
    "    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hair_color')]\n",
    "    haircolor = hair_info[hair_info == 1]\n",
    "    if len(haircolor) == 0:\n",
    "        return None\n",
    "    return haircolor.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8fc5510-2fd8-42f9-a58a-c301286b3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_person_attributes(detection, row_index):\n",
    "    detection[\"hairtype\"] = get_hairtype(row_index)\n",
    "    detection[\"haircolor\"] = get_haircolor(row_index)\n",
    "    add_boolean_person_attributes(detection, row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0091b7eb-d570-4b21-8290-17fee15d226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perceived_gender_presentation(row_index):\n",
    "    gender_info = gt_df.loc[row_index, gt_df.columns.str.startswith('gender')]\n",
    "    pgp = gender_info[gender_info == 1]\n",
    "    if len(pgp) == 0:\n",
    "        return None\n",
    "    return pgp.index[0].replace(\"gender_presentation_\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "def get_perceived_age_presentation(row_index):\n",
    "    age_info = gt_df.loc[row_index, gt_df.columns.str.startswith('age')]\n",
    "    pap = age_info[age_info == 1]\n",
    "    if len(pap) == 0:\n",
    "        return None\n",
    "    return pap.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c49856f0-9c40-44f1-b8dc-afc8a0c80786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skintone(row_index):\n",
    "    skin_info = gt_df.loc[row_index, gt_df.columns.str.startswith('skin_tone')]\n",
    "    return skin_info.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a702e8b-93a2-4e8b-879c-f82be93830ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_protected_attributes(detection, row_index):\n",
    "    detection[\"perceived_age_presentation\"] = get_perceived_age_presentation(row_index)\n",
    "    detection[\"perceived_gender_presentation\"] = get_perceived_gender_presentation(row_index)\n",
    "    detection[\"skin_tone\"] = get_skintone(row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01f3a95d-e491-4de4-be5a-d53de547f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISIBILITY_ATTRS = (\"visible_torso\", \"visible_face\", \"visible_minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc9ed1ed-c71d-4583-963f-bfa23563309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lighting(row_index):\n",
    "    lighting_info = gt_df.loc[row_index, gt_df.columns.str.startswith('lighting')]\n",
    "    lighting = lighting_info[lighting_info == 1]\n",
    "    if len(lighting) == 0:\n",
    "        return None\n",
    "    lighting = lighting.index[0].replace(\"lighting_\", \"\").replace(\"_\", \" \")\n",
    "    return lighting\n",
    "\n",
    "def add_other_attributes(detection, row_index):\n",
    "    detection[\"lighting\"] = get_lighting(row_index)\n",
    "    for attr in VISIBILITY_ATTRS:\n",
    "        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3de3c768-bfa3-4727-95ed-59346a5788a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detection(row_index, sample):\n",
    "    bbox_dict = json.loads(gt_df.loc[row_index, \"bounding_box\"])\n",
    "    x, y, w, h = bbox_dict[\"x\"], bbox_dict[\"y\"], bbox_dict[\"width\"], bbox_dict[\"height\"]\n",
    "    cat1, cat2 = bbox_dict[\"dict_attributes\"][\"cat1\"], bbox_dict[\"dict_attributes\"][\"cat2\"]\n",
    "\n",
    "    person_id = gt_df.loc[row_index, \"person_id\"]\n",
    "\n",
    "    img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "\n",
    "    bounding_box = [x/img_width, y/img_height, w/img_width, h/img_height]\n",
    "    detection = fo.Detection(\n",
    "        label=cat1, \n",
    "        bounding_box=bounding_box,\n",
    "        person_id=person_id,\n",
    "        )\n",
    "\n",
    "    detection[\"class2\"] = cat2\n",
    "\n",
    "    add_person_attributes(detection, row_index)\n",
    "    add_protected_attributes(detection, row_index)\n",
    "    add_other_attributes(detection, row_index)\n",
    "\n",
    "    return detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1869986-c9eb-454e-b164-cd5f8b598286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ground_truth_labels(dataset):\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        sample_annos = gt_df[gt_df['filename'] == sample.filename]\n",
    "        detections = []\n",
    "        for row in sample_annos.iterrows():\n",
    "            row_index = row[0]\n",
    "            detection = create_detection(row_index, sample)\n",
    "            detections.append(detection)\n",
    "        sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "    dataset.add_dynamic_sample_fields()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec896ec-9aa5-4033-839d-ced0b1ae54d6",
   "metadata": {},
   "source": [
    "# Add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d068d1d0-38e0-43b7-b827-5006503182d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 31703/31703 [5.7m elapsed, 0s remaining, 91.8 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "## add all of the ground truth labels\n",
    "add_ground_truth_labels(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a7918-51d0-41f4-a53f-a2324a250437",
   "metadata": {},
   "source": [
    "# Student and Teacher Disparity Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e20632e4-2b30-4c89-84a6-b4d0aa7e334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = fo.load_dataset('IDP')\n",
    "# fo.list_datasets()\n",
    "# dataset = fo.load_dataset('local_images_dir')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b1137d9-6b03-469e-bebf-6bd050112555",
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_mapping = {\n",
    "    'bartender': 'Chef',\n",
    "    'doctor': 'Doctor',\n",
    "    'carpenter': 'Engineer',\n",
    "    'computer_user': 'Engineer',\n",
    "    'electrician': 'Engineer',\n",
    "    'farmer': 'Farmer',\n",
    "    'gardener': 'Farmer',\n",
    "    'fireman': 'Firefighter',\n",
    "    'judge': 'Judge',\n",
    "    'laborer': 'Mechanic',\n",
    "    'machinist': 'Mechanic',\n",
    "    'astronaut': 'Pilot',\n",
    "    'lawman': 'Police',\n",
    "    'guard': 'Police',\n",
    "    'waiter': 'Waiter',\n",
    "    'soldier': 'Police'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "845a37c7-1e35-4963-a0fb-988e4558adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_label(detections):\n",
    "    for detection in detections.detections:\n",
    "        detection.label = reversed_mapping.get(detection.label, detection.label)\n",
    "        if detection.class2:\n",
    "            detection.class2 = reversed_mapping.get(detection.class2, detection.class2)\n",
    "    return detections\n",
    "\n",
    "def compute_weighted_avg_skin_tone(detections):\n",
    "    for detection in detections.detections:\n",
    "        skin_tone_dict = detection.skin_tone\n",
    "        total_counts = sum(value for key, value in skin_tone_dict.items() if key != 'skin_tone_na')\n",
    "        if total_counts == 0:\n",
    "            detection['binary_skin_tone'] = 0\n",
    "            continue\n",
    "        weighted_avg_skin_tone = sum(int(key.split('_')[-1]) * value for key, value in skin_tone_dict.items() if key != 'skin_tone_na') / total_counts\n",
    "        detection['light_skin_tone_indicator'] = False if weighted_avg_skin_tone > 5.5 else True\n",
    "\n",
    "# Iterate through the dataset and update the labels\n",
    "for sample in dataset:\n",
    "    sample['ground_truth'] = update_label(sample['ground_truth'])  \n",
    "    compute_weighted_avg_skin_tone(sample['ground_truth'])\n",
    "    sample.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4fb3f810-0334-498d-8aea-01051532de8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the student model architecture and weights\n",
    "student_model = StudentModel(in_features=16, num_classes=10)\n",
    "# student_model = resnet18(weights=None)\n",
    "teacher_model = TeacherModel(in_features=16, num_classes=10)\n",
    "\n",
    "# weights_path = 'student_model_weights_ckd_1.pth'\n",
    "student_weights_path = 'student_model_weights3.pth'\n",
    "teacher_weights_path = 'teacher_model_weights3.pth'\n",
    "\n",
    "student_model.load_state_dict(torch.load(weights_path))\n",
    "teacher_model.load_state_dict(torch.load(weights_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f06058c4-f25f-4905-8c3f-e82e714e67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_loader(image_paths, sample_ids, batch_size):\n",
    "    mean = [0.5071, 0.4867, 0.4408]\n",
    "    std = [0.2675, 0.2565, 0.2761]\n",
    "    transforms = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize((256, 256)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = fout.TorchImageDataset(\n",
    "        image_paths, sample_ids=sample_ids, transform=transforms\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "model = student_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fff460c1-0bdf-46f4-949d-a620be5f2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, imgs):\n",
    "    logits = model(imgs).detach().cpu().numpy()\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    odds = np.exp(logits)\n",
    "    confidences = np.max(odds, axis=1) / np.sum(odds, axis=1)\n",
    "    return predictions, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d813418-51c8-442b-b2da-5e58d5d32886",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "view = dataset.match({\"ground_truth.detections.label\": {\"$in\": ['Police', 'Mechanic', 'Pilot', 'Firefighter', 'Doctor',\n",
    "       'Farmer', 'Engineer', 'Waiter', 'Judge', 'Chef']}})\n",
    "\n",
    "image_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view])\n",
    "data_loader = make_data_loader(image_paths, sample_ids, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8de2d72f-361d-44cd-9505-1e4a10057c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0\n",
    "# for i in view.iter_samples():\n",
    "#     print(i.ground_truth.detections)\n",
    "#     counter += 1\n",
    "#     if counter >= 10:\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "563f5bdc-622a-4109-8cbd-1d54d09f3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    \"9\" : \"Waiter\",\n",
    "    \"6\" : \"Mechanic\",\n",
    "    \"2\" : \"Engineer\",\n",
    "    \"4\" : \"Firefighter\",\n",
    "    \"0\" : \"Chef\",\n",
    "    \"5\" : \"Judge\",\n",
    "    \"7\" : \"Pilot\",\n",
    "    \"8\" : \"Police\",\n",
    "    \"3\" : \"Farmer\",\n",
    "    \"1\" : \"Doctor\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ecb3bdd-31ef-4a66-afee-f6ed696f08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform prediction and store results in dataset\n",
    "\n",
    "for imgs, sample_ids in data_loader:\n",
    "    imgs = imgs.to(device)\n",
    "    predictions, confidences = predict(model, imgs)\n",
    "\n",
    "    # Add predictions to your FiftyOne dataset\n",
    "    for sample_id, prediction, confidence in zip(\n",
    "        sample_ids, predictions, confidences\n",
    "    ):\n",
    "        sample = view[sample_id]\n",
    "        sample[\"pred\"] = fo.Classification(\n",
    "            label=classes[str(prediction)],  # Use the mapping to get class labels\n",
    "            confidence=confidence,\n",
    "        )\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "364c1cc6-e500-4a32-9b5b-6425e21864db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_classification_modelr(dataset, prediction_field):\n",
    "    eval_key = \"eval_\" + prediction_field\n",
    "    \n",
    "    for sample in dataset.iter_samples(progress=True):\n",
    "        labels = []\n",
    "\n",
    "        # Iterate through the detections and extract the \"label\" field\n",
    "        for detection in sample.ground_truth.detections:\n",
    "            label = detection.label\n",
    "            label2 = detection.class2\n",
    "            if label is not None:  # Check if \"label\" exists in the dictionary\n",
    "                labels.append(label)\n",
    "            if label2 is not None: \n",
    "                labels.append(label2)\n",
    "                \n",
    "        if sample[prediction_field].label in labels:\n",
    "            sample[eval_key] = True\n",
    "        else:\n",
    "            sample[eval_key] = False\n",
    "        sample.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1067a92a-6d53-4d62-b512-cb43083d3777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 10510/10510 [25.6s elapsed, 0s remaining, 405.0 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "_evaluate_classification_modelr(view, 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f7f87bbe-6a3c-40da-86b3-0baee1ba3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_recall(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp > 0 else 1e-6\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "407b491e-e3d7-4d97-9161-554390b1af7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_results(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp > 0 else 1e-6\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "204e607b-164f-4326-9971-f09bbd4ce18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_classification_recall(dataset, label_field, concept, attributes):\n",
    "    sub_patch_view_primary = dataset.filter_labels(\"ground_truth\", F(\"label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_patch_view_primary = sub_patch_view_primary.filter_labels('ground_truth', F(f\"skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_patch_view_primary = sub_patch_view_primary.filter_labels('ground_truth', F(f\"{attribute[0]}\") == attribute[1])\n",
    "    primary = _compute_classification_recall(sub_patch_view_primary, label_field)\n",
    "    return primary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bff09138-a09c-4f10-9e3f-2ebf3de961f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_disparity(dataset, label_field, concept, attribute1, attribute2):\n",
    "    recall1 = get_concept_attr_classification_recall(dataset, label_field, concept, attribute1)\n",
    "    recall2 = get_concept_attr_classification_recall(dataset, label_field, concept, attribute2)\n",
    "    return (recall1 - recall2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8fcac311-f6a8-49f4-bca7-9edf63851a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "preds = []\n",
    "for i in view.iter_samples(): \n",
    "    preds.append(i.pred.label)\n",
    "\n",
    "# Get counts for each unique label\n",
    "label_counts = Counter(preds)\n",
    "\n",
    "# Create a bar chart\n",
    "plt.bar(label_counts.keys(), label_counts.values(), edgecolor='k')\n",
    "\n",
    "# Add labels and a title\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "# Rotate x-axis labels to vertical\n",
    "plt.xticks(rotation='vertical')\n",
    "\n",
    "# Show the bar chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3994f554-273f-4e4f-a1c7-5e3c2c85fcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Police:0.21349963316214232\n",
      "Mechanic:0.2577903682719547\n",
      "Pilot:0.0196078431372549\n",
      "Firefighter:0.16733067729083664\n",
      "Doctor:0.1712\n",
      "Farmer:0.8025435073627845\n",
      "Engineer:0.17090216010165185\n",
      "Waiter:0.08888888888888889\n",
      "Judge:0.125\n",
      "Chef:0.09090909090909091\n"
     ]
    }
   ],
   "source": [
    "concepts = ['Police', 'Mechanic', 'Pilot', 'Firefighter', 'Doctor',\n",
    "       'Farmer', 'Engineer', 'Waiter', 'Judge', 'Chef']\n",
    "\n",
    "for i in concepts: \n",
    "    print(f'{i}:{_compute_classification_recall(view.filter_labels(\"ground_truth\", F(\"label\") == i), \"pred\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "41f5ef13-5599-41df-b219-e105d95c6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired attribute list for disparity calcs\n",
    "attributes = {\n",
    "    'hairtype': ['straight', 'curly', 'bald', 'wavy', 'dreadlocks', 'coily'],\n",
    "    'haircolor': ['black', 'blonde', 'red', 'colored', 'brown', 'grey'],\n",
    "    'perceived_age_presentation': ['young', 'middle', 'older'],\n",
    "    'perceived_gender_presentation': ['fem', 'masc', 'non binary'],\n",
    "    'has_facial_hair': [False, True],\n",
    "    'has_tattoo': [False, True],\n",
    "    'has_cap': [False, True],\n",
    "    'has_mask': [False, True],\n",
    "    'has_headscarf': [False, True],\n",
    "    'has_eyeware': [False, True],\n",
    "    'light_skin_tone_indicator': [False, True], # 0 is light, 1 is dark\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d266e00e-e215-4398-b071-4fba8ef96b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average disparity for hairtype in concept Police: 0.0943\n",
      "Average disparity for haircolor in concept Police: 0.0803\n",
      "Average disparity for perceived_age_presentation in concept Police: 0.0134\n",
      "Average disparity for perceived_gender_presentation in concept Police: 0.0467\n",
      "Average disparity for has_facial_hair in concept Police: 0.0489\n",
      "Average disparity for has_tattoo in concept Police: 0.0864\n",
      "Average disparity for has_cap in concept Police: 0.0126\n",
      "Average disparity for has_mask in concept Police: 0.0133\n",
      "Average disparity for has_headscarf in concept Police: 0.0332\n",
      "Average disparity for has_eyeware in concept Police: 0.0055\n",
      "Average disparity for light_tone_indicator in concept Police: 0.0379\n",
      "Average disparity for hairtype in concept Mechanic: 0.0959\n",
      "Average disparity for haircolor in concept Mechanic: 0.0673\n",
      "Average disparity for perceived_age_presentation in concept Mechanic: 0.0084\n",
      "Average disparity for perceived_gender_presentation in concept Mechanic: 0.1908\n",
      "Average disparity for has_facial_hair in concept Mechanic: 0.0048\n",
      "Average disparity for has_tattoo in concept Mechanic: 0.0502\n",
      "Average disparity for has_cap in concept Mechanic: 0.0662\n",
      "Average disparity for has_mask in concept Mechanic: 0.0304\n",
      "Average disparity for has_headscarf in concept Mechanic: 0.0256\n",
      "Average disparity for has_eyeware in concept Mechanic: 0.0049\n",
      "Average disparity for light_tone_indicator in concept Mechanic: 0.0304\n",
      "Average disparity for hairtype in concept Pilot: 0.0159\n",
      "Average disparity for haircolor in concept Pilot: 0.0111\n",
      "Average disparity for perceived_age_presentation in concept Pilot: 0.0112\n",
      "Average disparity for perceived_gender_presentation in concept Pilot: 0.0108\n",
      "Average disparity for has_facial_hair in concept Pilot: 0.0199\n",
      "Average disparity for has_tattoo in concept Pilot: 0.0196\n",
      "Average disparity for has_cap in concept Pilot: 0.0060\n",
      "Average disparity for has_mask in concept Pilot: 0.0340\n",
      "Average disparity for has_headscarf in concept Pilot: 0.0201\n",
      "Average disparity for has_eyeware in concept Pilot: 0.0197\n",
      "Average disparity for light_tone_indicator in concept Pilot: 0.0202\n",
      "Average disparity for hairtype in concept Firefighter: 0.2160\n",
      "Average disparity for haircolor in concept Firefighter: 0.3927\n",
      "Average disparity for perceived_age_presentation in concept Firefighter: 0.1099\n",
      "Average disparity for perceived_gender_presentation in concept Firefighter: 0.1720\n",
      "Average disparity for has_facial_hair in concept Firefighter: 0.0396\n",
      "Average disparity for has_tattoo in concept Firefighter: 0.1673\n",
      "Average disparity for has_cap in concept Firefighter: 0.0146\n",
      "Average disparity for has_mask in concept Firefighter: 0.1153\n",
      "Average disparity for has_headscarf in concept Firefighter: 0.1181\n",
      "Average disparity for has_eyeware in concept Firefighter: 0.0312\n",
      "Average disparity for light_tone_indicator in concept Firefighter: 0.0081\n",
      "Average disparity for hairtype in concept Doctor: 0.2105\n",
      "Average disparity for haircolor in concept Doctor: 0.2074\n",
      "Average disparity for perceived_age_presentation in concept Doctor: 0.0273\n",
      "Average disparity for perceived_gender_presentation in concept Doctor: 0.1142\n",
      "Average disparity for has_facial_hair in concept Doctor: 0.0001\n",
      "Average disparity for has_tattoo in concept Doctor: 0.1712\n",
      "Average disparity for has_cap in concept Doctor: 0.0107\n",
      "Average disparity for has_mask in concept Doctor: 0.0099\n",
      "Average disparity for has_headscarf in concept Doctor: 0.0068\n",
      "Average disparity for has_eyeware in concept Doctor: 0.0182\n",
      "Average disparity for light_tone_indicator in concept Doctor: 0.1011\n",
      "Average disparity for hairtype in concept Farmer: 0.1419\n",
      "Average disparity for haircolor in concept Farmer: 0.1686\n",
      "Average disparity for perceived_age_presentation in concept Farmer: 0.0549\n",
      "Average disparity for perceived_gender_presentation in concept Farmer: 0.1324\n",
      "Average disparity for has_facial_hair in concept Farmer: 0.0418\n",
      "Average disparity for has_tattoo in concept Farmer: 0.2032\n",
      "Average disparity for has_cap in concept Farmer: 0.0389\n",
      "Average disparity for has_mask in concept Farmer: 0.0533\n",
      "Average disparity for has_headscarf in concept Farmer: 0.0391\n",
      "Average disparity for has_eyeware in concept Farmer: 0.0206\n",
      "Average disparity for light_tone_indicator in concept Farmer: 0.0391\n",
      "Average disparity for hairtype in concept Engineer: 0.0950\n",
      "Average disparity for haircolor in concept Engineer: 0.0611\n",
      "Average disparity for perceived_age_presentation in concept Engineer: 0.0415\n",
      "Average disparity for perceived_gender_presentation in concept Engineer: 0.1313\n",
      "Average disparity for has_facial_hair in concept Engineer: 0.0251\n",
      "Average disparity for has_tattoo in concept Engineer: 0.1626\n",
      "Average disparity for has_cap in concept Engineer: 0.1740\n",
      "Average disparity for has_mask in concept Engineer: 0.1027\n",
      "Average disparity for has_headscarf in concept Engineer: 0.1103\n",
      "Average disparity for has_eyeware in concept Engineer: 0.0326\n",
      "Average disparity for light_tone_indicator in concept Engineer: 0.0508\n",
      "Average disparity for hairtype in concept Waiter: 0.0514\n",
      "Average disparity for haircolor in concept Waiter: 0.0784\n",
      "Average disparity for perceived_age_presentation in concept Waiter: 0.0968\n",
      "Average disparity for perceived_gender_presentation in concept Waiter: 0.0578\n",
      "Average disparity for has_facial_hair in concept Waiter: 0.0875\n",
      "Average disparity for has_tattoo in concept Waiter: 0.0896\n",
      "Average disparity for has_cap in concept Waiter: 0.0029\n",
      "Average disparity for has_mask in concept Waiter: 0.0465\n",
      "Average disparity for has_headscarf in concept Waiter: 0.0947\n",
      "Average disparity for has_eyeware in concept Waiter: 0.0634\n",
      "Average disparity for light_tone_indicator in concept Waiter: 0.0125\n",
      "Average disparity for hairtype in concept Judge: 0.4441\n",
      "Average disparity for haircolor in concept Judge: 0.0996\n",
      "Average disparity for perceived_age_presentation in concept Judge: 0.0370\n",
      "Average disparity for perceived_gender_presentation in concept Judge: 0.1159\n",
      "Average disparity for has_facial_hair in concept Judge: 0.0222\n",
      "Average disparity for has_tattoo in concept Judge: 0.1250\n",
      "Average disparity for has_cap in concept Judge: 0.1364\n",
      "Average disparity for has_mask in concept Judge: 0.1250\n",
      "Average disparity for has_headscarf in concept Judge: 0.1250\n",
      "Average disparity for has_eyeware in concept Judge: 0.0586\n",
      "Average disparity for light_tone_indicator in concept Judge: 0.1304\n",
      "Average disparity for hairtype in concept Chef: 0.0513\n",
      "Average disparity for haircolor in concept Chef: 0.3832\n",
      "Average disparity for perceived_age_presentation in concept Chef: 0.0784\n",
      "Average disparity for perceived_gender_presentation in concept Chef: 0.1111\n",
      "Average disparity for has_facial_hair in concept Chef: 0.0242\n",
      "Average disparity for has_tattoo in concept Chef: 0.0976\n",
      "Average disparity for has_cap in concept Chef: 0.0952\n",
      "Average disparity for has_mask in concept Chef: 0.0952\n",
      "Average disparity for has_headscarf in concept Chef: 0.0930\n",
      "Average disparity for has_eyeware in concept Chef: 0.0322\n",
      "Average disparity for light_tone_indicator in concept Chef: 0.1731\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store cumulative disparity and count for each attribute / concept combo\n",
    "disparity_dict = {}\n",
    "\n",
    "# Iterate through each concept\n",
    "for concept in concepts:\n",
    "    # Iterate through each attribute\n",
    "    for attribute_name, attribute_values in attributes.items():\n",
    "        total_disparity = 0\n",
    "        count = 0\n",
    "        # Iterate through each pair of attribute values\n",
    "        for i in range(len(attribute_values)):\n",
    "            for j in range(i + 1, len(attribute_values)):\n",
    "                attribute1 = {attribute_name: attribute_values[i]}\n",
    "                attribute2 = {attribute_name: attribute_values[j]}\n",
    "                # Compute disparity\n",
    "                disparity = compute_disparity(view, 'pred', concept, attribute1, attribute2)\n",
    "                total_disparity += abs(disparity)\n",
    "                count += 1\n",
    "        # Update the dictionary with the average disparity for this attribute within this concept\n",
    "        key = (concept, attribute_name)\n",
    "        average_disparity = total_disparity / count if count > 0 else 0\n",
    "        disparity_dict[key] = average_disparity\n",
    "\n",
    "# Output the average disparity for each attribute within each concept\n",
    "for key, average_disparity in disparity_dict.items():\n",
    "    concept, attribute_name = key\n",
    "    print(f'Average disparity for {attribute_name} in concept {concept}: {average_disparity:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d7d7a61c-9e31-43dd-9e19-4910891d0abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average disparity in concept Police: 0.0706\n",
      "Average disparity in concept Mechanic: 0.0758\n",
      "Average disparity in concept Pilot: 0.0142\n",
      "Average disparity in concept Firefighter: 0.2435\n",
      "Average disparity in concept Doctor: 0.1631\n",
      "Average disparity in concept Farmer: 0.1315\n",
      "Average disparity in concept Engineer: 0.0818\n",
      "Average disparity in concept Waiter: 0.0653\n",
      "Average disparity in concept Judge: 0.2172\n",
      "Average disparity in concept Chef: 0.1790\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store cumulative disparity and count for each concept\n",
    "concept_disparity_dict = {}\n",
    "\n",
    "# Iterate through each concept\n",
    "for concept in concepts:\n",
    "    total_disparity = 0\n",
    "    count = 0\n",
    "    # Iterate through each attribute\n",
    "    for attribute_name, attribute_values in attributes.items():\n",
    "        # Iterate through each pair of attribute values\n",
    "        for i in range(len(attribute_values)):\n",
    "            for j in range(i + 1, len(attribute_values)):\n",
    "                attribute1 = {attribute_name: attribute_values[i]}\n",
    "                attribute2 = {attribute_name: attribute_values[j]}\n",
    "                # Compute disparity\n",
    "                disparity = compute_disparity(view, 'pred', concept, attribute1, attribute2)\n",
    "                total_disparity += abs(disparity)\n",
    "                count += 1\n",
    "    # Update the dictionary with the average disparity for this concept\n",
    "    average_disparity = total_disparity / count if count > 0 else 0\n",
    "    concept_disparity_dict[concept] = average_disparity\n",
    "\n",
    "# Output the average disparity for each concept\n",
    "for concept, average_disparity in concept_disparity_dict.items():\n",
    "    print(f'Average disparity in concept {concept}: {average_disparity:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25d79afe-ba89-4651-a3e8-aeb58d2cff55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1242"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcluate overall model disparity\n",
    "mean_disparity = round(np.mean(list(concept_disparity_dict.values())), 4)\n",
    "mean_disparity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "44e87906-4cfc-4661-972e-a991c94fe944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write overall disparity to txt file\n",
    "file_name = f'mean_overall_disparity_{weights_path.replace(\".pth\", \"\")}.txt'\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(str(mean_disparity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c1c62e65-bb4e-444a-b05b-e0baf6120d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved to mean_concept_disparity_student_model_weights3.txt\n"
     ]
    }
   ],
   "source": [
    "# write disparity by concept to txt file\n",
    "file_name = f'mean_concept_disparity_{weights_path.replace(\".pth\", \"\")}.txt'\n",
    "\n",
    "with open(file_name, 'w') as file:\n",
    "    json.dump(concept_disparity_dict, file, indent=4)\n",
    "\n",
    "print(f'Dictionary saved to {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94748fc0-eb07-4b34-9941-270f6fc3188a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fab484-83d8-45d3-8363-41c6c358fc75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
