{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f95522e-06b6-40ba-b190-c751ec0809d7",
   "metadata": {},
   "source": [
    "# Norm and Dir with CIFAR100\n",
    "__resnet32x4 and resnet 8x4__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d692c1e4-7222-46be-a9c4-ffd1331863b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loss_functions import DKDLoss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from models_package.models import Teacher, Student\n",
    "from torchvision import datasets, transforms, models\n",
    "import models_package\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import os\n",
    "# new libraries\n",
    "from data.data_loader import load_cifar10, load_cifar100, load_imagenet, load_prof\n",
    "import boto3\n",
    "import io\n",
    "from utils.loss_functions import tkd_kdloss, DD_loss, AD_loss, RKDDistanceLoss, RKDAngleLoss\n",
    "from utils.compare_tools import compare_model_size, compare_inference_time, compare_performance_metrics, plot_comparison\n",
    "from utils.misc_tools import best_LR, train_teacher, retrieve_teacher_class_weights, new_teacher_class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa78713-3fc2-461a-bed6-e4e500e0d133",
   "metadata": {},
   "source": [
    "## Find best LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94bb557-3c3d-45c0-9b41-ccad6ca0ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.003  # 0.01 for resnet34x2 & 0.1 for resnet8 & 0.003 for resnet 8x4\n",
    "num_epochs = 240\n",
    "num_workers = 2\n",
    "batch_size = 16\n",
    "temperature = 4.0\n",
    "alpha = 0.9\n",
    "momentum = 0.9\n",
    "num_classes = 100\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "\n",
    "# new parameters\n",
    "# lr_input = 0.1\n",
    "# momentum_input = 0.9\n",
    "weight_decay_input = 5e-4\n",
    "# epochs = 20\n",
    "# T = 4.0 # temperatureture\n",
    "# alpha = 0.9\n",
    "patience = 5  # for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f4ec2-e075-43a4-8889-67d11543463a",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e47e3a-3889-4b4b-b552-f16efff3aac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load IdenProf dataset\n",
    "trainloader, testloader  = load_cifar100(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b17689-868d-48c0-a54b-955cd8e49067",
   "metadata": {},
   "source": [
    "## Load in models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ba578-9fee-4f80-a4f3-c7f8277c41bc",
   "metadata": {},
   "source": [
    "### resnet32x4_idenprof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08995195-43a8-4d2e-a181-af291b5282c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "\n",
    "# Create instances of your models\n",
    "# teacher_model = torchvision.models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1).cuda()\n",
    "# teacher_model.eval()  # Set teacher model to evaluation mode\n",
    "# student_model = torchvision.models.resnet18(weights=None).cuda()\n",
    "\n",
    "teacher_name = 'resnet32x4_cifar'\n",
    "teacher_model = models_package.__dict__[teacher_name](num_class=num_classes)\n",
    "# print(teacher_model.fc.in_features)\n",
    "teacher_model.fc = nn.Linear(16384, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a6c9c8-9d5f-46ad-9373-fd0aed875848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=16384, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52d20e-74bc-47de-98fe-459fffd5a4ff",
   "metadata": {},
   "source": [
    "### resnet8_idenprof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "351776d3-928e-41ae-bdcf-67077724f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_name = 'resnet8_idenprof'\n",
    "# teacher_model = models_package.__dict__[teacher_name](num_class=10)\n",
    "# teacher_model.fc = nn.Linear(teacher_model.fc.in_features, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f15cba-6773-47ad-bf3c-3a168a4ad3e0",
   "metadata": {},
   "source": [
    "### resnet8x4_idenprof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d207ab1-84fd-4a3c-9d3b-5ea16e136325",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_name = 'resnet8x4_cifar'\n",
    "student_model = models_package.__dict__[student_name](num_class=num_classes)\n",
    "# student_model.fc = nn.Linear(student_model.fc.in_features, num_classes)\n",
    "student_model.fc = nn.Linear(16384, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "918fa4ed-2b3e-4b40-be75-1a6500c55eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optimizer and scheduler for the teacher model\n",
    "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "teacher_scheduler = torch.optim.lr_scheduler.StepLR(teacher_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Optimizer and scheduler for the student model\n",
    "student_optimizer = optim.SGD(student_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "student_scheduler = torch.optim.lr_scheduler.StepLR(student_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Assuming the device is a CUDA device if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38683df-c675-4462-af47-bc4c418ab11b",
   "metadata": {},
   "source": [
    "## Best LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef9c911-8961-4f19-bf38-50f5fd897712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 3125/3125 [1:07:28<00:00,  1.30s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████| 3125/3125 [1:07:27<00:00,  1.30s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████| 3125/3125 [1:07:27<00:00,  1.30s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best learning rate: 0.0412096506841688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0412096506841688"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_lr = best_LR('resnet32x4_lr', teacher_model, trainloader, \n",
    "                     criterion, teacher_optimizer, \n",
    "                     teacher_scheduler, num_epochs=3, emb = True)\n",
    "teacher_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee5fbe5-581e-4af0-aeef-c2bd0368ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_lr = best_LR('resnet8x4_lr', student_model, trainloader,\n",
    "                     criterion, student_optimizer, student_scheduler, \n",
    "                     num_epochs=3, emb = True)\n",
    "student_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "912793a1-0a69-474f-8470-3b2114d7c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_lr = 0.04684733314426343\n",
    "student_lr = 0.04965552407346163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c54efd-b7d9-44ea-a269-d0d12e87258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler for the student model\n",
    "student_optimizer = optim.SGD(student_model.parameters(), lr=student_lr, momentum=momentum)\n",
    "student_scheduler = torch.optim.lr_scheduler.StepLR(student_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Optimizer and scheduler for the teacher model\n",
    "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=teacher_lr, momentum=momentum)\n",
    "teacher_scheduler = torch.optim.lr_scheduler.StepLR(teacher_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Assuming the device is a CUDA device if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff0123-c39b-442f-b806-d879d38f466c",
   "metadata": {},
   "source": [
    "## Train Leaderboard Teacher Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9676211-b112-4cfc-b9ff-25295f196562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(losses):\n",
    "    epochs = range(1, len(losses) + 1)\n",
    "    plt.plot(epochs, losses)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_teacher(model_name, model, trainloader, criterion, optimizer, scheduler, num_epochs=240, patience=5):\n",
    "    ''' A function to train the teacher models'''\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    epoch_losses = [] \n",
    "    val_losses = []\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    best_train_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        for i, (inputs, labels) in enumerate(tqdm(trainloader)):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            _, outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            if i % 100 == 99:  # Print every 100 mini-batches\n",
    "                # print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        epoch_loss /= num_batches  \n",
    "        epoch_losses.append(epoch_loss)\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_val_loss = 0.0\n",
    "        num_batches = 0  \n",
    "        with torch.no_grad():\n",
    "            for i, (inputs, labels) in enumerate(tqdm(testloader)):\n",
    "                val_inputs, val_labels = inputs.to(device), labels.to(device)\n",
    "                # val_inputs = val_data['img'].to(device)\n",
    "                # val_labels = val_data['label'].to(device)\n",
    "    \n",
    "                # Forward pass for validation\n",
    "                _, val_outputs = model(val_inputs)\n",
    "    \n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                total_val_loss += val_loss.item()\n",
    "    \n",
    "                # Compute the validation accuracy\n",
    "                _, predicted = torch.max(val_outputs, 1)\n",
    "                total_samples += val_labels.size(0)\n",
    "                total_correct += (predicted == val_labels).sum().item()\n",
    "                num_batches += 1\n",
    "            \n",
    "            total_val_loss /= num_batches\n",
    "            val_losses.append(total_val_loss)\n",
    "            accuracy = total_correct / total_samples\n",
    "            print(f'*****Epoch {epoch + 1}/{num_epochs}*****\\n' \n",
    "            f'*****Train Loss: {epoch_loss: .6f} Val Loss: {total_val_loss: .6f}*****\\n'\n",
    "            f'*****Validation Accuracy: {accuracy * 100:.2f}%*****\\n')\n",
    "\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if epoch_loss < best_train_loss:\n",
    "            best_train_loss = epoch_loss\n",
    "            patience_counter = 0 \n",
    "            \n",
    "            # checkpoint\n",
    "            save_path = './weights/'\n",
    "\n",
    "            model_save_path = os.path.join(save_path, model_name)\n",
    "            \n",
    "            os.makedirs(model_save_path, exist_ok=True)\n",
    "        \n",
    "            model_save_name = os.path.join(model_save_path, 'checkpoint.pth')\n",
    "            mode_weights_name = os.path.join(model_save_path, 'weights.pth')\n",
    "        \n",
    "            torch.save(model.state_dict(), mode_weights_name)\n",
    "            torch.save(model, model_save_name)\n",
    "            \n",
    "            # model_save_name = str(save_path + model_name + '/checkpoint.pth')\n",
    "            # mode_weights_name = str(save_path + model_name + '/weights.pth')\n",
    "\n",
    "            # torch.save(model.state_dict(), mode_weights_name)\n",
    "            # torch.save(model, model_save_name)\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Finished Training Teacher\")\n",
    "    plot_loss_curve(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2e635-6af8-485c-8e1f-54dc2cca58a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 3125/3125 [1:06:41<00:00,  1.28s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 3125/3125 [21:39<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Epoch 1/260*****\n",
      "*****Train Loss:  4.677284 Val Loss:  4.150500*****\n",
      "*****Validation Accuracy: 7.84%*****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 3125/3125 [1:03:43<00:00,  1.22s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 3125/3125 [21:39<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Epoch 2/260*****\n",
      "*****Train Loss:  4.079858 Val Loss:  3.986141*****\n",
      "*****Validation Accuracy: 10.72%*****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 3125/3125 [1:03:47<00:00,  1.22s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 3125/3125 [21:39<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Epoch 3/260*****\n",
      "*****Train Loss:  3.935922 Val Loss:  3.858477*****\n",
      "*****Validation Accuracy: 12.53%*****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 3125/3125 [1:03:48<00:00,  1.23s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 3125/3125 [21:39<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Epoch 4/260*****\n",
      "*****Train Loss:  3.870678 Val Loss:  3.780193*****\n",
      "*****Validation Accuracy: 13.52%*****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 3125/3125 [1:03:49<00:00,  1.23s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 3125/3125 [21:39<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Epoch 5/260*****\n",
      "*****Train Loss:  3.808471 Val Loss:  3.731171*****\n",
      "*****Validation Accuracy: 14.45%*****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 3125/3125 [1:03:48<00:00,  1.23s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 3125/3125 [21:41<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Epoch 6/260*****\n",
      "*****Train Loss:  3.746586 Val Loss:  3.686303*****\n",
      "*****Validation Accuracy: 15.54%*****\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████████████████████████████████████████████████████████████   | 2992/3125 [1:01:06<02:42,  1.22s/it]"
     ]
    }
   ],
   "source": [
    "teacher_resnet32x4 = \\\n",
    "    train_teacher('resnet_32x4', teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, num_epochs=260, patience=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b5bbf58-6e41-4401-a90b-06c8df2a5d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############################# Testing Starts #############################\n",
    "# def train_teacher(model_name, model, trainloader, testloader, criterion, optimizer, scheduler, num_epochs=240, patience=5):\n",
    "#     ''' A function to train and validate the teacher models'''\n",
    "#     best_val_loss = float('inf')\n",
    "#     patience_counter = 0\n",
    "#     val_losses = []\n",
    "    \n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         running_loss = 0.0\n",
    "#         for inputs, labels in tqdm(trainloader):\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             _, outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         # Validation\n",
    "#         model.eval()\n",
    "#         total_val_loss = 0.0\n",
    "#         total_correct = 0\n",
    "#         total_samples = 0\n",
    "#         with torch.no_grad():\n",
    "#             for inputs, labels in tqdm(testloader):\n",
    "#                 inputs, labels = inputs.to(device), labels.to(device)\n",
    "#                 _, outputs = model(inputs)\n",
    "#                 val_loss = criterion(outputs, labels)\n",
    "#                 total_val_loss += val_loss.item()\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 total_correct += (predicted == labels).sum().item()\n",
    "#                 total_samples += labels.size(0)\n",
    "\n",
    "#         avg_val_loss = total_val_loss / len(testloader)\n",
    "#         val_losses.append(avg_val_loss)\n",
    "#         accuracy = total_correct / total_samples\n",
    "#         print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {running_loss / len(trainloader):.4f}, Val Loss: {avg_val_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "#         # Early Stopping\n",
    "#         if avg_val_loss < best_val_loss:\n",
    "#             best_val_loss = avg_val_loss\n",
    "#             patience_counter = 0\n",
    "#             # Save model\n",
    "#             torch.save(model.state_dict(), f'{model_name}_best.pth')\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping\")\n",
    "#             break\n",
    "\n",
    "#         scheduler.step()\n",
    "#         model.train()\n",
    "\n",
    "#     # Plot validation losses\n",
    "#     plt.plot(val_losses)\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Validation Loss')\n",
    "#     plt.title('Validation Loss Over Epochs')\n",
    "#     plt.show()\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # Train the model\n",
    "# teacher_model = train_teacher('resnet32x4_cifar', teacher_model, trainloader, testloader, \n",
    "#                               criterion, teacher_optimizer, teacher_scheduler,\n",
    "#                               num_epochs=260, patience=5)\n",
    "# ############################# Testing Ends #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2b0fb-f884-4ab0-ae34-02b5d4a3821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the student and teacher model weights and architecture\n",
    "torch.save(teacher_model.state_dict(), 'teacher_model_weights_resnet324_cifar100.pth')\n",
    "torch.save(teacher_model, 'teacher_model_resnet324_cifar100.pth')\n",
    "print('teacher weights and architecture saved and exported')\n",
    "\n",
    "# torch.save(student_model.state_dict(), 'student_model_weights_resnet84_wider.pth')\n",
    "# torch.save(student_model, 'student_model_resnet84_wider.pth')\n",
    "# print('student weights and architecture saved and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14230783-3c9d-4428-8286-0520dbaf986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_student_with_distillation(student, teacher, trainloader, testloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs, patience=5):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    student.to(device)\n",
    "    teacher.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        student.train()\n",
    "        teacher.eval()  # Ensure the teacher is in evaluation mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in tqdm(trainloader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            student_outputs = student(inputs)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "            \n",
    "            ce_loss = criterion(student_outputs[0], labels)\n",
    "            kd_loss = tkd_kdloss(student_outputs[0], teacher_outputs[0], temperature)\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        # Validation\n",
    "        student.eval()\n",
    "        total_val_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(testloader):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                _, val_outputs = student(inputs)\n",
    "                val_loss = criterion(val_outputs, labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "                _, predicted = torch.max(val_outputs, 1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(testloader)\n",
    "        val_accuracy = total_correct / total_samples\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} - Loss: {running_loss / len(trainloader):.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy * 100:.2f}%')\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(student.state_dict(), 'student_model_best.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Finished Training Student\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfafaa88-8aa8-4752-bc30-4f8b5b6e3e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_student_with_distillation(student_model, teacher_model, trainloader, \n",
    "                                testloader, criterion, student_optimizer, \n",
    "                                student_scheduler, device, alpha, temperature, \n",
    "                                num_epochs=240, patience=patience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b4a165-4e16-492f-b40f-b7c7b3d17905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the student and teacher model weights and architecture\n",
    "# torch.save(teacher_model.state_dict(), 'teacher_model_weights_resnet324_wider.pth')\n",
    "# torch.save(teacher_model, 'teacher_model_resnet324_wider.pth')\n",
    "# print('teacher weights and architecture saved and exported')\n",
    "\n",
    "torch.save(student_model.state_dict(), 'student_model_weights_resnet84_cifar100.pth')\n",
    "torch.save(student_model, 'student_model_resnet84_cifar100.pth')\n",
    "print('student weights and architecture saved and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c407e2-9229-47ac-8f01-4170682fb8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the student model architecture and weights\n",
    "student_model = torch.load('student_model_resnet84_cifar100.pth')\n",
    "student_model.load_state_dict(torch.load('student_model_weights_resnet84_cifar100.pth'))\n",
    "student_model = student_model.to(device)\n",
    "\n",
    "# Load the student model architecture and weights\n",
    "teacher_model = torch.load('teacher_model_resnet324_cifar100.pth')\n",
    "teacher_model.load_state_dict(torch.load('teacher_model_weights_resnet324_cifar100.pth'))\n",
    "teacher_model = teacher_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a099fde1-6280-4074-a560-d2449f77eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    inputs, _ = next(iter(dataloader))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher(inputs)\n",
    "    teacher_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        student_outputs = student(inputs)\n",
    "    student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n",
    "\n",
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs.to(device))\n",
    "            student_outputs = student(inputs.to(device))\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(torch.argmax(teacher_outputs, dim=1).cpu().numpy())\n",
    "        all_student_preds.append(torch.argmax(student_outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),  # Updated line\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def plot_comparison(labels, teacher_values, student_values, title, ylabel):\n",
    "    # Convert parameter count to millions\n",
    "    if 'Parameter Count' in title or 'Parameter Count' in ylabel:\n",
    "        teacher_values = [value / 1e6 for value in teacher_values]\n",
    "        student_values = [value / 1e6 for value in student_values]\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, teacher_values, width, label='Teacher')\n",
    "    rects2 = ax.bar(x + width/2, student_values, width, label='Student')\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ada358-e1a5-470b-aace-a88274c3de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the comparison and plotting functions after training\n",
    "teacher_params, student_params = compare_model_size(teacher_model, student_model)\n",
    "teacher_time, student_time = compare_inference_time(teacher_model, student_model, trainloader)\n",
    "performance_metrics = compare_performance_metrics(teacher_model, student_model, trainloader)\n",
    "\n",
    "# Extracting the metric values for plotting\n",
    "performance_labels = ['accuracy', 'precision', 'recall', 'f1']\n",
    "teacher_performance_values = [performance_metrics[metric][0] for metric in performance_labels]\n",
    "student_performance_values = [performance_metrics[metric][1] for metric in performance_labels]\n",
    "\n",
    "# Plotting the comparison for performance metrics\n",
    "plot_comparison(performance_labels, teacher_performance_values, student_performance_values, 'Performance Comparison', 'Score')\n",
    "\n",
    "# Plotting the comparison for model size\n",
    "model_size_labels = ['Model Size']\n",
    "teacher_model_size_values = [teacher_params]\n",
    "student_model_size_values = [student_params]\n",
    "plot_comparison(model_size_labels, teacher_model_size_values, student_model_size_values, 'Model Size Comparison', 'Parameter Count (millions)')\n",
    "\n",
    "# Plotting the comparison for inference time\n",
    "inference_time_labels = ['Inference Time']\n",
    "teacher_inference_time_values = [teacher_time]\n",
    "student_inference_time_values = [student_time]\n",
    "plot_comparison(inference_time_labels, teacher_inference_time_values, student_inference_time_values, 'Inference Time Comparison', 'Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51452f7-eac4-4f7c-b9c5-63df2cb55968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the comparison and plotting functions after training\n",
    "teacher_params, student_params = compare_model_size(teacher_model, student_model)\n",
    "teacher_time, student_time = compare_inference_time(teacher_model, student_model, testloader)\n",
    "performance_metrics = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "\n",
    "# Extracting the metric values for plotting\n",
    "performance_labels = ['accuracy', 'precision', 'recall', 'f1']\n",
    "teacher_performance_values = [performance_metrics[metric][0] for metric in performance_labels]\n",
    "student_performance_values = [performance_metrics[metric][1] for metric in performance_labels]\n",
    "\n",
    "# Plotting the comparison for performance metrics\n",
    "plot_comparison(performance_labels, teacher_performance_values, student_performance_values, 'Performance Comparison', 'Score')\n",
    "\n",
    "# Plotting the comparison for model size\n",
    "model_size_labels = ['Model Size']\n",
    "teacher_model_size_values = [teacher_params]\n",
    "student_model_size_values = [student_params]\n",
    "plot_comparison(model_size_labels, teacher_model_size_values, student_model_size_values, 'Model Size Comparison', 'Parameter Count (millions)')\n",
    "\n",
    "# Plotting the comparison for inference time\n",
    "inference_time_labels = ['Inference Time']\n",
    "teacher_inference_time_values = [teacher_time]\n",
    "student_inference_time_values = [student_time]\n",
    "plot_comparison(inference_time_labels, teacher_inference_time_values, student_inference_time_values, 'Inference Time Comparison', 'Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f25322-df68-4f32-a5a3-1b6c71953aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c168d62-cb67-4d7b-a1e8-8e8e7e538f4c",
   "metadata": {},
   "source": [
    "## Stop Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3566ec7-188c-4c69-9000-389a916f890f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a4c98-c17b-45c1-8093-73a84e81ec8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbc8ee4-3a38-40fd-9261-79b1bf2d15db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3d28eb-9c4c-4f5f-bcf0-c9ed3995bec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b014c319-5c75-40b6-ad6c-54bcdf34103b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ccacd-e7dc-434b-9db3-7b532e6ac951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b100ebbb-97d0-459a-a26f-3e62299fd6ff",
   "metadata": {},
   "source": [
    "## Extract Class Weights for Norm and Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f15c2d-b937-4214-9964-0a3e0e75e5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load in model and weights\n",
    "model_path = './weights/resnet_32x4/checkpoint.pth'\n",
    "weights_path = './weights/resnet_32x4/weights.pth'\n",
    "test_path = './weights/resnet_32x4/test.pth'\n",
    "# idenprof_resnet32x4_model = torch.load(weights_path)\n",
    "# # idenprof_resnet32x4_model.load_state_dict(torch.load(weights_path))\n",
    "# # idenprof_resnet32x4_model.eval()\n",
    "# # idenprof_resnet32x4_model.items()\n",
    "\n",
    "# # import torch, torchvision.models\n",
    "# # model = torchvision.models.vgg16()\n",
    "# # path = 'test.pth'\n",
    "# torch.save(idenprof_resnet32x4_model.state_dict(), test_path) # nothing else here\n",
    "# idenprof_resnet32x4_model.load_state_dict(torch.load(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4834393a-0ebf-43c7-a9b1-f5d445195b03",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m models_package\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[model_name](num_class\u001b[38;5;241m=\u001b[39mnum_class)\n\u001b[1;32m      4\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(weights_path)\n\u001b[0;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "model_name = 'resnet32x4_idenprof'\n",
    "num_class = 10\n",
    "model = models_package.__dict__[model_name](num_class=num_class)\n",
    "checkpoint = torch.load(weights_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87edb587-ea4f-4155-b20f-38ed0d9240d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdb61e73-3833-484a-8021-9f0fb61ace0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (fc): Linear(in_features=12544, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(idenprof_resnet32x4_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a872e6e-428c-413b-bdcf-363fe4ddc5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_teacher_class_weights(model_name, model_weight_path, num_class, data_name, dataloader, batch_size):\n",
    "    ''' Use the extracted feature embeddings to create a json of class means for teacher'''\n",
    "    model = models_package.__dict__[model_name](num_class=num_class)\n",
    "    model_ckpt = models_package.__dict__[model_name](num_class=num_class)\n",
    "    print('Visualized the embedding feature of the {} model on the train set'.format(model_name))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_ckpt.to(device)\n",
    "    model_ckpt.load_state_dict(torch.load(model_weight_path))\n",
    "    model_ckpt.eval()\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in model_ckpt.items():\n",
    "        name = k[7:]   # remove 'module.'\n",
    "        new_state_dict[name] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model = model.cuda()\n",
    "\n",
    "    emb = get_emb_fea(model=model, dataloader=dataloader, batch_size=batch_size)\n",
    "    emb_json = json.dumps(emb, indent=4)\n",
    "    with open(\"./class_means/{}_embedding_fea/{}.json\".format(data_name, model_name), 'w', encoding='utf-8') as f:\n",
    "        f.write(emb_json)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c9a6a35-ba7c-4173-becc-a625ea0fd8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualized the embedding feature of the resnet32x4_idenprof model on the train set\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretrieve_teacher_class_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet32x4_idenprof\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmodel_weight_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./weights/resnet_32x4/weights.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mnum_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43midenprof\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                              \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/W210-Capstone/notebooks/utils/misc_tools.py:181\u001b[0m, in \u001b[0;36mretrieve_teacher_class_weights\u001b[0;34m(model_name, model_weight_path, num_class, data_name, dataloader, batch_size)\u001b[0m\n\u001b[1;32m    179\u001b[0m model_ckpt\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    180\u001b[0m new_state_dict \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m--> 181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_ckpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m():\n\u001b[1;32m    182\u001b[0m     name \u001b[38;5;241m=\u001b[39m k[\u001b[38;5;241m7\u001b[39m:]   \u001b[38;5;66;03m# remove 'module.'\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     new_state_dict[name] \u001b[38;5;241m=\u001b[39m v\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "retrieve_teacher_class_weights(model_name = 'resnet32x4_idenprof', \n",
    "                               model_weight_path = './weights/resnet_32x4/weights.pth', \n",
    "                               num_class = 10, \n",
    "                               data_name = 'idenprof',\n",
    "                               dataloader = trainloader, \n",
    "                               batch_size = 128\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06a0ce2c-6075-40e9-b922-9066069d1d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualized the embedding feature of the resnet32x4_idenprof model on the train set\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnew_teacher_class_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet32x4_idenprof\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mmodel_weight_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./weights/resnet_32x4/weights.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mnum_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43midenprof\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                              \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/W210-Capstone/notebooks/utils/misc_tools.py:210\u001b[0m, in \u001b[0;36mnew_teacher_class_weights\u001b[0;34m(model_name, model_weight_path, num_class, data_name, dataloader, batch_size)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# model_ckpt.to(device)\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;66;03m# model_ckpt.load_state_dict(torch.load(model_weight_path))\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# model_ckpt.eval()\u001b[39;00m\n\u001b[1;32m    209\u001b[0m new_checkpoint \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    211\u001b[0m     name \u001b[38;5;241m=\u001b[39m k[\u001b[38;5;241m7\u001b[39m:] \u001b[38;5;66;03m# remove module.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m     new_checkpoint[name] \u001b[38;5;241m=\u001b[39m v\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model_state_dict'"
     ]
    }
   ],
   "source": [
    "new_teacher_class_weights(model_name = 'resnet32x4_idenprof', \n",
    "                               model_weight_path = './weights/resnet_32x4/weights.pth', \n",
    "                               num_class = 10, \n",
    "                               data_name = 'idenprof',\n",
    "                               dataloader = trainloader, \n",
    "                               batch_size = 128\n",
    "                              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d81180-aa65-44b9-b1c1-b4567c941d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ade374e-af03-49f7-b88d-b1308479820e",
   "metadata": {},
   "source": [
    "## Train Leaderboard Student Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3372ef8-48e4-44d6-920d-a2529a5fa199",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Need studnet model loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dfc81a-80c7-4909-ac9d-c321d19274c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Studnet Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c2041-5f41-47cc-9f2b-3729a8f1100d",
   "metadata": {},
   "source": [
    "## Save Models and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b7201-9308-43b8-a7ad-aea5898e6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## backup\n",
    "# Save the student and teacher model weights and architecture\n",
    "torch.save(teacher_model.state_dict(), 'teacher_model_weights_resnet8_4.pth')\n",
    "torch.save(teacher_model, 'testing_teacher_model_resnet8_4.pth')\n",
    "print('student weights and architecture saved and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c6094-8e8b-4c15-a886-3222640db9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Saving weights and movel using s3 bucket ######################\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3 = session.client('s3')\n",
    "\n",
    "bucket_name = '210bucket' \n",
    "\n",
    "# Teacher Model\n",
    "#### IMPORTANT!!!!! Change the file name so that you do not overwrite the existing files\n",
    "teacher_model_weights_path = 'weights/teacher_model_weights_resnet8_4.pth'\n",
    "teacher_model_path = 'models/testing_teacher_model_resnet8_4.pth'\n",
    "\n",
    "# Save state dict to buffer\n",
    "teacher_model_weights_buffer = io.BytesIO()\n",
    "torch.save(teacher_model.state_dict(), teacher_model_weights_buffer)\n",
    "teacher_model_weights_buffer.seek(0)\n",
    "\n",
    "# Save entire model to buffer\n",
    "teacher_model_buffer = io.BytesIO()\n",
    "torch.save(teacher_model, teacher_model_buffer)\n",
    "teacher_model_buffer.seek(0)\n",
    "\n",
    "# Upload to S3\n",
    "s3.put_object(Bucket=bucket_name, Key=teacher_model_weights_path, Body=teacher_model_weights_buffer)\n",
    "s3.put_object(Bucket=bucket_name, Key=teacher_model_path, Body=teacher_model_buffer)\n",
    "print('teacher weights and architecture saved and exported to S3')\n",
    "\n",
    "# # Student Model\n",
    "# #### IMPORTANT!!!!! Change the file name so that you do not overwrite the existing files\n",
    "# student_model_weights_path = 'weights/student_model_weights.pth' \n",
    "# student_model_path = 'models/student_model.pth'\n",
    "\n",
    "# # Save state dict to buffer\n",
    "# student_model_weights_buffer = io.BytesIO()\n",
    "# torch.save(student_model.state_dict(), student_model_weights_buffer)\n",
    "# student_model_weights_buffer.seek(0)\n",
    "\n",
    "# # Save entire model to buffer\n",
    "# student_model_buffer = io.BytesIO()\n",
    "# torch.save(student_model, student_model_buffer)\n",
    "# student_model_buffer.seek(0)\n",
    "\n",
    "# # Upload to S3\n",
    "# s3.put_object(Bucket=bucket_name, Key=student_model_weights_path, Body=student_model_weights_buffer)\n",
    "# s3.put_object(Bucket=bucket_name, Key=student_model_path, Body=student_model_buffer)\n",
    "# print('student weights and architecture saved and exported to S3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6731e70-0e45-4569-a2d1-15a459a05e9f",
   "metadata": {},
   "source": [
    "## Read Models and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b0894a3-e04a-4d27-a0b9-7b795e15ed59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=12544, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a session using Boto3 again \n",
    "session = boto3.session.Session()\n",
    "\n",
    "s3 = session.client('s3')\n",
    "bucket_name = '210bucket'  \n",
    "\n",
    "teacher_model_weights_s3_path = 'weights/idenprof_teacher_resnet32x4_weights.pth'\n",
    "# student_model_weights_s3_path = 'weights/testing_student_model_weights_rkd_prof.pth'\n",
    "\n",
    "# Read files directly into memory\n",
    "teacher_model_weights_buffer = io.BytesIO()\n",
    "# student_model_weights_buffer = io.BytesIO()\n",
    "\n",
    "s3.download_fileobj(bucket_name, teacher_model_weights_s3_path, teacher_model_weights_buffer)\n",
    "# s3.download_fileobj(bucket_name, student_model_weights_s3_path, student_model_weights_buffer)\n",
    "\n",
    "# Load the weights into the models\n",
    "teacher_model_weights_buffer.seek(0)  # Move to the beginning of the buffer\n",
    "# student_model_weights_buffer.seek(0)  \n",
    "\n",
    "######## MAKE SURE THAT YOU HAVE THE CORRECT MODELS FOR WEIGHTS ########\n",
    "# Teacher\n",
    "# teacher_name = 'resnet8x4_idenprof'\n",
    "teacher_name = 'resnet32x4_idenprof'\n",
    "teacher_model = models_package.__dict__[teacher_name](num_class=10)\n",
    "teacher_model.fc = nn.Linear(teacher_model.fc.in_features, 10)\n",
    "teacher_model.load_state_dict(torch.load(teacher_model_weights_buffer))\n",
    "teacher_model.eval()\n",
    "# # Student\n",
    "# student_model = CustomResNet18()\n",
    "# student_model.load_state_dict(torch.load(student_model_weights_buffer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2eeb83d-334b-4d55-8a38-ce7679c09b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in checkpoint: odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.0.downsample.1.bias', 'layer1.0.downsample.1.running_mean', 'layer1.0.downsample.1.running_var', 'layer1.0.downsample.1.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.bn2.num_batches_tracked', 'layer1.3.conv1.weight', 'layer1.3.bn1.weight', 'layer1.3.bn1.bias', 'layer1.3.bn1.running_mean', 'layer1.3.bn1.running_var', 'layer1.3.bn1.num_batches_tracked', 'layer1.3.conv2.weight', 'layer1.3.bn2.weight', 'layer1.3.bn2.bias', 'layer1.3.bn2.running_mean', 'layer1.3.bn2.running_var', 'layer1.3.bn2.num_batches_tracked', 'layer1.4.conv1.weight', 'layer1.4.bn1.weight', 'layer1.4.bn1.bias', 'layer1.4.bn1.running_mean', 'layer1.4.bn1.running_var', 'layer1.4.bn1.num_batches_tracked', 'layer1.4.conv2.weight', 'layer1.4.bn2.weight', 'layer1.4.bn2.bias', 'layer1.4.bn2.running_mean', 'layer1.4.bn2.running_var', 'layer1.4.bn2.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.bn2.num_batches_tracked', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.bn2.num_batches_tracked', 'layer2.4.conv1.weight', 'layer2.4.bn1.weight', 'layer2.4.bn1.bias', 'layer2.4.bn1.running_mean', 'layer2.4.bn1.running_var', 'layer2.4.bn1.num_batches_tracked', 'layer2.4.conv2.weight', 'layer2.4.bn2.weight', 'layer2.4.bn2.bias', 'layer2.4.bn2.running_mean', 'layer2.4.bn2.running_var', 'layer2.4.bn2.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.bn2.num_batches_tracked', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.bn2.num_batches_tracked', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.bn2.num_batches_tracked', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import io\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import models_package  \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# # Function definitions\n",
    "\n",
    "\n",
    "#### without mean\n",
    "# def get_emb_fea(model, dataloader, batch_size):\n",
    "#     # Define the device\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)\n",
    "#     model.eval() \n",
    "#     embeddings = []\n",
    "\n",
    "#     with torch.no_grad(): \n",
    "#         for data in dataloader:\n",
    "#             inputs, labels = data\n",
    "#             inputs = inputs.to(device)\n",
    "\n",
    "#             output = model(inputs)\n",
    "#             if isinstance(output, tuple):\n",
    "#                 output = output[0]\n",
    "\n",
    "#             embeddings.append(output.cpu().numpy())\n",
    "\n",
    "#     embeddings = np.concatenate(embeddings, axis=0).tolist() \n",
    "#     return embeddings\n",
    "\n",
    "\n",
    "#### with mean\n",
    "def get_emb_fea(model, dataloader, batch_size):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "    class_embeddings = {}\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            output = model(inputs)\n",
    "            if isinstance(output, tuple):\n",
    "                output = output[0]\n",
    "\n",
    "            embeddings = output.cpu().numpy()\n",
    "\n",
    "            for emb, label in zip(embeddings, labels):\n",
    "                label = int(label)  \n",
    "                if label not in class_embeddings:\n",
    "                    class_embeddings[label] = []\n",
    "                class_embeddings[label].append(emb)\n",
    "\n",
    "    class_mean_embeddings = {label: np.mean(np.array(embs), axis=0).tolist() \n",
    "                             for label, embs in class_embeddings.items()}\n",
    "\n",
    "    return class_mean_embeddings\n",
    "\n",
    "\n",
    "#### the original function with a small update\n",
    "# def get_emb_fea(model, dataloader, batch_size):\n",
    "#     ''' Used to extract the feature embeddings in a teacher model '''\n",
    "#     model.eval()\n",
    "\n",
    "#     EMB = {}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in dataloader:\n",
    "#             images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "#             # compute output\n",
    "#             emb_fea, logits = model(images, embed=True)\n",
    "\n",
    "#             for emb, i in zip(emb_fea, labels):\n",
    "#                 i = i.item()\n",
    "#                 emb_size = len(emb) \n",
    "#                 if str(i) in EMB:\n",
    "#                     for j in range(emb_size):\n",
    "#                         EMB[str(i)][j].append(round(emb[j].item(), 4))\n",
    "#                 else:\n",
    "#                     EMB[str(i)] = [[] for _ in range(emb_size)]\n",
    "#                     for j in range(emb_size):\n",
    "#                         EMB[str(i)][j].append(round(emb[j].item(), 4))\n",
    "\n",
    "#     for key, value in EMB.items():\n",
    "#         for i in range(emb_size):\n",
    "#             EMB[key][i] = round(np.array(EMB[key][i]).mean(), 4)\n",
    "\n",
    "#     return EMB\n",
    "\n",
    "    \n",
    "\n",
    "def retrieve_teacher_class_weights(model_name, model_weight_path, num_class, data_name, dataloader, batch_size, bucket_name):\n",
    "    ''' Use the extracted feature embeddings to create a json of class means for teacher'''\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.client('s3')\n",
    "\n",
    "    teacher_model_weights_buffer = io.BytesIO()\n",
    "    s3.download_fileobj(bucket_name, model_weight_path, teacher_model_weights_buffer)\n",
    "    teacher_model_weights_buffer.seek(0)  \n",
    "\n",
    "    # Load the model\n",
    "    model = models_package.__dict__[model_name](num_class=num_class)\n",
    "    checkpoint = torch.load(teacher_model_weights_buffer)\n",
    "    print(\"Keys in checkpoint:\", checkpoint.keys())\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in checkpoint.items():\n",
    "        name = k[7:] if k.startswith('module.') else k\n",
    "        new_state_dict[name] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model = model.cuda()\n",
    "\n",
    "    # emb = get_emb_fea(model=model, dataloader=dataloader, batch_size=batch_size)\n",
    "    # emb_json = json.dumps(emb, indent=4)\n",
    "    # with open(\"./class_means/{}_embedding_fea/{}.json\".format(data_name, model_name), 'w', encoding='utf-8') as f:\n",
    "    #     f.write(emb_json)\n",
    "\n",
    "    emb = get_emb_fea(model=model, dataloader=dataloader, batch_size=batch_size)\n",
    "    emb_json = json.dumps(emb, indent=4)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    output_dir = \"./class_means/{}_embedding_fea\".format(data_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(\"{}/{}.json\".format(output_dir, model_name), 'w', encoding='utf-8') as f:\n",
    "        f.write(emb_json)\n",
    "\n",
    "# Calling the function\n",
    "model_name = 'resnet32x4_idenprof'\n",
    "model_weight_path = 'weights/idenprof_teacher_resnet32x4_weights.pth'\n",
    "num_class = 10\n",
    "data_name = 'idenprof'  \n",
    "batch_size = 0  \n",
    "bucket_name = '210bucket'  \n",
    "\n",
    "retrieve_teacher_class_weights(model_name, model_weight_path, num_class, data_name, testloader, batch_size, bucket_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4da13-d912-47e5-bef7-c0f571b343e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d06a397a-ad4f-4e61-be3f-708bfe76090b",
   "metadata": {},
   "source": [
    "## LB Help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a9087e-3273-4301-8ea4-4e6d7fd914b2",
   "metadata": {},
   "source": [
    "1. loading in model weights and idenprof dataset from s3 (LB will set up) __COMPLETE__\n",
    "2. save model weights to s3 bucket (moving forward) __COMPLETE__\n",
    "3. Teachers: help running resnet-34x2 (LB) -- needs to be trained on idenprof __COMPLETE__\n",
    "4. Make sure that resnet-8x4 is running __COMPLETE__\n",
    "5. Student: shufflenet-v1  -- just need to make them run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62b4c54-509b-4a69-afa5-bb4e1279e337",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
