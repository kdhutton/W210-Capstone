{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed182ac-bc78-4d44-b703-561a052cfc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "import warnings\n",
    "import torchvision\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.models as models\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances_argmin_min, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "from torchvision.models import EfficientNet\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7437867-ecd5-4643-9fb8-a2fc1d1f15f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0005 # 0.096779\n",
    "epochs = 200\n",
    "epochs_pretrain = 3\n",
    "epochs_optimal_lr = 3\n",
    "patience = 6\n",
    "momentum = 0.9\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "\n",
    "# list of lambda values to loop through for grid search\n",
    "epsilon = 0.05\n",
    "margin = 0.01\n",
    "lambda_factor_list = [0,50,100,150,200]\n",
    "\n",
    "# labels used including for plotting\n",
    "class_labels = [0, 1, 3, 4, 6, 7, 11, 15, 17, 18, 19, 20, 22, 25, 27, 28, 30, 31, 33, 35, 36, 37, 39, 43, 44, 50, 51, 54, 57, 58]\n",
    "class_labels_new = torch.tensor([i for i in range(len(class_labels))])\n",
    "num_classes = 16\n",
    "class_names_new = [f\"Class {label}\" for label in range(num_classes)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11b671f-0b71-438a-bf75-7ceb293b72ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Number of GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# set device to cuda if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Count the number of GPUs available\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "print(\"CUDA Available:\", cuda_available)\n",
    "print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eabaae7-724f-4ee9-9df8-9463d205b5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your JSON file is named 'your_file.json'\n",
    "file_path = './WIDER/Annotations/wider_attribute_trainval.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    # Load the JSON data from the file\n",
    "    data = json.load(file)\n",
    "\n",
    "class_idx = data['scene_id_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c142a7d7-9627-48b6-a5aa-86527150ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label_mapping = {\n",
    "    0: \"Team_Sports\",\n",
    "    1: \"Celebration\",\n",
    "    2: \"Parade\",\n",
    "    3: \"Waiter_Or_Waitress\",\n",
    "    4: \"Individual_Sports\",\n",
    "    5: \"Surgeons\",\n",
    "    6: \"Spa\",\n",
    "    7: \"Law_Enforcement\",\n",
    "    8: \"Business\",\n",
    "    9: \"Dresses\",\n",
    "    10: \"Water_Activities\",\n",
    "    11: \"Picnic\",\n",
    "    12: \"Rescue\",\n",
    "    13: \"Cheering\",\n",
    "    14: \"Performance_And_Entertainment\",\n",
    "    15: \"Family\"\n",
    "}\n",
    "\n",
    "# Ensure that all 16 new classes are covered\n",
    "# If some classes are not explicitly mentioned in new_label_mapping, add them\n",
    "for i in range(num_classes):\n",
    "    if i not in new_label_mapping:\n",
    "        new_label_mapping[i] = \"Additional Category {}\".format(i)\n",
    "\n",
    "class_idx = new_label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cab11dc9-d6c6-4c45-8452-cff149af8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, ann_files, augs, img_size, dataset):\n",
    "\n",
    "        # Create a mapping from old labels to new labels\n",
    "        self.label_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted(class_labels))}\n",
    "\n",
    "        self.new_label_mapping = {\n",
    "            0: 2,  # Parade\n",
    "            1: 8,  # Business\n",
    "            2: 7,  # Law Enforcement\n",
    "            3: 14,  # Performance and Entertainment\n",
    "            4: 1,  # Celebration\n",
    "            5: 13,  # Cheering\n",
    "            6: 8,  # Business\n",
    "            7: 8,  # Business\n",
    "            8: 1,  # Celebration\n",
    "            9: 14,  # Performance and Entertainment\n",
    "            10: 15, # Family\n",
    "            11: 15, # Family\n",
    "            12: 11, # Picnic\n",
    "            13: 7, # Law Enforcement\n",
    "            14: 6, # Spa\n",
    "            15: 13, # Cheering\n",
    "            16: 5, # Surgeons\n",
    "            17: 3, # Waiter or Waitress\n",
    "            18: 4, # Individual Sports\n",
    "            19: 0, # Team Sports\n",
    "            20: 0, # Team Sports\n",
    "            21: 0, # Team Sports\n",
    "            22: 4, # Individual Sports\n",
    "            23: 10, # Water Activities\n",
    "            24: 4, # Individual Sports\n",
    "            25: 1, # Celebration\n",
    "            26: 9, # Dresses\n",
    "            27: 12, # Rescue\n",
    "            28: 10,# Water Activities\n",
    "            29: 0  # Team Sports\n",
    "        }\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.ann_files = ann_files\n",
    "        self.augment = self.augs_function(augs, img_size)\n",
    "        # Initialize transformations directly\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "            ] \n",
    "        )\n",
    "        if self.dataset == \"wider\":\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                ] \n",
    "            )        \n",
    "\n",
    "        self.anns = []\n",
    "        self.load_anns()\n",
    "        print(self.augment)\n",
    "\n",
    "    def augs_function(self, augs, img_size):            \n",
    "        t = []\n",
    "        if 'randomflip' in augs:\n",
    "            t.append(transforms.RandomHorizontalFlip())\n",
    "        if 'ColorJitter' in augs:\n",
    "            t.append(transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0))\n",
    "        if 'resizedcrop' in augs:\n",
    "            t.append(transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0)))\n",
    "        if 'RandAugment' in augs:\n",
    "            t.append(transforms.RandAugment())\n",
    "\n",
    "        t.append(transforms.Resize((img_size, img_size)))\n",
    "\n",
    "        return transforms.Compose(t)\n",
    "    \n",
    "    def load_anns(self):\n",
    "        self.anns = []\n",
    "        for ann_file in self.ann_files:\n",
    "            json_data = json.load(open(ann_file, \"r\"))\n",
    "            self.anns += json_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Make sure the index is within bounds\n",
    "        idx = idx % len(self)\n",
    "        ann = self.anns[idx]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to open the image file\n",
    "            img = Image.open(f'WIDER/Image/{ann[\"file_name\"]}').convert(\"RGB\")\n",
    "\n",
    "            # If this is the wider dataset, proceed with specific processing\n",
    "            # x, y, w, h = ann['bbox']\n",
    "            # img_area = img.crop([x, y, x+w, y+h])\n",
    "            img_area = self.augment(img)\n",
    "            img_area = self.transform(img_area)\n",
    "            attributes_list = [target['attribute'] for target in ann['targets']]\n",
    "            num_people = len(attributes_list)\n",
    "            attributes_distribution = [max(sum(attribute), 0)/num_people for attribute in zip(*attributes_list)]\n",
    "            # Extract label from image path\n",
    "            img_path = f'WIDER/Image/{ann[\"file_name\"]}'\n",
    "            label = self.extract_label(img_path)  # You might need to implement this method\n",
    "            \n",
    "            return {\n",
    "                \"label\": label,\n",
    "                \"target\": torch.tensor([attributes_distribution[0]], dtype=torch.float32),\n",
    "                \"img\": img_area\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            # If any error occurs during the processing of an image, log the error and the index\n",
    "            print(f\"Error processing image at index {idx}: {e}\")\n",
    "            # Instead of returning None, raise the exception\n",
    "            raise\n",
    "\n",
    "    def extract_label(self, img_path):\n",
    "        original_label = None\n",
    "    \n",
    "        if \"WIDER/Image/train\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/train/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/test\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/test/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/val\" in img_path:  # Handle validation images\n",
    "            label_str = img_path.split(\"WIDER/Image/val/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "    \n",
    "        if original_label is not None:\n",
    "            remapped_label = self.label_mapping[original_label]\n",
    "            new_label_mapping = self.new_label_mapping[remapped_label]\n",
    "            return new_label_mapping\n",
    "        else:\n",
    "            raise ValueError(f\"Label could not be extracted from path: {img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742c75bb-56ba-497e-8160-745d070907fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = ['data/wider/trainval_wider.json']\n",
    "test_file = ['data/wider/test_wider.json']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21c07e8c-9d6a-4f00-9b72-dbb9f35f6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # Filter out any None items in the batch\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    # If after filtering the batch is empty, handle this case by either returning an empty tensor or raising an exception\n",
    "    if len(batch) == 0:\n",
    "        raise ValueError(\"Batch is empty after filtering out None items.\")\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e188300a-7896-4ed7-ba20-549a3057fbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = DataSet(train_file, augs = ['RandAugment'], img_size = 226, dataset = 'wider')\n",
    "train_dataset = DataSet(train_file, augs = [], img_size = 226, dataset = 'wider')\n",
    "test_dataset = DataSet(test_file, augs = [], img_size = 226, dataset = 'wider')\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                         num_workers=num_workers, collate_fn=custom_collate)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c464baa9-1e78-4409-8133-cbbc815ace9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_hot_encode(labels, num_classes):\n",
    "#     return np.eye(num_classes)[labels]\n",
    "\n",
    "# def calculate_recall_multiclass(conf_matrix):\n",
    "#     if conf_matrix.size == 0:\n",
    "#         return np.array([0])  # Return an array with a single zero if the confusion matrix is empty\n",
    "\n",
    "#     recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "#     recalls[np.isnan(recalls)] = 0  # Replace NaN with 0\n",
    "#     return recalls  # This will always be an array\n",
    "\n",
    "# def evaluate_model_with_gender_multiclass(pred, label, gender, num_classes):\n",
    "#     predictions = pred.cpu().numpy()\n",
    "#     true_labels = label.cpu().numpy()\n",
    "#     gender = gender.cpu().numpy()\n",
    "\n",
    "#     class_disparities = []  # This is the correct variable name\n",
    "#     batch_biases = []  # Store batch biases\n",
    "\n",
    "#     for cls in range(num_classes):\n",
    "#         male_indices = np.where((gender >= 0.5) & (true_labels == cls))[0]\n",
    "#         female_indices = np.where((gender < 0.5) & (true_labels == cls))[0]\n",
    "\n",
    "#         if len(male_indices) > 0 and len(female_indices) > 0:\n",
    "#             male_pred_cls = predictions[male_indices]\n",
    "#             female_pred_cls = predictions[female_indices]\n",
    "#             male_true_cls = true_labels[male_indices]\n",
    "#             female_true_cls = true_labels[female_indices]\n",
    "\n",
    "#             # Ensure that both male and female true labels contain the class 'cls'\n",
    "#             if cls in male_true_cls and cls in female_true_cls:\n",
    "#                 male_conf_matrix = confusion_matrix(male_true_cls, male_pred_cls, labels=[cls])\n",
    "#                 female_conf_matrix = confusion_matrix(female_true_cls, female_pred_cls, labels=[cls])\n",
    "\n",
    "#                 # if male_conf_matrix.size > 0 and female_conf_matrix.size > 0:\n",
    "#                     male_recall_cls = calculate_recall_multiclass(male_conf_matrix)[0]  # Take first element\n",
    "#                     female_recall_cls = calculate_recall_multiclass(female_conf_matrix)[0]  # Take first element\n",
    "\n",
    "#                     disparity = male_recall_cls - female_recall_cls\n",
    "#                     class_disparities.append(disparity)\n",
    "#                     batch_biases.append(disparity)\n",
    "\n",
    "#     return np.mean(batch_biases) if batch_biases else 0, class_disparities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73b6bb60-dc13-4d86-b5a9-f84b376823dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "def calculate_recall_multiclass(conf_matrix):\n",
    "    recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "    recalls[np.isnan(recalls)] = 0  # Replace NaN with 0\n",
    "    return recalls\n",
    "\n",
    "def evaluate_model_with_gender_multiclass(pred, label, gender, num_classes):\n",
    "    predictions = pred.cpu()\n",
    "    true_labels = label.cpu()\n",
    "    gender = gender.cpu()\n",
    "\n",
    "    # Identify male and female indices based on the gender threshold\n",
    "    male_indices = np.where(gender >= 0.5)[0]\n",
    "    female_indices = np.where(gender < 0.5)[0]\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    one_hot_labels = one_hot_encode(true_labels, num_classes=num_classes)\n",
    "    one_hot_preds = one_hot_encode(predictions, num_classes=num_classes)\n",
    "\n",
    "    # Initialize recall arrays\n",
    "    male_recall = np.zeros(num_classes)\n",
    "    female_recall = np.zeros(num_classes)\n",
    "\n",
    "    # Extract predictions and labels for male and female indices\n",
    "    male_predictions = np.argmax(one_hot_preds[male_indices], axis=1)\n",
    "    female_predictions = np.argmax(one_hot_preds[female_indices], axis=1)\n",
    "    male_labels = np.argmax(one_hot_labels[male_indices], axis=1)\n",
    "    female_labels = np.argmax(one_hot_labels[female_indices], axis=1)\n",
    "\n",
    "    # Check if the class labels are within the expected range\n",
    "    assert (0 <= male_predictions.min() < num_classes) and (0 <= male_predictions.max() < num_classes), \"Invalid class indices in male predictions\"\n",
    "    assert (0 <= female_predictions.min() < num_classes) and (0 <= female_predictions.max() < num_classes), \"Invalid class indices in female predictions\"\n",
    "    assert (0 <= male_labels.min() < num_classes) and (0 <= male_labels.max() < num_classes), \"Invalid class indices in male labels\"\n",
    "    assert (0 <= female_labels.min() < num_classes) and (0 <= female_labels.max() < num_classes), \"Invalid class indices in female labels\"\n",
    "\n",
    "    # Calculate confusion matrices for each gender\n",
    "    male_conf_matrix = confusion_matrix(male_labels, male_predictions, labels=np.arange(num_classes))\n",
    "    female_conf_matrix = confusion_matrix(female_labels, female_predictions, labels=np.arange(num_classes))\n",
    "\n",
    "    # Calculate recall for each class and gender\n",
    "    male_recall[:len(male_conf_matrix)] = calculate_recall_multiclass(male_conf_matrix)\n",
    "    female_recall[:len(female_conf_matrix)] = calculate_recall_multiclass(female_conf_matrix)\n",
    "\n",
    "    return male_recall - female_recall, male_conf_matrix, female_conf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3df5bc74-4a24-42a0-9ca4-a4e04861d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load EfficientNet B0 model\n",
    "actor = models.efficientnet_b0(pretrained=True)\n",
    "\n",
    "# Determine the number of output features from the feature extractor part of EfficientNet B0\n",
    "num_ftrs = actor.classifier[1].in_features  # This is the correct number of input features for your adversarial classifier\n",
    "\n",
    "# Modify the classifier layer of the EfficientNet model to match your number of classes\n",
    "actor.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "# Move the EfficientNet model to the GPU\n",
    "actor = actor.to(device)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=256):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)  # Outputting a single value for bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))  # Assuming bias is a probability-like value\n",
    "        return x\n",
    "\n",
    "# Initialize the Critic model\n",
    "critic = Critic(input_size=16).to(device)  # Adjust the input size based on your model's output\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=learning_rate)\n",
    "critic_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Redefine your main model optimizer if needed\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=learning_rate)\n",
    "actor_loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2777235b-0ae2-47e8-a1a8-562402565000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(actor, actor_optimizer, actor_loss_fn, critic, critic_optimizer, critic_loss_fn,\n",
    "               lambda_factor, epsilon, margin, patience, epochs, device, base_save_dir):\n",
    "    train_accuracies = []\n",
    "    train_disparities = []\n",
    "    train_mean_non_zero_abs_disparities = []\n",
    "    train_losses = []\n",
    "    train_main_losses = []\n",
    "    train_critic_losses = []\n",
    "    val_accuracies = []\n",
    "    val_disparities = []\n",
    "    val_mean_non_zero_abs_disparities = []\n",
    "    val_losses = []\n",
    "    val_main_losses = []\n",
    "    val_critic_losses = []\n",
    "    \n",
    "    patience_counter = 0 \n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = 0\n",
    "    best_val_mean_abs_disparity = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Create a subdirectory for the current lambda_factor\n",
    "    lambda_dir = os.path.join(base_save_dir, f'lambda_{lambda_factor}')\n",
    "    os.makedirs(lambda_dir, exist_ok=True)\n",
    "    \n",
    "    \n",
    "    # Training and Validation Loop\n",
    "    for epoch in range(epochs):\n",
    "        # Initialize metrics for each epoch\n",
    "        epoch_train_disparities = []\n",
    "        epoch_train_losses = []\n",
    "        epoch_train_accuracies = []\n",
    "        epoch_val_disparities = []\n",
    "        epoch_val_losses = []\n",
    "        epoch_val_accuracies = []\n",
    "    \n",
    "        confusion_male = np.zeros((num_classes, num_classes))\n",
    "        confusion_female = np.zeros((num_classes, num_classes))\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_val_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Training\n",
    "        actor.train()\n",
    "        for batch_data in tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}, Training'):\n",
    "            # Load data to device\n",
    "            images = batch_data[\"img\"].to(device)\n",
    "            labels = batch_data[\"label\"].to(device)\n",
    "            gender_scores = batch_data[\"target\"].to(device)\n",
    "    \n",
    "            # Forward pass through actor\n",
    "            actor_output = actor(images)\n",
    "            class_predictions = torch.argmax(actor_output, dim=1)\n",
    "    \n",
    "            # Compute bias\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (class_predictions == labels).sum().item()\n",
    "            num_batches += 1\n",
    "            recall_diff = evaluate_model_with_gender_multiclass(class_predictions, labels, gender_scores, num_classes=num_classes)\n",
    "            confusion_male += recall_diff[1]\n",
    "            confusion_female += recall_diff[2]\n",
    "            bias = np.mean(recall_diff[0])\n",
    "            bias_mean = torch.tensor([bias], device=device, dtype=torch.float32)\n",
    "    \n",
    "            critic_optimizer.zero_grad()\n",
    "            \n",
    "            for param in critic.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in actor.parameters():\n",
    "                param.requires_grad = False\n",
    "                \n",
    "            critic.train()\n",
    "            actor.eval()\n",
    "            \n",
    "            critic_output = critic(actor_output)\n",
    "            critic_loss = critic_loss_fn(critic_output, bias_mean)\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "    \n",
    "            critic_optimizer.step()\n",
    "    \n",
    "            for param in critic.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in actor.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "            actor.train()\n",
    "            critic.eval()\n",
    "    \n",
    "            actor_optimizer.zero_grad()\n",
    "    \n",
    "            critic_output = critic(actor_output)\n",
    "            main_loss = actor_loss_fn(actor_output, labels)\n",
    "    \n",
    "            combined_loss = max(1, lambda_factor * (abs(critic_output[0][0]) - epsilon + margin) + 1) * main_loss\n",
    "    \n",
    "            combined_loss.backward(retain_graph=True)\n",
    "            actor_optimizer.step()\n",
    "    \n",
    "            # Calculate and accumulate metrics\n",
    "            accuracy = (class_predictions == labels).float().mean().item()\n",
    "            epoch_train_accuracies.append(accuracy)\n",
    "            epoch_train_disparities.append(bias)\n",
    "        \n",
    "            # Record the losses\n",
    "            epoch_train_losses.append((combined_loss.item(), main_loss.item(), critic_loss.item()))\n",
    "    \n",
    "        confusion_male /= num_batches\n",
    "        confusion_female /= num_batches\n",
    "    \n",
    "        # Calculate training metrics for the epoch\n",
    "        train_epoch_disparity = calculate_recall_multiclass(confusion_male) - calculate_recall_multiclass(confusion_female)\n",
    "        train_non_zero_abs_values = np.abs(train_epoch_disparity[train_epoch_disparity != 0])\n",
    "        \n",
    "        # Store average training metrics for the epoch\n",
    "        train_accuracy = np.mean(epoch_train_accuracies)\n",
    "        train_disparity = np.mean(epoch_train_disparities)\n",
    "        train_mean_non_zero_abs_disparity = np.mean(train_non_zero_abs_values)\n",
    "        train_combined_loss = np.mean([x[0] for x in epoch_train_losses])\n",
    "        train_main_loss = np.mean([x[1] for x in epoch_train_losses])\n",
    "        train_critic_loss = np.mean([x[2] for x in epoch_train_losses])\n",
    "    \n",
    "        train_accuracies.append(train_accuracy)\n",
    "        train_disparities.append(train_disparity)\n",
    "        train_mean_non_zero_abs_disparities.append(train_mean_non_zero_abs_disparity)\n",
    "        train_losses.append(train_combined_loss)\n",
    "        train_main_losses.append(train_main_loss)\n",
    "        train_critic_losses.append(train_critic_loss)\n",
    "    \n",
    "        # Validation Phase\n",
    "        confusion_male = np.zeros((num_classes, num_classes))\n",
    "        confusion_female = np.zeros((num_classes, num_classes))\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        actor.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch_data in tqdm(testloader, desc=f'Epoch {epoch+1}/{epochs}, Validation'):\n",
    "                # Load data to device\n",
    "                images = batch_data[\"img\"].to(device)\n",
    "                labels = batch_data[\"label\"].to(device)\n",
    "                gender_scores = batch_data[\"target\"].to(device)\n",
    "        \n",
    "                # Forward pass\n",
    "                actor_output = actor(images)\n",
    "                val_critic_output = critic(actor_output)\n",
    "                class_predictions = torch.argmax(actor_output, dim=1)\n",
    "        \n",
    "                # Calculate and accumulate validation metrics\n",
    "                accuracy = (class_predictions == labels).float().mean().item()\n",
    "    \n",
    "                # Compute bias\n",
    "                num_batches += 1\n",
    "                recall_diff = evaluate_model_with_gender_multiclass(class_predictions, labels, gender_scores, num_classes=num_classes)\n",
    "                confusion_male += recall_diff[1]\n",
    "                confusion_female += recall_diff[2]\n",
    "                \n",
    "                # Calculate validation losses (similar to training losses)\n",
    "                batch_bias = np.mean(recall_diff[0])\n",
    "                mean_batch_bias = torch.tensor([batch_bias], device=device, dtype=torch.float32)\n",
    "                val_main_loss = actor_loss_fn(actor_output, labels)\n",
    "                val_critic_loss = critic_loss_fn(val_critic_output, mean_batch_bias)\n",
    "        \n",
    "                val_combined_loss = max(1, lambda_factor * (abs(val_critic_output[0][0]) - epsilon + margin) + 1) * val_main_loss\n",
    "    \n",
    "                epoch_val_accuracies.append(accuracy)\n",
    "                epoch_val_losses.append((val_combined_loss.item(), val_main_loss.item(), val_critic_loss.item()))\n",
    "                \n",
    "            confusion_male /= num_batches\n",
    "            confusion_female /= num_batches\n",
    "    \n",
    "            val_epoch_disparity = calculate_recall_multiclass(confusion_male) - calculate_recall_multiclass(confusion_female)\n",
    "            val_non_zero_abs_values = np.abs(val_epoch_disparity[val_epoch_disparity != 0])\n",
    "    \n",
    "            # Store average training metrics for the epoch\n",
    "            val_accuracy = np.mean(epoch_val_accuracies)\n",
    "            val_disparity = np.mean(epoch_val_disparities)\n",
    "            val_mean_non_zero_abs_disparity = np.mean(val_non_zero_abs_values)\n",
    "            val_combined_loss = np.mean([x[0] for x in epoch_val_losses])\n",
    "            val_main_loss = np.mean([x[1] for x in epoch_val_losses])\n",
    "            val_critic_loss = np.mean([x[2] for x in epoch_val_losses])\n",
    "        \n",
    "            val_accuracies.append(val_accuracy)\n",
    "            val_disparities.append(val_disparity)\n",
    "            val_mean_non_zero_abs_disparities.append(val_mean_non_zero_abs_disparity)\n",
    "            val_losses.append(val_combined_loss)\n",
    "            val_main_losses.append(val_main_loss)\n",
    "            val_critic_losses.append(val_critic_loss)\n",
    "\n",
    "            # Check if current validation combined loss is lower than the best combined loss\n",
    "        if val_combined_loss < best_val_loss:\n",
    "            best_val_loss = val_combined_loss\n",
    "            best_val_accuracy = val_accuracy\n",
    "            best_val_mean_non_zero_abs_disparity = val_mean_non_zero_abs_disparity\n",
    "        \n",
    "            # Create a mapping of class recall disparities\n",
    "            class_recall_mapping = {class_name: val_epoch_disparity[int(class_label)] for class_label, class_name in class_idx.items()}\n",
    "        \n",
    "            best_model_state = {\n",
    "                'epoch': epoch,\n",
    "                'actor_state_dict': actor.state_dict(),\n",
    "                'critic_state_dict': critic.state_dict(),\n",
    "                'optimizer_actor_state_dict': actor_optimizer.state_dict(),\n",
    "                'optimizer_critic_state_dict': critic_optimizer.state_dict(),\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_accuracy': best_val_accuracy,\n",
    "                'best_val_mean_abs_disparity': best_val_mean_non_zero_abs_disparity,\n",
    "                'class_recall_mapping': class_recall_mapping\n",
    "            }\n",
    "            save_path = os.path.join(lambda_dir, f'best_model_lambda_{lambda_factor}.pth')\n",
    "            torch.save(best_model_state, save_path)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Epoch {epoch + 1} Metrics:\")\n",
    "        print(\"-\"*50)\n",
    "        print(f\"TRAINING Accuracy: {train_accuracy:.6f}, VALIDATION Accuracy: {val_accuracy:.6f}\")\n",
    "        print(f\"TRAINING Disparity: {train_mean_non_zero_abs_disparity:.6f}, VALIDATION Disparity: {val_mean_non_zero_abs_disparity:.6f}\")\n",
    "        print(f\"TRAINING Combined Loss: {train_combined_loss:.6f}, VALIDATION Combined Loss: {val_combined_loss:.6f}\")\n",
    "        print(\"-\"*50 + \"\\n\")\n",
    "        # Print disparities by class label\n",
    "        for class_label, recall_diff in class_recall_mapping.items():\n",
    "            print(f\"Class {class_label}: Val Disparity = {recall_diff:.6f}\")\n",
    "        print(\"=\"*50 + \"\\n\")\n",
    "      \n",
    "        # Plotting\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot Training and Validation Accuracy\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(train_accuracies, label='Training Accuracy')\n",
    "        plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot Training and Validation Disparity\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(train_mean_non_zero_abs_disparities, label='Training Mean Absolute Disparity')\n",
    "        plt.plot(val_mean_non_zero_abs_disparities, label='Validation Mean Absolute Disparity')\n",
    "        plt.title('Training and Validation Disparity')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Disparity')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Plot Training Loss Including Components\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(train_losses, label='Training Combined Loss')\n",
    "        plt.plot(train_main_losses, label='Training Main Loss')\n",
    "        plt.plot(train_critic_losses, label='Training Critic Loss')\n",
    "        plt.plot(val_losses, label='Validation Combined Loss')\n",
    "        plt.plot(val_main_losses, label='Validation Main Loss')\n",
    "        plt.plot(val_critic_losses, label='Validation Critic Loss')\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    best_epoch = best_model_state['epoch'] + 1 if best_model_state else epochs\n",
    "    print(f\"Finished Training with lambda value of {lambda_factor}. Best (stopping) epoch number: {best_epoch}\")\n",
    "\n",
    "    return best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a59ae3e-ff0e-4b4b-a054-183eafd2e269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Training:   4%|███▉                                                                                       | 7/162 [00:06<02:21,  1.09it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Loop through the lambda_factor_list\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lambda_factor \u001b[38;5;129;01min\u001b[39;00m lambda_factor_list:\n\u001b[0;32m----> 7\u001b[0m     best_model_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactor_loss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcritic_loss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mlambda_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmargin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_save_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_save_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     all_best_model_states[lambda_factor] \u001b[38;5;241m=\u001b[39m best_model_state\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Save the collective best model states to a file\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 98\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(actor, actor_optimizer, actor_loss_fn, critic, critic_optimizer, critic_loss_fn, lambda_factor, epsilon, margin, patience, epochs, device, base_save_dir)\u001b[0m\n\u001b[1;32m     94\u001b[0m main_loss \u001b[38;5;241m=\u001b[39m actor_loss_fn(actor_output, labels)\n\u001b[1;32m     96\u001b[0m combined_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, lambda_factor \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mabs\u001b[39m(critic_output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m-\u001b[39m epsilon \u001b[38;5;241m+\u001b[39m margin) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m main_loss\n\u001b[0;32m---> 98\u001b[0m \u001b[43mcombined_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m actor_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Calculate and accumulate metrics\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:433\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_tensor_str\u001b[38;5;241m.\u001b[39m_str(\u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    435\u001b[0m ):\n\u001b[1;32m    436\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m            used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create dict to store best model states\n",
    "all_best_model_states = {}\n",
    "base_save_dir = \"Test_Dir\"\n",
    "\n",
    "# Loop through the lambda_factor_list\n",
    "for lambda_factor in lambda_factor_list:\n",
    "    best_model_state = train_model(actor, actor_optimizer, actor_loss_fn, critic, critic_optimizer, critic_loss_fn,\n",
    "                                   lambda_factor, epsilon, margin, patience, epochs, device, base_save_dir=base_save_dir)\n",
    "    all_best_model_states[lambda_factor] = best_model_state\n",
    "\n",
    "# Save the collective best model states to a file\n",
    "collective_save_path = os.path.join(base_save_dir, 'all_best_model_states.pth')\n",
    "torch.save(all_best_model_states, collective_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d96960-7dec-414b-a22d-2044772c2901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "# load models\n",
    "collective_save_path = os.path.join(base_save_dir, 'all_best_model_states.pth')\n",
    "\n",
    "# Load the saved model states\n",
    "all_best_model_states = torch.load(collective_save_path)\n",
    "\n",
    "#Ex:\n",
    "lambda_value = 0\n",
    "if lambda_value in all_best_model_states:\n",
    "    best_model_state = all_best_model_states[lambda_value]\n",
    "    best_val_accuracy = best_model_state['best_val_accuracy']\n",
    "    print(f\"Best validation accuracy for lambda {lambda_value}: {best_val_accuracy}\")\n",
    "else:\n",
    "    print(f\"No model state found for lambda {lambda_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f8d2be-db17-4f32-ac35-594f77f550ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR REFERENCE - https://github.com/abacusai/intraprocessing_debiasing/blob/main/algorithms/adversarial.py\n",
    "# \n",
    "# \"\"\"\n",
    "# Adversarial Intraprocessing Algorithm.\n",
    "# \"\"\"\n",
    "# import logging\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# from models import load_model, Critic\n",
    "# from utils import get_best_thresh, get_test_objective, get_valid_objective, compute_bias\n",
    "\n",
    "# logger = logging.getLogger(\"Debiasing\")\n",
    "\n",
    "\n",
    "# def adversarial_debiasing(model_state_dict, data, config, device):\n",
    "#     logger.info('Training Adversarial model.')\n",
    "#     actor = load_model(data.num_features, config.get('hyperparameters', {}))\n",
    "#     actor.load_state_dict(model_state_dict)\n",
    "#     actor.to(device)\n",
    "#     hid = config['hyperparameters']['hid'] if 'hyperparameters' in config else 32\n",
    "#     critic = Critic(hid * config['adversarial']['batch_size'], num_deep=config['adversarial']['num_deep'], hid=hid)\n",
    "#     critic.to(device)\n",
    "#     critic_optimizer = optim.Adam(critic.parameters())\n",
    "#     critic_loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "#     actor_optimizer = optim.Adam(actor.parameters(), lr=config['adversarial']['lr'])\n",
    "#     actor_loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "#     for epoch in range(config['adversarial']['epochs']):\n",
    "#         for param in critic.parameters():\n",
    "#             param.requires_grad = True\n",
    "#         for param in actor.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         actor.eval()\n",
    "#         critic.train()\n",
    "#         for step in range(config['adversarial']['critic_steps']):\n",
    "#             critic_optimizer.zero_grad()\n",
    "#             indices = torch.randint(0, data.X_valid.size(0), (config['adversarial']['batch_size'],))\n",
    "#             cX_valid = data.X_valid_gpu[indices]\n",
    "#             cy_valid = data.y_valid[indices]\n",
    "#             cp_valid = data.p_valid[indices]\n",
    "#             with torch.no_grad():\n",
    "#                 scores = actor(cX_valid)[:, 0].reshape(-1).cpu().numpy()\n",
    "\n",
    "#             bias = compute_bias(scores, cy_valid.numpy(), cp_valid, config['metric'])\n",
    "\n",
    "#             res = critic(actor.trunc_forward(cX_valid))\n",
    "#             loss = critic_loss_fn(torch.tensor([bias], device=device), res[0])\n",
    "#             loss.backward()\n",
    "#             train_loss = loss.item()\n",
    "#             critic_optimizer.step()\n",
    "#             if (epoch % 10 == 0) and (step % 100 == 0):\n",
    "#                 logger.info(f'=======> Critic Epoch: {(epoch, step)} loss: {train_loss}')\n",
    "\n",
    "#         for param in critic.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         for param in actor.parameters():\n",
    "#             param.requires_grad = True\n",
    "#         actor.train()\n",
    "#         critic.eval()\n",
    "#         for step in range(config['adversarial']['actor_steps']):\n",
    "#             actor_optimizer.zero_grad()\n",
    "#             indices = torch.randint(0, data.X_valid.size(0), (config['adversarial']['batch_size'],))\n",
    "#             cy_valid = data.y_valid_gpu[indices]\n",
    "#             cX_valid = data.X_valid_gpu[indices]\n",
    "\n",
    "#             pred_bias = critic(actor.trunc_forward(cX_valid)) # outputs of adversary model\n",
    "#             bceloss = actor_loss_fn(actor(cX_valid)[:, 0], cy_valid)\n",
    "\n",
    "#             # loss = lam*abs(pred_bias) + (1-lam)*loss   \n",
    "\n",
    "#             # max(1, lambda * abs(outputs of adversary model) - epsilon (hypderparam) + margin + 1)\n",
    "#             # epsilon = target bias (0.09?)\n",
    "#             # margin = margin of error\n",
    "#             objloss = max(1, config['adversarial']['lambda']*(abs(pred_bias[0][0])-config['objective']['epsilon']+config['adversarial']['margin'])+1) * bceloss\n",
    "#             # ce-loss instead of BCE\n",
    "#             # dont need to binarize\n",
    "            \n",
    "#             objloss.backward()\n",
    "#             train_loss = objloss.item()\n",
    "#             actor_optimizer.step()\n",
    "#             if (epoch % 10 == 0) and (step % 100 == 0):\n",
    "#                 logger.info(f'=======> Actor Epoch: {(epoch, step)} loss: {train_loss}')\n",
    "\n",
    "#         if epoch % 10 == 0:\n",
    "#             with torch.no_grad():\n",
    "#                 scores = actor(data.X_valid_gpu)[:, 0].reshape(-1, 1).cpu().numpy()\n",
    "#                 _, best_adv_obj = get_best_thresh(scores, np.linspace(0, 1, 1001), data, config, valid=False, margin=config['adversarial']['margin'])\n",
    "#                 logger.info(f'Objective: {best_adv_obj}')\n",
    "\n",
    "#     logger.info('Finding optimal threshold for Adversarial model.')\n",
    "#     with torch.no_grad():\n",
    "#         scores = actor(data.X_valid_gpu)[:, 0].reshape(-1, 1).cpu().numpy()\n",
    "\n",
    "#     best_adv_thresh, _ = get_best_thresh(scores, np.linspace(0, 1, 1001), data, config, valid=False, margin=config['adversarial']['margin'])\n",
    "\n",
    "#     logger.info('Evaluating Adversarial model on best threshold.')\n",
    "#     with torch.no_grad():\n",
    "#         labels = (actor(data.X_valid_gpu)[:, 0] > best_adv_thresh).reshape(-1, 1).cpu().numpy()\n",
    "#     results_valid = get_valid_objective(labels, data, config)\n",
    "#     logger.info(f'Results: {results_valid}')\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         labels = (actor(data.X_test_gpu)[:, 0] > best_adv_thresh).reshape(-1, 1).cpu().numpy()\n",
    "#     results_test = get_test_objective(labels, data, config)\n",
    "\n",
    "#     # return results_valid, results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4836d70b-6686-4e3a-a9e6-e5266f7227ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53514c47-79ca-481f-a3d9-8b9f3abffbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
