{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f79206-feac-4302-abb1-d699b6354a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck, ResNet18_Weights, ResNet34_Weights, resnet18\n",
    "from torchvision.datasets import ImageFolder\n",
    "from utils.loss_functions import tkd_kdloss\n",
    "\n",
    "from models_package.models import Teacher, Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3c0ec8-98dc-4933-b9a3-e96fce39c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001379 # 0.096779\n",
    "num_epochs = 2 # 200\n",
    "temperature = 4.0\n",
    "alpha = 0.9\n",
    "momentum = 0.9\n",
    "num_classes = 30\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "patience = 7  # for early stopping\n",
    "lmda = 3\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "class_labels = [0, 1, 3, 4, 6, 7, 11, 15, 17, 18, 19, 20, 22, 25, 27, 28, 30, 31, 33, 35, 36, 37, 39, 43, 44, 50, 51, 54, 57, 58]\n",
    "class_labels_new = torch.tensor([i for i in range(len(class_labels))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2770eac-10b7-47de-ba06-9aa63cd7ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c922f94-bf69-40d5-bc88-1e6fab4b4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedBatchSampler:\n",
    "    \"\"\"Stratified batch sampling\n",
    "    Provides equal representation of target classes in each batch\n",
    "    \"\"\"\n",
    "    def __init__(self, y, batch_size, shuffle=True):\n",
    "        if torch.is_tensor(y):\n",
    "            y = y.numpy()\n",
    "        assert len(y.shape) == 1, 'label array must be 1D'\n",
    "        n_batches = int(len(y) / batch_size)\n",
    "        self.skf = StratifiedKFold(n_splits=n_batches, shuffle=shuffle)\n",
    "        self.X = torch.randn(len(y),1).numpy()\n",
    "        self.y = y\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.skf.random_state = torch.randint(0,int(1e8),size=()).item()\n",
    "        for train_idx, test_idx in self.skf.split(self.X, self.y):\n",
    "            yield test_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c84514-66c9-4543-8448-cafa751730ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, ann_files, augs, img_size, dataset):\n",
    "        # Define the original class labels\n",
    "\n",
    "        # Create a mapping from old labels to new labels\n",
    "        self.label_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted(class_labels))}\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.ann_files = ann_files\n",
    "        self.augment = self.augs_function(augs, img_size)\n",
    "        # Initialize transformations directly\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "            ] \n",
    "        )\n",
    "        if self.dataset == \"wider\":\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                ] \n",
    "            )        \n",
    "\n",
    "        self.anns = []\n",
    "        self.load_anns()\n",
    "        print(self.augment)\n",
    "\n",
    "    def augs_function(self, augs, img_size):            \n",
    "        t = []\n",
    "        if 'randomflip' in augs:\n",
    "            t.append(transforms.RandomHorizontalFlip())\n",
    "        if 'ColorJitter' in augs:\n",
    "            t.append(transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0))\n",
    "        if 'resizedcrop' in augs:\n",
    "            t.append(transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0)))\n",
    "        if 'RandAugment' in augs:\n",
    "            t.append(transforms.RandAugment())\n",
    "\n",
    "        t.append(transforms.Resize((img_size, img_size)))\n",
    "\n",
    "        return transforms.Compose(t)\n",
    "    \n",
    "    def load_anns(self):\n",
    "        self.anns = []\n",
    "        for ann_file in self.ann_files:\n",
    "            json_data = json.load(open(ann_file, \"r\"))\n",
    "            self.anns += json_data['images']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Make sure the index is within bounds\n",
    "        idx = idx % len(self)\n",
    "        ann = self.anns[idx]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to open the image file\n",
    "            img = Image.open(f'WIDER/Image/{ann[\"file_name\"]}').convert(\"RGB\")\n",
    "\n",
    "            # If this is the wider dataset, proceed with specific processing\n",
    "            # x, y, w, h = ann['bbox']\n",
    "            # img_area = img.crop([x, y, x+w, y+h])\n",
    "            img_area = self.augment(img)\n",
    "            img_area = self.transform(img_area)\n",
    "            attributes_list = [target['attribute'] for target in ann['targets']]\n",
    "            num_people = len(attributes_list)\n",
    "            attributes_distribution = [max(sum(attribute), 0)/num_people for attribute in zip(*attributes_list)]\n",
    "            # Extract label from image path\n",
    "            img_path = f'WIDER/Image/{ann[\"file_name\"]}'\n",
    "            label = self.extract_label(img_path)  # You might need to implement this method\n",
    "            \n",
    "            return {\n",
    "                \"label\": label,\n",
    "                \"target\": torch.tensor([attributes_distribution[0]], dtype=torch.float32),\n",
    "                \"img\": img_area\n",
    "            }\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            # If any error occurs during the processing of an image, log the error and the index\n",
    "            print(f\"Error processing image at index {idx}: {e}\")\n",
    "            # Instead of returning None, raise the exception\n",
    "            raise\n",
    "\n",
    "    def extract_label(self, img_path):\n",
    "        original_label = None\n",
    "    \n",
    "        if \"WIDER/Image/train\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/train/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/test\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/test/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/val\" in img_path:  # Handle validation images\n",
    "            label_str = img_path.split(\"WIDER/Image/val/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "    \n",
    "        if original_label is not None:\n",
    "            remapped_label = self.label_mapping[original_label]\n",
    "            return remapped_label\n",
    "        else:\n",
    "            raise ValueError(f\"Label could not be extracted from path: {img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2490c2-046a-4a19-9717-642adbabb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = ['data/wider/trainval_wider.json']\n",
    "test_file = ['data/wider/test_wider.json']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "012d6bd8-61d5-4a20-845b-689234718508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # Filter out any None items in the batch\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    # If after filtering the batch is empty, handle this case by either returning an empty tensor or raising an exception\n",
    "    if len(batch) == 0:\n",
    "        # Option 1: Return a placeholder tensor (adapt the shape to match your data)\n",
    "        # return torch.tensor([]), torch.tensor([])\n",
    "        # Option 2: Raise an exception\n",
    "        raise ValueError(\"Batch is empty after filtering out None items.\")\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe2adb5-687d-4055-a108-a9c041836b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandAugment(num_ops=2, magnitude=9, num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataSet(train_file, augs = ['RandAugment'], img_size = 226, dataset = 'wider')\n",
    "test_dataset = DataSet(test_file, augs = [], img_size = 226, dataset = 'wider')\n",
    "\n",
    "\n",
    "trainloader = DataLoader(train_dataset, \n",
    "                         batch_sampler=StratifiedBatchSampler(torch.tensor([train_dataset[i]['label'] for i in range(len(train_dataset))]), batch_size=batch_size),\n",
    "                         num_workers=num_workers, collate_fn=custom_collate)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ed9413-2d40-49b9-ba36-f1d43cc95ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = DataSet(train_file, augs = ['RandAugment'], img_size = 226, dataset = 'wider')\n",
    "# test_dataset = DataSet(test_file, augs = [], img_size = 226, dataset = 'wider')\n",
    "\n",
    "\n",
    "# trainloader = DataLoader(train_dataset, \n",
    "#                           batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=custom_collate)\n",
    "# testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f42f0ac9-3af2-4990-a8c1-f7f2af96c03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10324\n",
      "3453\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66cdd881-3047-4bab-84c8-9ceaf347b958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'target': tensor([0.6250]),\n",
       " 'img': tensor([[[-0.7137, -0.7308, -0.7308,  ...,  2.0948,  2.0605,  1.7009],\n",
       "          [-0.6965, -0.7650, -0.7308,  ...,  2.0948,  2.0434,  1.5982],\n",
       "          [-0.6794, -0.7822, -0.7479,  ...,  2.0777,  1.9920,  1.5297],\n",
       "          ...,\n",
       "          [-1.3130, -1.1760, -1.2274,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-0.7308, -0.7479, -0.7822,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-0.5082, -0.7137, -0.8507,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "         [[-0.5826, -0.6001, -0.6001,  ...,  2.0609,  2.0259,  1.6583],\n",
       "          [-0.5651, -0.6352, -0.6001,  ...,  2.0434,  2.0259,  1.6232],\n",
       "          [-0.5476, -0.6527, -0.6176,  ...,  2.0259,  1.9909,  1.5707],\n",
       "          ...,\n",
       "          [-1.1429, -1.0028, -1.0553,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-0.4776, -0.4776, -0.4951,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-0.1625, -0.4076, -0.5651,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "         [[-0.4450, -0.4624, -0.4624,  ...,  1.9951,  1.9603,  1.5942],\n",
       "          [-0.4275, -0.4973, -0.4624,  ...,  1.9777,  1.9428,  1.5245],\n",
       "          [-0.4101, -0.5147, -0.4798,  ...,  1.9603,  1.9254,  1.4722],\n",
       "          ...,\n",
       "          [-1.1421, -1.0376, -1.1073,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-0.6715, -0.7238, -0.7587,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-0.5147, -0.6890, -0.7936,  ..., -1.8044, -1.8044, -1.8044]]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f761db95-0936-4f2e-8005-7ed3079237e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def plot_label_frequencies(train_labels, class_names):\n",
    "#     # Count the occurrences of each label in the training set\n",
    "#     train_label_counts = np.bincount(train_labels)\n",
    "\n",
    "#     # Count the occurrences of each label in the test set\n",
    "\n",
    "#     # Create a bar plot\n",
    "#     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#     bar_width = 0.35\n",
    "#     index = np.arange(len(class_names))\n",
    "\n",
    "#     # Plot training set frequencies\n",
    "#     train_bars = ax.bar(index, train_label_counts, bar_width, label='Train Set')\n",
    "\n",
    "#     # Plot test set frequencies\n",
    "\n",
    "#     # Add labels, title, and legend\n",
    "#     ax.set_xlabel('Class')\n",
    "#     ax.set_ylabel('Frequency')\n",
    "#     ax.set_title('Label Frequencies in Train Sets')\n",
    "#     ax.set_xticks(index + bar_width / 2)\n",
    "#     ax.set_xticklabels(class_names)\n",
    "#     ax.legend()\n",
    "\n",
    "#     # Show the plot\n",
    "#     plt.show()\n",
    "\n",
    "# # Extract labels from the data\n",
    "# # train_labels = [train_dataset[i]['label'] for i in range(len(train_dataset))]\n",
    "# test_labels = [test_dataset[i]['label'] for i in range(len(test_dataset))]\n",
    "\n",
    "# # List of class names\n",
    "# class_names = [f'Class {i+1}' for i in range(1, len(set(test_labels)) + 1)]\n",
    "\n",
    "# # Plot label frequencies\n",
    "# plot_label_frequencies(test_labels, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d18d4b-a3c0-4c5b-ad75-e74b6b1b7296",
   "metadata": {},
   "source": [
    "# Start Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49c941e9-954c-4d7b-932e-06d3297c02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    data = next(dataiter)\n",
    "    inputs = data['img']\n",
    "    \n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher(inputs)\n",
    "    teacher_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        student_outputs = student(inputs)\n",
    "    student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n",
    "\n",
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs)\n",
    "            student_outputs = student(inputs)\n",
    "            \n",
    "        teacher_preds = torch.argmax(teacher_outputs, dim=1).cpu().numpy()\n",
    "        student_preds = torch.argmax(student_outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(teacher_preds)\n",
    "        all_student_preds.append(student_preds)\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'all_labels': all_labels,\n",
    "        'all_teacher_preds': all_teacher_preds,\n",
    "        'all_student_preds': all_student_preds\n",
    "    }\n",
    "    \n",
    "def plot_comparison(labels, teacher_values, student_values, title, ylabel):\n",
    "    # Convert parameter count to millions\n",
    "    if 'Parameter Count' in title or 'Parameter Count' in ylabel:\n",
    "        teacher_values = [value / 1e6 for value in teacher_values]\n",
    "        student_values = [value / 1e6 for value in student_values]\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, teacher_values, width, label='Teacher')\n",
    "    rects2 = ax.bar(x + width/2, student_values, width, label='Student')\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c37c867-c1d1-438c-ac22-755904ce121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def one_hot_encode(labels, num_classes):\n",
    "    return np.eye(num_classes)[labels]\n",
    "\n",
    "def calculate_recall_multiclass(conf_matrix):\n",
    "    recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "    recalls[np.isnan(recalls)] = 0  # Replace NaN with 0\n",
    "    return recalls\n",
    "\n",
    "def evaluate_model_with_gender_multiclass(pred, label, gender, num_classes=30):\n",
    "    predictions = pred.cpu()\n",
    "    true_labels = label.cpu()\n",
    "    gender = gender.cpu()\n",
    "\n",
    "    # Identify male and female indices based on the gender threshold\n",
    "    male_indices = np.where(gender >= 0.5)[0]\n",
    "    female_indices = np.where(gender < 0.5)[0]\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    one_hot_labels = one_hot_encode(true_labels, num_classes=num_classes)\n",
    "    one_hot_preds = one_hot_encode(predictions, num_classes=num_classes)\n",
    "    # Initialize recall arrays\n",
    "    male_recall = np.zeros(num_classes)\n",
    "    female_recall = np.zeros(num_classes)\n",
    "\n",
    "    # Extract predictions and labels for male and female indices\n",
    "    male_predictions = np.argmax(one_hot_preds[male_indices], axis=1)\n",
    "    female_predictions = np.argmax(one_hot_preds[female_indices], axis=1)\n",
    "    male_labels = np.argmax(one_hot_labels[male_indices], axis=1)\n",
    "    female_labels = np.argmax(one_hot_labels[female_indices], axis=1)\n",
    "\n",
    "    # Check if the class labels are within the expected range\n",
    "    assert (0 <= male_predictions.min() < num_classes) and (0 <= male_predictions.max() < num_classes), \"Invalid class indices in male predictions\"\n",
    "    assert (0 <= female_predictions.min() < num_classes) and (0 <= female_predictions.max() < num_classes), \"Invalid class indices in female predictions\"\n",
    "    assert (0 <= male_labels.min() < num_classes) and (0 <= male_labels.max() < num_classes), \"Invalid class indices in male labels\"\n",
    "    assert (0 <= female_labels.min() < num_classes) and (0 <= female_labels.max() < num_classes), \"Invalid class indices in female labels\"\n",
    "\n",
    "    # Calculate confusion matrices for each gender\n",
    "    male_conf_matrix = confusion_matrix(male_labels, male_predictions, labels=np.arange(num_classes))\n",
    "    female_conf_matrix = confusion_matrix(female_labels, female_predictions, labels=np.arange(num_classes))\n",
    "\n",
    "    # Calculate recall for each class and gender\n",
    "    male_recall[:len(male_conf_matrix)] = calculate_recall_multiclass(male_conf_matrix)\n",
    "    female_recall[:len(female_conf_matrix)] = calculate_recall_multiclass(female_conf_matrix)\n",
    "\n",
    "    return male_recall - female_recall, male_conf_matrix, female_conf_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c6b93cb-37c9-448f-a8d9-424d3c4d40fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the models\n",
    "###################### Testing 1 ######################\n",
    "# Create instances of your models\n",
    "# teacher_model = torchvision.models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1).to(device)\n",
    "# teacher_model.fc = nn.Linear(512,30)\n",
    "student_model = torchvision.models.resnet18(weights=None).to(device)\n",
    "student_model.fc = nn.Linear(512,30)\n",
    "\n",
    "\n",
    "# # Load teacher\n",
    "teacher_model = torch.load('teacher_model_ckd_prof.pth')\n",
    "teacher_model.load_state_dict(torch.load('teacher_model_weights_ckd_prof_checkpoint.pth'))\n",
    "# # Load the studnet\n",
    "# student_model = torch.load('student_model_ckd_prof.pth')\n",
    "# student_model.load_state_dict(torch.load('student_model_weights_ckd_prof_checkpoint.pth'))\n",
    "# student_model = student_model.to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "836f2993-0505-47c6-85ee-2ad2f45db946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adversary(nn.Module):\n",
    "    def __init__(self, input_size=30):\n",
    "        super(Adversary, self).__init__()\n",
    "\n",
    "        self.a1 = nn.Linear(input_size, 16)\n",
    "        self.a2 = nn.Linear(16, 1)  # Output size 1 for regression\n",
    "        nn.init.xavier_normal_(self.a1.weight)\n",
    "        nn.init.kaiming_normal_(self.a2.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        adversary = F.relu(self.a1(input_ids))\n",
    "        adversary_output = self.a2(adversary)  # Linear activation for regression\n",
    "        return adversary_output\n",
    "\n",
    "# Instantiate the Adversary\n",
    "adv = Adversary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a761874d-cc14-482e-9c8c-6342adea3cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_student(student, teacher, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs, patience=5):\n",
    "    teacher.eval()\n",
    "    teacher.to(device)\n",
    "    best_val_loss = float('inf')  \n",
    "    patience_counter = 0 \n",
    "    student_epoch_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        student.train()\n",
    "        student.to(device)\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        # epoch_disparity = 0.0\n",
    "        \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student(inputs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "\n",
    "            \n",
    "            ce_loss = criterion(student_outputs, labels)\n",
    "            kd_loss = tkd_kdloss(student_outputs, teacher_outputs, temperature=temperature)  # Make sure this returns a scalar\n",
    "            \n",
    "            # If not scalar, sum up to make sure the loss is scalar\n",
    "            if kd_loss.ndim != 0:\n",
    "                kd_loss = kd_loss.sum()\n",
    "                \n",
    "            # Now combine the losses\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        print(f'*******Epoch {epoch}: loss - {epoch_loss}')\n",
    "        student_epoch_losses.append(epoch_loss)\n",
    "\n",
    "    return student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "005d66e3-f169-4d59-bc29-b5c0e05816b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_adversary(adv, student, adversary_optimizer, trainloader, adv_criterion, epochs):\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_batches = 0\n",
    "    for i, data in enumerate(tqdm(trainloader)): # starting from the 0th batch\n",
    "        # get the inputs and labels\n",
    "        adv.train()\n",
    "        adv.to(device)\n",
    "        inputs = data['img'].to(device)\n",
    "        labels = data['label'].to(device)\n",
    "        targets = data['target'].to(device)\n",
    "        student = student.to(device)\n",
    "        adversary_optimizer.zero_grad()\n",
    "        classifier_prev_output = student(inputs)\n",
    "        adversary_output = adv(classifier_prev_output)\n",
    "        adversary_loss = adv_criterion(adversary_output, targets) # compute loss\n",
    "        adversary_loss.backward() # back prop\n",
    "        adversary_optimizer.step()\n",
    "        epoch_loss += adversary_loss.item()\n",
    "        epoch_batches += 1\n",
    "\n",
    "    print(\"Average Pretrain Adversary epoch loss: \", epoch_loss/epoch_batches)\n",
    "\n",
    "  return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7b4731c-f105-4192-9650-6587c46f6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler for the student model\n",
    "student_optimizer = optim.SGD(student_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "student_scheduler = torch.optim.lr_scheduler.StepLR(student_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Optimizer and scheduler for the teacher model\n",
    "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "teacher_scheduler = torch.optim.lr_scheduler.StepLR(teacher_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "optimizer_adv = optim.Adam(adv.parameters(), lr=learning_rate)\n",
    "\n",
    "# Instantiate the model and the loss function\n",
    "criterion_clf = nn.CrossEntropyLoss()\n",
    "adv_criterion = nn.MSELoss()\n",
    "# print(\"Adversarial Criterion:\", adv_criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98741008-3ae1-4265-9434-7ae0d7dfad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_model = pretrain_student(student_model, teacher_model, trainloader, criterion_clf, student_optimizer, student_scheduler, device, alpha, temperature, num_epochs=2, patience=5)\n",
    "# adv = pretrain_adversary(adv, student_model, optimizer_adv, trainloader, adv_criterion, epochs=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3b7719f-4d65-4601-8e1d-1015de2201cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### finding the optimal learning rate\n",
    "# def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, num_epochs=5, lr_range=(1e-4, 1e-1), plot_loss=True):\n",
    "#     model.train()\n",
    "#     model.to(device)\n",
    "#     lr_values = np.logspace(np.log10(lr_range[0]), np.log10(lr_range[1]), num_epochs * len(trainloader))  # Generate learning rates for each batch\n",
    "#     lr_iter = iter(lr_values)\n",
    "#     losses = []\n",
    "#     lrs = []\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         for i, batch in enumerate(tqdm(trainloader)):\n",
    "#             lr = next(lr_iter)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = lr  # Set new learning rate\n",
    "            \n",
    "#             inputs, labels = batch['img'].to(device), batch['label'].to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             losses.append(loss.item())\n",
    "#             lrs.append(lr)\n",
    "    \n",
    "#     # Calculate the derivative of the loss\n",
    "#     loss_derivative = np.gradient(losses)\n",
    "    \n",
    "#     # Find the learning rate corresponding to the minimum derivative (steepest decline)\n",
    "#     best_lr_index = np.argmin(loss_derivative)\n",
    "#     best_lr = lrs[best_lr_index]\n",
    "    \n",
    "#     if plot_loss:\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.figure()\n",
    "#         plt.plot(lrs, losses)\n",
    "#         plt.xscale('log')\n",
    "#         plt.xlabel('Learning Rate')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title('Learning Rate Range Test')\n",
    "#         plt.axvline(x=best_lr, color='red', linestyle='--', label=f'Best LR: {best_lr}')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "    \n",
    "#     print(f'Best learning rate: {best_lr}')\n",
    "#     return best_lr\n",
    "\n",
    "# ############# input ############## \n",
    "# best_lr = train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs=3)  \n",
    "# print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "411c4f7a-d0b4-478f-9fa5-5e152e08ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(losses):\n",
    "    epochs = range(1, len(losses) + 1)\n",
    "    plt.plot(epochs, losses)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "286386b2-d6e9-447e-95b5-b3dd8f18010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the teacher model\n",
    "def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, epochs, patience=5):\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    epoch_losses = [] \n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "\n",
    "        epoch_loss /= num_batches  \n",
    "        epoch_losses.append(epoch_loss)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_val_loss = 0.0\n",
    "        num_batches = 0  \n",
    "        with torch.no_grad():\n",
    "            for val_data in tqdm(testloader):\n",
    "                val_inputs = val_data['img'].to(device)\n",
    "                val_labels = val_data['label'].to(device)\n",
    "    \n",
    "                # Forward pass for validation\n",
    "                val_outputs = model(val_inputs)\n",
    "    \n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "\n",
    "                total_val_loss += val_loss.item()\n",
    "    \n",
    "                # Compute the validation accuracy\n",
    "                _, predicted = torch.max(val_outputs, 1)\n",
    "                total_samples += val_labels.size(0)\n",
    "                total_correct += (predicted == val_labels).sum().item()\n",
    "                num_batches += 1\n",
    "            total_val_loss /= num_batches\n",
    "            val_losses.append(total_val_loss)\n",
    "            accuracy = total_correct / total_samples\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss: .6f} Val Loss: {total_val_loss: .6f}| Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "            \n",
    "        # Check for early stopping\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            patience_counter = 0 \n",
    "            torch.save(model.state_dict(), f'teacher_model_weights_ckd_prof_checkpoint.pth')\n",
    "            torch.save(model, f'teacher_model_ckd_prof_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break  \n",
    "\n",
    "        scheduler.step()\n",
    "    plot_loss_curve(val_losses)\n",
    "    print(\"Finished Training Teacher\")\n",
    "\n",
    "def train_adversary(adv, student_features, optimizer, trainloader, criterion, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_batches = 0\n",
    "        for i, data in enumerate(tqdm(trainloader)):\n",
    "            # get the inputs and labels\n",
    "            inputs = data['img'].to(device)\n",
    "            targets = data['target'].to(device)\n",
    "            student_features = student.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            classifier_prev_output = student_features(inputs)\n",
    "            adversary_output = adv(classifier_prev_output)\n",
    "\n",
    "            adversary_loss = criterion(adversary_output, targets)\n",
    "            adversary_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += adversary_loss.item()\n",
    "            epoch_batches += 1\n",
    "\n",
    "        average_epoch_loss = epoch_loss / epoch_batches\n",
    "        print(\"Average Adversary epoch loss:\", average_epoch_loss)\n",
    "\n",
    "    return adv\n",
    "# Function to train the student model with knowledge distillation\n",
    "def train_student_with_distillation_disparity(student, teacher, adv, trainloader, testloader, \n",
    "                                              criterion, adv_criterion, optimizer, scheduler, \n",
    "                                              device, alpha, temperature, epochs, lmda, patience=5):\n",
    "    teacher.eval()\n",
    "    teacher.to(device)\n",
    "    num_classes = len(class_labels)\n",
    "    best_val_loss = float('inf')  \n",
    "    patience_counter = 0 \n",
    "    student_epoch_losses = []\n",
    "    val_losses = []\n",
    "    val_disparities = []\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        student.to(device)\n",
    "        adv.train()\n",
    "        adv.to(device)\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0 \n",
    "        confusion_male = np.zeros((num_classes, num_classes))\n",
    "        confusion_female = np.zeros((num_classes, num_classes))\n",
    "\n",
    "        # epoch_disparity = 0.0\n",
    "        \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            targets = data['target'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student(inputs)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "                \n",
    "            studetached = student_outputs.detach()   \n",
    "            adversary_output = adv(studetached)\n",
    "            adversary_loss = adv_criterion(adversary_output, targets)\n",
    "                \n",
    "            ce_loss = criterion(student_outputs, labels)\n",
    "            kd_loss = tkd_kdloss(student_outputs, teacher_outputs, temperature=temperature)  # Make sure this returns a scalar\n",
    "            \n",
    "\n",
    "            if kd_loss.ndim != 0:\n",
    "                kd_loss = kd_loss.sum()\n",
    "\n",
    "            \n",
    "            # Now combine the losses\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss - lmda * adversary_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            # epoch_disparity += disparity\n",
    "\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        # print(f'*******Epoch {epoch}: running_recall_with - {running_recall_with/num_batches}  |  running_recall_without - {running_recall_without/num_batches}  |  disparity - {epoch_disparity/num_batches}******')\n",
    "        student_epoch_losses.append(epoch_loss)\n",
    "\n",
    "        student.eval()\n",
    "        adv.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_val_loss = 0.0\n",
    "        num_batches = 0\n",
    "        # Validation after each epoch\n",
    "        with torch.no_grad():\n",
    "            for val_data in tqdm(testloader):\n",
    "                val_inputs = val_data['img'].to(device)\n",
    "                val_labels = val_data['label'].to(device)\n",
    "                val_targets = val_data['target'].to(device)\n",
    "                \n",
    "                # Forward pass for validation\n",
    "                val_student_outputs = student(val_inputs)\n",
    "                with torch.no_grad():\n",
    "                    val_teacher_outputs = teacher(val_inputs)\n",
    "\n",
    "                val_studetached = val_student_outputs.detach()   \n",
    "\n",
    "                val_adversary_output = adv(val_studetached)\n",
    "                val_adversary_loss = adv_criterion(val_adversary_output, val_targets)\n",
    "                val_ce_loss = criterion(val_student_outputs, val_labels)\n",
    "                val_kd_loss = tkd_kdloss(val_student_outputs, val_teacher_outputs, temperature=temperature)  # Make sure this returns a scalar\n",
    "                \n",
    "                if val_kd_loss.ndim != 0:\n",
    "                    val_kd_loss = val_kd_loss.sum()\n",
    "\n",
    "                \n",
    "                val_loss = alpha * val_kd_loss + (1 - alpha) * val_ce_loss - lmda * val_adversary_loss\n",
    "                total_val_loss += val_loss.item()\n",
    "    \n",
    "                # Compute the validation accuracy\n",
    "                _, predicted = torch.max(val_student_outputs, 1)\n",
    "                total_samples += val_labels.size(0)\n",
    "                total_correct += (predicted == val_labels).sum().item()\n",
    "                num_batches += 1\n",
    "                recall_diff = evaluate_model_with_gender_multiclass(predicted, val_labels, val_targets)\n",
    "                confusion_male += recall_diff[1]\n",
    "                confusion_female += recall_diff[2]\n",
    "    \n",
    "                print(\"Recall Difference per Class:\", recall_diff[0])\n",
    "\n",
    "            total_val_loss /= num_batches\n",
    "            confusion_male /= num_batches\n",
    "            confusion_female /= num_batches\n",
    "\n",
    "            epoch_disparity = calculate_recall_multiclass(confusion_male) - calculate_recall_multiclass(confusion_female)\n",
    "            val_losses.append(total_val_loss)\n",
    "            val_disparities.append(epoch_disparity)\n",
    "            accuracy = total_correct / total_samples\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs},' \n",
    "            f'Train Loss: {epoch_loss: .6f} Val Loss: {total_val_loss: .6f}'\n",
    "            f'Validation Accuracy: {accuracy * 100:.2f}%, Val Disparities: {epoch_disparity}')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            patience_counter = 0 \n",
    "            torch.save(student.state_dict(), f'student_model_weights_ckd_prof_checkpoint.pth')\n",
    "            torch.save(student, f'student_model_ckd_prof_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break  \n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"Finished Training Student\")\n",
    "    plot_loss_curve(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a06902-9ad4-47a6-9a67-181e2dc43218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [01:50<00:00,  2.75s/it]\n",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      "  7%|███▏                                        | 1/14 [00:04<01:03,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      " 14%|██████▎                                     | 2/14 [00:05<00:29,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      " 21%|█████████▍                                  | 3/14 [00:06<00:18,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.0188172 0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.        0.        0.        0.        0.        0.\n",
      " 0.        0.       ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      " 29%|████████████▌                               | 4/14 [00:06<00:12,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      " 36%|███████████████▋                            | 5/14 [00:09<00:16,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      " 43%|██████████████████▊                         | 6/14 [00:10<00:12,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.         -0.12467532  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      " 50%|██████████████████████                      | 7/14 [00:11<00:08,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.02777043 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      " 57%|█████████████████████████▏                  | 8/14 [00:12<00:06,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      " 64%|████████████████████████████▎               | 9/14 [00:14<00:07,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      " 71%|██████████████████████████████▋            | 10/14 [00:15<00:05,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      " 79%|█████████████████████████████████▊         | 11/14 [00:16<00:03,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      " 86%|████████████████████████████████████▊      | 12/14 [00:17<00:02,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      " 93%|███████████████████████████████████████▉   | 13/14 [00:18<00:01,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_50014/1587280395.py:8: RuntimeWarning: invalid value encountered in divide\n",
      "  recalls = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
      "100%|███████████████████████████████████████████| 14/14 [00:18<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Difference per Class: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0.]\n",
      "Epoch 1/2,Train Loss: -0.487656 Val Loss: -0.196542Validation Accuracy: 3.76%, Val Disparities: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.01689708 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.00416667 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▍                                       | 4/40 [00:15<01:34,  2.64s/it]"
     ]
    }
   ],
   "source": [
    "train_student_with_distillation_disparity(student_model, teacher_model, adv, trainloader, \n",
    "                                          testloader, criterion_clf, adv_criterion, student_optimizer, student_scheduler, \n",
    "                                          device, alpha, temperature, num_epochs, lmda=0.25, patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30aad75-7eb3-4a59-b1ac-dc3582cfa65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Testing 1 ######################\n",
    "# Save the student and teacher model weights and architecture\n",
    "# torch.save(student_model.state_dict(), 'student_model_weights_ckd_prof.pth')\n",
    "# torch.save(student_model, 'student_model_ckd_prof.pth')\n",
    "# print('student weights and architecture saved and exported')\n",
    "\n",
    "# torch.save(teacher_model.state_dict(), 'teacher_model_weights_ckd_prof.pth')\n",
    "# torch.save(teacher_model, 'teacher_model_ckd_prof.pth')\n",
    "# print('teacher weights and architecture saved and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995827c-1726-44e2-89aa-5f346a3d4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the comparison and plotting functions after training\n",
    "teacher_params, student_params = compare_model_size(teacher_model, student_model)\n",
    "teacher_time, student_time = compare_inference_time(teacher_model, student_model, testloader)\n",
    "performance_metrics = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "\n",
    "# Extracting the metric values for plotting\n",
    "performance_labels = ['accuracy', 'precision', 'recall', 'f1']\n",
    "teacher_performance_values = [performance_metrics['metrics'][metric][0] for metric in performance_labels]\n",
    "student_performance_values = [performance_metrics['metrics'][metric][1] for metric in performance_labels]\n",
    "\n",
    "# Plotting the comparison for performance metrics\n",
    "plot_comparison(performance_labels, teacher_performance_values, student_performance_values, 'Performance Comparison', 'Score')\n",
    "\n",
    "# Plotting the comparison for model size\n",
    "model_size_labels = ['Model Size']\n",
    "teacher_model_size_values = [teacher_params]\n",
    "student_model_size_values = [student_params]\n",
    "plot_comparison(model_size_labels, teacher_model_size_values, student_model_size_values, 'Model Size Comparison', 'Parameter Count (millions)')\n",
    "\n",
    "# Plotting the comparison for inference time\n",
    "inference_time_labels = ['Inference Time']\n",
    "teacher_inference_time_values = [teacher_time]\n",
    "student_inference_time_values = [student_time]\n",
    "plot_comparison(inference_time_labels, teacher_inference_time_values, student_inference_time_values, 'Inference Time Comparison', 'Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289f549-42fc-4c17-ad60-415775d9d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(preds, targets, condition):\n",
    "    \"\"\"\n",
    "    Calculate recall for a given condition in a multi-class setting.\n",
    "\n",
    "    :param preds: Predicted classes.\n",
    "    :param targets: True classes.\n",
    "    :param condition: Boolean tensor indicating the condition (subset) for which to calculate recall.\n",
    "    :return: Recall value.\n",
    "    \"\"\"\n",
    "    if condition.sum() == 0:  # No samples meet the condition\n",
    "        return 0.0\n",
    "\n",
    "    filtered_preds = preds[condition]\n",
    "    filtered_targets = targets[condition]\n",
    "\n",
    "    true_positive = (filtered_preds == filtered_targets).sum().float()\n",
    "    condition_positive = filtered_targets.size(0)\n",
    "\n",
    "    recall = true_positive / condition_positive if condition_positive > 0 else 0.0\n",
    "    return recall\n",
    "    \n",
    "def calculate_weighted_disparity(disparity_sums, counts):\n",
    "    \"\"\"\n",
    "    Calculate weighted disparity for each class-attribute pair.\n",
    "    \"\"\"\n",
    "    weighted_disparities = torch.zeros_like(disparity_sums)\n",
    "    for class_idx in range(disparity_sums.size(0)):\n",
    "        for attr_idx in range(disparity_sums.size(1)):\n",
    "            if counts[class_idx][attr_idx] > 0:\n",
    "                weighted_disparities[class_idx][attr_idx] = disparity_sums[class_idx][attr_idx] / counts[class_idx][attr_idx]\n",
    "            else:\n",
    "                weighted_disparities[class_idx][attr_idx] = 0.0\n",
    "    return weighted_disparities\n",
    "\n",
    "def evaluate_disparity(model, dataloader, num_classes, device):\n",
    "    \"\"\"\n",
    "    Evaluate the disparity on the test data with weighted consideration.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    disparity_sums = None\n",
    "    counts = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['img'].to(device)\n",
    "            targets = batch['label'].to(device)\n",
    "            attributes = batch['target'].to(device)\n",
    "\n",
    "            if disparity_sums is None:\n",
    "                disparity_sums = torch.zeros(num_classes, attributes.size(1), device=device)\n",
    "                counts = torch.zeros(num_classes, attributes.size(1), device=device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for class_idx in range(num_classes):\n",
    "                for attr_idx in range(attributes.size(1)):\n",
    "                    condition_present = (attributes[:, attr_idx] == 1) & (targets == class_idx)\n",
    "                    condition_absent = (attributes[:, attr_idx] == 0) & (targets == class_idx)\n",
    "\n",
    "                    if condition_present.sum() > 0 or condition_absent.sum() > 0:\n",
    "                        recall_present = calculate_recall(preds, targets, condition_present)\n",
    "                        recall_absent = calculate_recall(preds, targets, condition_absent)\n",
    "\n",
    "                        disparity = abs(recall_present - recall_absent)\n",
    "                        count = condition_present.sum() + condition_absent.sum()\n",
    "                        disparity_sums[class_idx][attr_idx] += disparity * count\n",
    "                        counts[class_idx][attr_idx] += count\n",
    "\n",
    "    weighted_disparities = calculate_weighted_disparity(disparity_sums, counts)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        for attr_idx in range(attributes.size(1)):\n",
    "            print(f\"Class: {class_idx}, Attr: {attr_idx}, Weighted Disparity: {weighted_disparities[class_idx][attr_idx]}\")\n",
    "\n",
    "    weighted_average = weighted_disparities.flatten()\n",
    "    weighted_average = weighted_average.sum()/weighted_average.numel()\n",
    "    return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3dd66-85c6-4a91-9c4d-f20fbc011342",
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity = evaluate_disparity(student_model, testloader, num_classes=num_classes, device=device)\n",
    "print(f'Average recall disparity across all attributes and classes: {disparity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa18cfd-8f83-4c3f-bbe9-c834403adf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_new = [f\"Class {label}\" for label in range(30)]\n",
    "def plot_prediction_distribution_and_confusion_matrix(labels, preds, class_names):\n",
    "    # Plotting the distribution of predictions\n",
    "    sns.countplot(x=preds)\n",
    "    plt.title('Distribution of Predictions')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Computing the confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names_new, columns=class_names_new)\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='g')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report for detailed metrics\n",
    "    print(classification_report(labels, preds, target_names=class_names, zero_division=0))\n",
    "\n",
    "performance_metrics_teacher = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "all_labels = performance_metrics_teacher['all_labels']\n",
    "all_teacher_preds = performance_metrics_teacher['all_teacher_preds']\n",
    "all_student_preds = performance_metrics_teacher['all_student_preds']\n",
    "\n",
    "# For the Teacher Model\n",
    "plot_prediction_distribution_and_confusion_matrix(all_labels, all_teacher_preds, class_names_new)\n",
    "\n",
    "# For the Student Model\n",
    "plot_prediction_distribution_and_confusion_matrix(all_labels, all_student_preds, class_names_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ccf26-26ea-468d-877a-d8c24d8b45a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf4430b-9213-444d-9288-b03b6011266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef97ca7-c21c-4463-8845-1d4cf2e45927",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules=list(student_model.children())[:-1]\n",
    "student_features=nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd9fea-f2c5-469c-aaf1-5a045dfb0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e634a-992d-4ae1-8883-fb8065970cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in student_model.named_parameters():\n",
    "    if \"weight\" in name:  # assuming the last layer has a weight parameter\n",
    "        last_layer_hidden_size = param.size(0)  # The first dimension is the hidden size\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05762185-9db6-46c5-93f1-3eb7e52402ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_layer_hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8f21a-ed8f-42bb-9b91-fce9fe3c1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d76a0f-a3e9-416d-861b-52c733cf9d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
