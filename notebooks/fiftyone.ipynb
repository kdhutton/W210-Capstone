{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c928e2-c18b-457c-8640-f03b4b11b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "import boto3\n",
    "import s3fs\n",
    "import io\n",
    "import time\n",
    "import botocore.exceptions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import torch.nn.functional as F\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, utils, models, datasets\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "from PIL import Image\n",
    "from pycocotools import mask as maskUtils\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f329f00-bc92-4a2c-9f76-9d7a8f3aff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your access:  ········\n",
      "Enter your secret:  ········\n"
     ]
    }
   ],
   "source": [
    "access_key = getpass.getpass(\"Enter your access: \")\n",
    "\n",
    "secret_key = password = getpass.getpass(\"Enter your secret: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04df1e08-51f4-42ee-9459-9f3fa86b1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'w210facetdata'\n",
    "annotations_prefix = 'annotations/'\n",
    "images_prefix = '/home/ubuntu/W210-Capstone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6d45210-7dcb-402d-a7d1-a3719f9a180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(key=access_key, secret=secret_key)\n",
    "\n",
    "# Use s3.open to open the CSV file and read its content into a Pandas DataFrame\n",
    "with s3.open(f's3://{bucket_name}/{annotations_prefix}annotations.csv', 'rb') as file:\n",
    "    gt_df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d50a822-72ce-49e4-9c5e-846f3c2ef96d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dataset name 'FACET14' is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## use relative paths to your image dirs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFACET14\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersistent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m dataset\u001b[38;5;241m.\u001b[39madd_images_dir(images_prefix)\n\u001b[1;32m      4\u001b[0m dataset\u001b[38;5;241m.\u001b[39mcompute_metadata()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/core/singletons.py:36\u001b[0m, in \u001b[0;36mDatasetSingleton.__call__\u001b[0;34m(cls, name, _create, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     _create\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mdeleted\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     34\u001b[0m ):\n\u001b[1;32m     35\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m     \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_create\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     name \u001b[38;5;241m=\u001b[39m instance\u001b[38;5;241m.\u001b[39mname  \u001b[38;5;66;03m# `__init__` may have changed `name`\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/core/dataset.py:270\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, name, persistent, overwrite, _create, _virtual, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m     delete_dataset(name)\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _create:\n\u001b[0;32m--> 270\u001b[0m     doc, sample_doc_cls, frame_doc_cls \u001b[38;5;241m=\u001b[39m \u001b[43m_create_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersistent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersistent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     doc, sample_doc_cls, frame_doc_cls \u001b[38;5;241m=\u001b[39m _load_dataset(\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m, name, virtual\u001b[38;5;241m=\u001b[39m_virtual\n\u001b[1;32m    276\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/core/dataset.py:6657\u001b[0m, in \u001b[0;36m_create_dataset\u001b[0;34m(obj, name, persistent, _patches, _frames, _clips, _src_collection)\u001b[0m\n\u001b[1;32m   6648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_dataset\u001b[39m(\n\u001b[1;32m   6649\u001b[0m     obj,\n\u001b[1;32m   6650\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6655\u001b[0m     _src_collection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   6656\u001b[0m ):\n\u001b[0;32m-> 6657\u001b[0m     slug \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_dataset_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6659\u001b[0m     _id \u001b[38;5;241m=\u001b[39m ObjectId()\n\u001b[1;32m   6661\u001b[0m     sample_collection_name \u001b[38;5;241m=\u001b[39m _make_sample_collection_name(\n\u001b[1;32m   6662\u001b[0m         _id, patches\u001b[38;5;241m=\u001b[39m_patches, frames\u001b[38;5;241m=\u001b[39m_frames, clips\u001b[38;5;241m=\u001b[39m_clips\n\u001b[1;32m   6663\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fiftyone/core/dataset.py:109\u001b[0m, in \u001b[0;36m_validate_dataset_name\u001b[0;34m(name, skip)\u001b[0m\n\u001b[1;32m    107\u001b[0m conn \u001b[38;5;241m=\u001b[39m foo\u001b[38;5;241m.\u001b[39mget_db_conn()\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mlist\u001b[39m(conn\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mfind(query, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m})\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m1\u001b[39m))):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not available\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m slug\n",
      "\u001b[0;31mValueError\u001b[0m: Dataset name 'FACET14' is not available"
     ]
    }
   ],
   "source": [
    "## use relative paths to your image dirs\n",
    "dataset = fo.Dataset(name = \"FACET14\", persistent=True)\n",
    "dataset.add_images_dir(images_prefix)\n",
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c721ef-146f-4f48-a411-7fb93bca9d84",
   "metadata": {},
   "source": [
    "# Object Detection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be99a8b0-a415-4560-a578-2b9f8bba27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOLEAN_PERSONAL_ATTRS = (\n",
    "    \"has_facial_hair\",\n",
    "    \"has_tattoo\",\n",
    "    \"has_cap\",\n",
    "    \"has_mask\",\n",
    "    \"has_headscarf\",\n",
    "    \"has_eyeware\",\n",
    ")\n",
    "def add_boolean_person_attributes(detection, row_index):\n",
    "    for attr in BOOLEAN_PERSONAL_ATTRS:\n",
    "        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "892ad9c7-cb94-49ee-bbe6-caf72e3e7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hairtype(row_index):\n",
    "    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hairtype')]\n",
    "    hairtype = hair_info[hair_info == 1]\n",
    "    if len(hairtype) == 0:\n",
    "        return None\n",
    "    return hairtype.index[0].split('_')[1]\n",
    "\n",
    "def get_haircolor(row_index):\n",
    "    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hair_color')]\n",
    "    haircolor = hair_info[hair_info == 1]\n",
    "    if len(haircolor) == 0:\n",
    "        return None\n",
    "    return haircolor.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8fc5510-2fd8-42f9-a58a-c301286b3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_person_attributes(detection, row_index):\n",
    "    detection[\"hairtype\"] = get_hairtype(row_index)\n",
    "    detection[\"haircolor\"] = get_haircolor(row_index)\n",
    "    add_boolean_person_attributes(detection, row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0091b7eb-d570-4b21-8290-17fee15d226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perceived_gender_presentation(row_index):\n",
    "    gender_info = gt_df.loc[row_index, gt_df.columns.str.startswith('gender')]\n",
    "    pgp = gender_info[gender_info == 1]\n",
    "    if len(pgp) == 0:\n",
    "        return None\n",
    "    return pgp.index[0].replace(\"gender_presentation_\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "def get_perceived_age_presentation(row_index):\n",
    "    age_info = gt_df.loc[row_index, gt_df.columns.str.startswith('age')]\n",
    "    pap = age_info[age_info == 1]\n",
    "    if len(pap) == 0:\n",
    "        return None\n",
    "    return pap.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c49856f0-9c40-44f1-b8dc-afc8a0c80786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skintone(row_index):\n",
    "    skin_info = gt_df.loc[row_index, gt_df.columns.str.startswith('skin_tone')]\n",
    "    return skin_info.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a702e8b-93a2-4e8b-879c-f82be93830ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_protected_attributes(detection, row_index):\n",
    "    detection[\"perceived_age_presentation\"] = get_perceived_age_presentation(row_index)\n",
    "    detection[\"perceived_gender_presentation\"] = get_perceived_gender_presentation(row_index)\n",
    "    detection[\"skin_tone\"] = get_skintone(row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01f3a95d-e491-4de4-be5a-d53de547f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISIBILITY_ATTRS = (\"visible_torso\", \"visible_face\", \"visible_minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc9ed1ed-c71d-4583-963f-bfa23563309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lighting(row_index):\n",
    "    lighting_info = gt_df.loc[row_index, gt_df.columns.str.startswith('lighting')]\n",
    "    lighting = lighting_info[lighting_info == 1]\n",
    "    if len(lighting) == 0:\n",
    "        return None\n",
    "    lighting = lighting.index[0].replace(\"lighting_\", \"\").replace(\"_\", \" \")\n",
    "    return lighting\n",
    "\n",
    "def add_other_attributes(detection, row_index):\n",
    "    detection[\"lighting\"] = get_lighting(row_index)\n",
    "    for attr in VISIBILITY_ATTRS:\n",
    "        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3de3c768-bfa3-4727-95ed-59346a5788a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detection(row_index, sample):\n",
    "    bbox_dict = json.loads(gt_df.loc[row_index, \"bounding_box\"])\n",
    "    x, y, w, h = bbox_dict[\"x\"], bbox_dict[\"y\"], bbox_dict[\"width\"], bbox_dict[\"height\"]\n",
    "    cat1, cat2 = bbox_dict[\"dict_attributes\"][\"cat1\"], bbox_dict[\"dict_attributes\"][\"cat2\"]\n",
    "\n",
    "    person_id = gt_df.loc[row_index, \"person_id\"]\n",
    "\n",
    "    img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "\n",
    "    bounding_box = [x/img_width, y/img_height, w/img_width, h/img_height]\n",
    "    detection = fo.Detection(\n",
    "        label=cat1, \n",
    "        bounding_box=bounding_box,\n",
    "        person_id=person_id,\n",
    "        )\n",
    "    if cat2 != 'none':\n",
    "        detection[\"class2\"] = cat2\n",
    "\n",
    "    add_person_attributes(detection, row_index)\n",
    "    add_protected_attributes(detection, row_index)\n",
    "    add_other_attributes(detection, row_index)\n",
    "\n",
    "    return detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1869986-c9eb-454e-b164-cd5f8b598286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ground_truth_labels(dataset):\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        sample_annos = gt_df[gt_df['filename'] == sample.filename]\n",
    "        detections = []\n",
    "        for row in sample_annos.iterrows():\n",
    "            row_index = row[0]\n",
    "            detection = create_detection(row_index, sample)\n",
    "            detections.append(detection)\n",
    "        sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "    dataset.add_dynamic_sample_fields()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec896ec-9aa5-4033-839d-ced0b1ae54d6",
   "metadata": {},
   "source": [
    "# Add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d068d1d0-38e0-43b7-b827-5006503182d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 31702/31702 [5.5m elapsed, 0s remaining, 93.8 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "## add all of the ground truth labels\n",
    "add_ground_truth_labels(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e53d3-c90d-467e-83c2-6b1bc5717fdd",
   "metadata": {},
   "source": [
    "# Add Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b485560-887e-47f9-b980-a9460a481942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 31702/31702 [26.0m elapsed, 0s remaining, 16.5 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "def add_coco_masks_to_dataset(dataset):\n",
    "    with s3.open(f's3://{bucket_name}/{annotations_prefix}coco_masks.json', 'rb') as file:\n",
    "        coco_masks = json.load(file)\n",
    "    cmas = coco_masks[\"annotations\"]\n",
    "\n",
    "    FILENAME_TO_ID = {\n",
    "        img[\"file_name\"]: img[\"id\"]\n",
    "        for img in coco_masks[\"images\"]\n",
    "    }\n",
    "\n",
    "    CAT_TO_LABEL = {cat[\"id\"]: cat[\"name\"] for cat in coco_masks[\"categories\"]}\n",
    "\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        fn = sample.filename\n",
    "\n",
    "        if fn not in FILENAME_TO_ID:\n",
    "            continue\n",
    "\n",
    "        img_id = FILENAME_TO_ID[fn]\n",
    "        img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "        sample_annos = [a for a in cmas if a[\"image_id\"] == img_id]\n",
    "        if len(sample_annos) == 0:\n",
    "            continue\n",
    "\n",
    "        coco_detections = []\n",
    "        for ann in sample_annos:\n",
    "            label = CAT_TO_LABEL[ann[\"category_id\"]]\n",
    "            bbox = ann['bbox']\n",
    "            ann_id = ann['ann_id']\n",
    "            person_id = ann['facet_person_id']\n",
    "\n",
    "            mask = maskUtils.decode(ann[\"segmentation\"])\n",
    "            mask = Image.fromarray(255*mask)\n",
    "\n",
    "            ## Change bbox to be in the format [x, y, x, y]\n",
    "            bbox[2] = bbox[0] + bbox[2]\n",
    "            bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "            ## Get the cropped image\n",
    "            cropped_mask = np.array(mask.crop(bbox)).astype(bool)\n",
    "\n",
    "            ## Convert to relative [x, y, w, h] coordinates\n",
    "            bbox[2] = bbox[2] - bbox[0]\n",
    "            bbox[3] = bbox[3] - bbox[1]\n",
    "\n",
    "            bbox[0] = bbox[0]/img_width\n",
    "            bbox[1] = bbox[1]/img_height\n",
    "            bbox[2] = bbox[2]/img_width\n",
    "            bbox[3] = bbox[3]/img_height\n",
    "\n",
    "            new_detection = fo.Detection(\n",
    "                label=label, \n",
    "                bounding_box=bbox,\n",
    "                person_id=person_id,\n",
    "                ann_id=ann_id,\n",
    "                mask=cropped_mask,\n",
    "                )\n",
    "            coco_detections.append(new_detection)\n",
    "        sample[\"coco_masks\"] = fo.Detections(detections=coco_detections)\n",
    "\n",
    "## add the masks\n",
    "add_coco_masks_to_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a7cb6d-c307-434a-9500-15ae6a4ccadb",
   "metadata": {},
   "source": [
    "# Import Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57ed90e2-cd2c-4b69-a7a3-d63e9effea28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 🚀 2023-10-21 Python-3.10.9 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt to yolov5m.pt...\n",
      "100%|███████████████████████████████████████| 40.8M/40.8M [00:00<00:00, 143MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients, 48.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "yolov5 = foz.load_zoo_model('yolov5m-coco-torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5be206d9-d465-4094-8aff-7b8ede9ccf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ NMS time limit 0.550s exceeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 31702/31702 [24.5m elapsed, 0s remaining, 21.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "dataset.apply_model(yolov5, label_field=\"yolov5m\")\n",
    "### Just retain the \"person\" detections\n",
    "people_view_values = dataset.filter_labels(\"yolov5m\", F(\"label\") == \"person\").values(\"yolov5m\")\n",
    "dataset.set_values(\"yolov5m\", people_view_values)\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22f25c-8c22-4c4a-ae09-05e07ec88e17",
   "metadata": {},
   "source": [
    "# Clip classification model --> Replace with teacher/student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7880f2c8-3d3a-4744-89cb-b69ca608520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a list of all 52 classes\n",
    "facet_classes = dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "## instantiate a CLIP model with these classes\n",
    "clip = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    text_prompt=\"A photo of a\",\n",
    "    classes=facet_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58a70100-8fa6-4b34-a754-f86b4b3a52b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 49551/49551 [24.6m elapsed, 0s remaining, 34.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "patch_view = dataset.to_patches(\"ground_truth\")\n",
    "patch_view.apply_model(clip, label_field=\"clip\")\n",
    "dataset.save_view(\"patch_view\", patch_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "339f2b7d-5058-4278-81be-32515887d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IOU_THRESHS = np.round(np.arange(0.5, 1.0, 0.05), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14507c5d-78b9-4895-9232-a277c0b61aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_detection_model(dataset, label_field):\n",
    "    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n",
    "    dataset.evaluate_detections(label_field, \"ground_truth\", eval_key=eval_key, classwise=False)\n",
    "    \n",
    "    for sample in dataset.iter_samples(progress=True):\n",
    "        for pred in sample[label_field].detections:\n",
    "            iou_field = f\"{eval_key}_iou\"\n",
    "            if iou_field not in pred:\n",
    "                continue\n",
    "\n",
    "            iou = pred[iou_field]\n",
    "            for it in IOU_THRESHS:\n",
    "                pred[f\"{iou_field}_{str(it).replace('.', '')}\"] = iou >= it\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "02f4e194-37cc-4de9-a76b-51ed0de18ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% |█████████████| 31702/31702 [4.6m elapsed, 0s remaining, 65.7 samples/s]       \n",
      " 100% |█████████████| 31702/31702 [8.1m elapsed, 0s remaining, 49.7 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "_evaluate_detection_model(dataset, 'yolov5m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e682ce1b-787e-47db-95b8-fb17ed5d9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_detection_mAR(sample_collection, label_field):\n",
    "    \"\"\"Computes the mean average recall of the specified detection field.\n",
    "    -- computed as the average over iou thresholds of the recall at\n",
    "    each threshold.\n",
    "    \"\"\"\n",
    "    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n",
    "    iou_recalls = []\n",
    "    for it in IOU_THRESHS:\n",
    "        field_str = f\"{label_field}.detections.{eval_key}_iou_{str(it).replace('.', '')}\"\n",
    "        counts = sample_collection.count_values(field_str)\n",
    "        tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "        recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "        iou_recalls.append(recall)\n",
    "\n",
    "    return np.mean(iou_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8eb73ee8-28ac-4416-8d7a-791959206e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_detection_mAR(dataset, label_field, concept, attributes):\n",
    "    sub_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_view = sub_view.filter_labels(\"ground_truth\", F(f\"skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_view = sub_view.filter_labels(\"ground_truth\", F(attribute[0]) == attribute[1])\n",
    "    return _compute_detection_mAR(sub_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d651365b-d969-456b-aa02-78a47f6d4d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4259259259259259"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept = 'lawman'\n",
    "attributes = {\"hairtype\": \"straight\", \"haircolor\": \"brown\"}\n",
    "get_concept_attr_detection_mAR(dataset, \"yolov5m\", concept, attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "320aa339-ddd0-4e4d-800e-aa6ac8d31b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_classification_model(dataset, prediction_field):\n",
    "    patch_view = dataset.load_saved_view(\"patch_view\")\n",
    "    eval_key = \"eval_\" + prediction_field\n",
    "    \n",
    "    for sample in patch_view.iter_samples(progress=True):\n",
    "        sample[eval_key] = (\n",
    "            sample.ground_truth.label == sample[prediction_field].label\n",
    "        )\n",
    "        sample.save()\n",
    "    dataset.save_view(\"patch_view\", patch_view, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "42ffe5b3-dfa1-42ad-bb88-52b04622bdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 49551/49551 [2.5m elapsed, 0s remaining, 329.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "_evaluate_classification_model(dataset, 'clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f830a851-7ff9-4ee1-9082-3810dc511cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_recall(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field.split(\"_\")[0]\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4ae5f107-e21f-46de-a8d3-3a329aa88916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_classification_recall(dataset, label_field, concept, attributes):\n",
    "    patch_view = dataset.load_saved_view(\"patch_view\")\n",
    "    sub_patch_view = patch_view.match(F(\"ground_truth.label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_patch_view = sub_patch_view.match(F(f\"ground_truth.skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_patch_view = sub_patch_view.match(F(f\"ground_truth.{attribute[0]}\") == attribute[1])\n",
    "    return _compute_classification_recall(sub_patch_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "62bf03f9-f46d-4ffd-bb0f-e868ad9116ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = {'hairtype': 'curly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "60239f5d-239e-4b3d-af81-a81936cab0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_concept_attr_classification_recall(dataset, \"clip\", concept, attribute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1c238e25-c163-4774-82dd-f9a96bb3e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_recall(dataset, label_field, concept, attribute):\n",
    "    if label_field in dataset.get_field_schema().keys():\n",
    "        return get_concept_attr_detection_mAR(dataset, label_field, concept, attribute)\n",
    "    else:\n",
    "        return get_concept_attr_classification_recall(dataset, label_field, concept, attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8e153b95-8f68-4a3c-a562-f1eee4136004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_disparity(dataset, label_field, concept, attribute1, attribute2):\n",
    "    recall1 = get_concept_attr_recall(dataset, label_field, concept, attribute1)\n",
    "    recall2 = get_concept_attr_recall(dataset, label_field, concept, attribute2)\n",
    "    return recall1 - recall2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "55758f91-25ff-4b29-80b2-8e82878221af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astronaut: -0.8269230769230769\n",
      "singer: -0.0008051529790660261\n",
      "judge: -0.06666666666666667\n",
      "student: 0.16279069767441856\n"
     ]
    }
   ],
   "source": [
    "attrs1 = {\"hairtype\": \"curly\"}\n",
    "attrs2 = {\"hairtype\": \"straight\"}\n",
    "for concept in [\"astronaut\", \"singer\", \"judge\", \"student\"]:\n",
    "    disparity = compute_disparity(dataset, \"clip\", concept, attrs1, attrs2)     \n",
    "    print(f\"{concept}: {disparity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18673b56-e446-4efe-89d8-d7296a216a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
