{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c928e2-c18b-457c-8640-f03b4b11b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import tarfile\n",
    "import shutil\n",
    "import torchvision\n",
    "import random\n",
    "import warnings\n",
    "import boto3\n",
    "import s3fs\n",
    "import io\n",
    "import time\n",
    "import botocore.exceptions\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, utils, models, datasets\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   # interactive mode\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as VF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "352a4027-18a6-4a0a-96ac-d948316ba36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d1fa2c-dab5-460d-b206-70e033a9c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from pycocotools import mask as maskUtils\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f329f00-bc92-4a2c-9f76-9d7a8f3aff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your access:  Â·Â·Â·Â·Â·Â·Â·Â·\n",
      "Enter your secret:  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "access_key = getpass.getpass(\"Enter your access: \")\n",
    "\n",
    "secret_key = password = getpass.getpass(\"Enter your secret: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04df1e08-51f4-42ee-9459-9f3fa86b1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'w210facetdata'\n",
    "annotations_prefix = 'annotations/'\n",
    "images_prefix = '/home/ubuntu/W210-Capstone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4353f856-80e0-4cd8-b76b-4b47358b3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(key=access_key, secret=secret_key)\n",
    "\n",
    "# Use s3.open to open the CSV file and read its content into a Pandas DataFrame\n",
    "with s3.open(f's3://{bucket_name}/{annotations_prefix}annotations.csv', 'rb') as file:\n",
    "    df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da8f83f8-bcc1-4357-a1e0-fae628eede3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FACET', 'FACET10', 'FACET11', 'FACET12', 'FACET2', 'FACET3', 'FACET4']\n"
     ]
    }
   ],
   "source": [
    "print(fo.list_datasets())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "577f2f0a-4282-447f-8f74-48fb90994d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31702/31702 [4.6s elapsed, 0s remaining, 7.0K samples/s]      \n",
      "Computing metadata...\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31702/31702 [1.0m elapsed, 0s remaining, 602.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "#if first time:\n",
    "dataset = fo.Dataset(name=\"FACET13\", persistent=True)\n",
    "# dataset = fo.load_dataset(name = \"FACET10\")\n",
    "dataset.add_images_dir(images_prefix)\n",
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcdf029e-d145-4a9b-84ae-aaa9c0fc1311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:        FACET13\n",
      "Media type:  image\n",
      "Num samples: 31702\n",
      "Persistent:  True\n",
      "Tags:        []\n",
      "Sample fields:\n",
      "    id:       fiftyone.core.fields.ObjectIdField\n",
      "    filepath: fiftyone.core.fields.StringField\n",
      "    tags:     fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d66dfdeb-dbd7-4ec4-b855-7b9808fbda12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=03f07f1f-be26-4225-89b4-e2cf395695ef\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f0412dd59f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed7057df-9e08-4a44-b4c6-8caf21559cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOLEAN_PERSONAL_ATTRS = (\n",
    "    \"has_facial_hair\",\n",
    "    \"has_tattoo\",\n",
    "    \"has_cap\",\n",
    "    \"has_mask\",\n",
    "    \"has_headscarf\",\n",
    "    \"has_eyeware\",\n",
    ")\n",
    "\n",
    "def add_boolean_person_attributes(detection, row_index):\n",
    "    for attr in BOOLEAN_PERSONAL_ATTRS:\n",
    "        detection[attr] = df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8083936e-3904-41d6-b306-5ae63facbe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hairtype(row_index):\n",
    "    hair_info = df.loc[row_index, df.columns.str.startswith('hairtype')]\n",
    "    hairtype = hair_info[hair_info == 1]\n",
    "    if len(hairtype) == 0:\n",
    "        return None\n",
    "    return hairtype.index[0].split('_')[1]\n",
    "\n",
    "def get_haircolor(row_index):\n",
    "    hair_info = df.loc[row_index, df.columns.str.startswith('hair_color')]\n",
    "    haircolor = hair_info[hair_info == 1]\n",
    "    if len(haircolor) == 0:\n",
    "        return None\n",
    "    return haircolor.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ff15e27-308f-4784-9f66-0bff3121d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skintone(row_index):\n",
    "    skin_info = df.loc[row_index, df.columns.str.startswith('skin_tone')]\n",
    "    return skin_info.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39d27714-007c-4b46-8630-a37b23c6293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perceived_gender_presentation(row_index):\n",
    "    gender_info = df.loc[row_index, df.columns.str.startswith('gender')]\n",
    "    pgp = gender_info[gender_info == 1]\n",
    "    if len(pgp) == 0:\n",
    "        return None\n",
    "    return pgp.index[0].replace(\"gender_presentation_\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "def get_perceived_age_presentation(row_index):\n",
    "    age_info = df.loc[row_index, df.columns.str.startswith('age')]\n",
    "    pap = age_info[age_info == 1]\n",
    "    if len(pap) == 0:\n",
    "        return None\n",
    "    return pap.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c0b23c2-63ce-4ad2-99af-73a2d8daa9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_person_attributes(detection, row_index):\n",
    "    detection[\"hairtype\"] = get_hairtype(row_index)\n",
    "    detection[\"haircolor\"] = get_haircolor(row_index)\n",
    "    add_boolean_person_attributes(detection, row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de5cabab-e119-4403-9f55-f1cc0eebbe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_protected_attributes(detection, row_index):\n",
    "    detection[\"perceived_age_presentation\"] = get_perceived_age_presentation(row_index)\n",
    "    detection[\"perceived_gender_presentation\"] = get_perceived_gender_presentation(row_index)\n",
    "    detection[\"skin_tone\"] = get_skintone(row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b2a296e-e6f3-4bc3-b1fa-fd4d8d084833",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISIBILITY_ATTRS = (\"visible_torso\", \"visible_face\", \"visible_minimal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67710320-4bef-40da-85ad-046bb7ca9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lighting(row_index):\n",
    "    lighting_info = df.loc[row_index, df.columns.str.startswith('lighting')]\n",
    "    lighting = lighting_info[lighting_info == 1]\n",
    "    if len(lighting) == 0:\n",
    "        return None\n",
    "    lighting = lighting.index[0].replace(\"lighting_\", \"\").replace(\"_\", \" \")\n",
    "    return lighting\n",
    "\n",
    "def add_other_attributes(detection, row_index):\n",
    "    detection[\"lighting\"] = get_lighting(row_index)\n",
    "    for attr in VISIBILITY_ATTRS:\n",
    "        detection[attr] = df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cba77a1c-bc24-4f66-9b86-097a881a08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detection(row_index, sample):\n",
    "    bbox_dict = json.loads(df.loc[row_index, \"bounding_box\"])\n",
    "    x, y, w, h = bbox_dict[\"x\"], bbox_dict[\"y\"], bbox_dict[\"width\"], bbox_dict[\"height\"]\n",
    "    cat1, cat2 = bbox_dict[\"dict_attributes\"][\"cat1\"], bbox_dict[\"dict_attributes\"][\"cat2\"]\n",
    "\n",
    "    person_id = df.loc[row_index, \"person_id\"]\n",
    "\n",
    "    img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "\n",
    "    bounding_box = [x/img_width, y/img_height, w/img_width, h/img_height]\n",
    "    detection = fo.Detection(\n",
    "        label=cat1, \n",
    "        bounding_box=bounding_box,\n",
    "        person_id=person_id,\n",
    "        )\n",
    "    if cat2 != 'none':\n",
    "        detection[\"class2\"] = cat2\n",
    "\n",
    "    add_person_attributes(detection, row_index)\n",
    "    add_protected_attributes(detection, row_index)\n",
    "    add_other_attributes(detection, row_index)\n",
    "\n",
    "    return detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c28cf19-5bb2-43a8-87a1-b5a7cfd1a315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31702/31702 [5.8m elapsed, 0s remaining, 91.1 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "def add_ground_truth_labels(dataset):\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        sample_annos = df[df['filename'] == sample.filename]\n",
    "        detections = []\n",
    "        for row in sample_annos.iterrows():\n",
    "            row_index = row[0]\n",
    "            detection = create_detection(row_index, sample)\n",
    "            detections.append(detection)\n",
    "        sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "    dataset.add_dynamic_sample_fields()\n",
    "\n",
    "## add all of the ground truth labels\n",
    "add_ground_truth_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79d3141a-7a1a-4b7c-ae80-a0bb64f5ba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31702/31702 [26.0m elapsed, 0s remaining, 16.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "def add_coco_masks_to_dataset(dataset):\n",
    "    with s3.open(f's3://{bucket_name}/{annotations_prefix}coco_masks.json', 'rb') as file:\n",
    "        coco_masks = json.load(file)\n",
    "    cmas = coco_masks[\"annotations\"]\n",
    "\n",
    "    FILENAME_TO_ID = {\n",
    "        img[\"file_name\"]: img[\"id\"]\n",
    "        for img in coco_masks[\"images\"]\n",
    "    }\n",
    "\n",
    "    CAT_TO_LABEL = {cat[\"id\"]: cat[\"name\"] for cat in coco_masks[\"categories\"]}\n",
    "\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        fn = sample.filename\n",
    "\n",
    "        if fn not in FILENAME_TO_ID:\n",
    "            continue\n",
    "\n",
    "        img_id = FILENAME_TO_ID[fn]\n",
    "        img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "        sample_annos = [a for a in cmas if a[\"image_id\"] == img_id]\n",
    "        if len(sample_annos) == 0:\n",
    "            continue\n",
    "\n",
    "        coco_detections = []\n",
    "        for ann in sample_annos:\n",
    "            label = CAT_TO_LABEL[ann[\"category_id\"]]\n",
    "            bbox = ann['bbox']\n",
    "            ann_id = ann['ann_id']\n",
    "            person_id = ann['facet_person_id']\n",
    "\n",
    "            mask = maskUtils.decode(ann[\"segmentation\"])\n",
    "            mask = Image.fromarray(255*mask)\n",
    "\n",
    "            ## Change bbox to be in the format [x, y, x, y]\n",
    "            bbox[2] = bbox[0] + bbox[2]\n",
    "            bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "            ## Get the cropped image\n",
    "            cropped_mask = np.array(mask.crop(bbox)).astype(bool)\n",
    "\n",
    "            ## Convert to relative [x, y, w, h] coordinates\n",
    "            bbox[2] = bbox[2] - bbox[0]\n",
    "            bbox[3] = bbox[3] - bbox[1]\n",
    "\n",
    "            bbox[0] = bbox[0]/img_width\n",
    "            bbox[1] = bbox[1]/img_height\n",
    "            bbox[2] = bbox[2]/img_width\n",
    "            bbox[3] = bbox[3]/img_height\n",
    "\n",
    "            new_detection = fo.Detection(\n",
    "                label=label, \n",
    "                bounding_box=bbox,\n",
    "                person_id=person_id,\n",
    "                ann_id=ann_id,\n",
    "                mask=cropped_mask,\n",
    "                )\n",
    "            coco_detections.append(new_detection)\n",
    "        sample[\"coco_masks\"] = fo.Detections(detections=coco_detections)\n",
    "\n",
    "## add the masks\n",
    "add_coco_masks_to_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "438782a2-82c1-4550-8372-24aa5ce267f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2023-10-21 Python-3.10.9 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt to yolov5m.pt...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 40.8M/40.8M [00:00<00:00, 197MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients, 48.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "yolov5 = foz.load_zoo_model('yolov5m-coco-torch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9fbc471-115b-4d5f-929a-f1358a428db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ NMS time limit 0.550s exceeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31702/31702 [24.1m elapsed, 0s remaining, 21.5 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "dataset.apply_model(yolov5, label_field=\"yolov5m\")\n",
    "\n",
    "### Just retain the \"person\" detections\n",
    "people_view_values = dataset.filter_labels(\"yolov5m\", VF(\"label\") == \"person\").values(\"yolov5m\")\n",
    "dataset.set_values(\"yolov5m\", people_view_values)\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8153299c-410c-4c3b-b48e-440a141ec107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## get a list of all 52 classes\n",
    "# facet_classes = dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "# ## instantiate a CLIP model with these classes\n",
    "# clip = foz.load_zoo_model(\n",
    "#     \"clip-vit-base32-torch\",\n",
    "#     text_prompt=\"A photo of a\",\n",
    "#     classes=facet_classes,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfc122ac-88cf-4066-bca1-4403e33bef3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfiftyone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfout\u001b[39;00m\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m fout\u001b[38;5;241m.\u001b[39mTorchImageModelConfig(\n\u001b[1;32m      5\u001b[0m     {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentrypoint_fcn\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision.models.mobilenet.mobilenet_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentrypoint_args\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMobileNet_V2_Weights.DEFAULT\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_processor_cls\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiftyone.utils.torch.ClassifierOutputProcessor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mlabels_path\u001b[49m,\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_min_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m224\u001b[39m,\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_max_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m],\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_std\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m],\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings_layer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<classifier.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     }\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m fout\u001b[38;5;241m.\u001b[39mTorchImageModel(config)\n\u001b[1;32m     19\u001b[0m dataset\u001b[38;5;241m.\u001b[39mapply_model(model, label_field\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagenet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels_path' is not defined"
     ]
    }
   ],
   "source": [
    "import fiftyone.utils.torch as fout\n",
    "\n",
    "\n",
    "config = fout.TorchImageModelConfig(\n",
    "    {\n",
    "        \"entrypoint_fcn\": \"torchvision.models.mobilenet.mobilenet_v2\",\n",
    "        \"entrypoint_args\": {\"weights\": \"MobileNet_V2_Weights.DEFAULT\"},\n",
    "        \"output_processor_cls\": \"fiftyone.utils.torch.ClassifierOutputProcessor\",\n",
    "        \"labels_path\": labels_path,\n",
    "        \"image_min_dim\": 224,\n",
    "        \"image_max_dim\": 2048,\n",
    "        \"image_mean\": [0.485, 0.456, 0.406],\n",
    "        \"image_std\": [0.229, 0.224, 0.225],\n",
    "        \"embeddings_layer\": \"<classifier.1\",\n",
    "    }\n",
    ")\n",
    "model = fout.TorchImageModel(config)\n",
    "\n",
    "dataset.apply_model(model, label_field=\"imagenet\")\n",
    "embeddings = dataset.compute_embeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f86f6c6e-a15f-48c5-bcb5-16b173103930",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a list of all 52 classes\n",
    "facet_classes = dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "## instantiate a CLIP model with these classes\n",
    "clip = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    text_prompt=\"A photo of a\",\n",
    "    classes=facet_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93b9298d-e336-4ab1-8771-45d1463348bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49551/49551 [24.6m elapsed, 0s remaining, 30.8 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "patch_view = dataset.to_patches(\"ground_truth\")\n",
    "patch_view.apply_model(clip, label_field=\"clip\")\n",
    "dataset.save_view(\"patch_view\", patch_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a515c09-6ff8-483c-8cb3-90d453629184",
   "metadata": {},
   "outputs": [],
   "source": [
    "IOU_THRESHS = np.round(np.arange(0.5, 1.0, 0.05), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b145be3-1164-4276-84b6-b6baa559ad90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_detection_model(dataset, label_field):\n",
    "    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n",
    "    dataset.evaluate_detections(label_field, \"ground_truth\", eval_key=eval_key, classwise=False)\n",
    "    \n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        for pred in sample[label_field].detections:\n",
    "            iou_field = f\"{eval_key}_iou\"\n",
    "            if iou_field not in pred:\n",
    "                continue\n",
    "\n",
    "            iou = pred[iou_field]\n",
    "            for it in IOU_THRESHS:\n",
    "                pred[f\"{iou_field}_{str(it).replace('.', '')}\"] = iou >= it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c8dd2e7-32c0-410f-b127-a0610b53e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_detection_mAR(sample_collection, label_field):\n",
    "    \"\"\"Computes the mean average recall of the specified detection field.\n",
    "    -- computed as the average over iou thresholds of the recall at\n",
    "    each threshold.\n",
    "    \"\"\"\n",
    "    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n",
    "    iou_recalls = []\n",
    "    for it in IOU_THRESHS:\n",
    "        field_str = f\"{label_field}.detections.{eval_key}_iou_{str(it).replace('.', '')}\"\n",
    "        counts = sample_collection.count_values(field_str)\n",
    "        tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "        recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "        iou_recalls.append(recall)\n",
    "\n",
    "    return np.mean(iou_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e2da5e0-0886-4db8-b336-0fdbe0eec11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_detection_mAR(dataset, label_field, concept, attributes):\n",
    "    sub_view = dataset.filter_labels(\"ground_truth\", VF(\"label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_view = sub_view.filter_labels(\"ground_truth\", VF(f\"skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_view = sub_view.filter_labels(f\"ground_truth\", VF(attribute[0]) == attribute[1])\n",
    "    return _compute_detection_mAR(sub_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd51f058-4bc6-4b7f-b5b5-4067327edf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept = 'student'\n",
    "attributes = {\"hairtype\": \"curly\"}\n",
    "get_concept_attr_detection_mAR(dataset, \"yolov5m\", concept, attributes)\n",
    "## 0.875"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ec5c747-c124-4636-9b96-5aea6e6dc6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Detections: {                                                               \n",
      "    'detections': [\n",
      "        <Detection: {\n",
      "            'id': '6533f6eba0573a0b4f03a824',\n",
      "            'attributes': {},\n",
      "            'tags': [],\n",
      "            'label': 'person',\n",
      "            'bounding_box': [\n",
      "                0.5043890029191971,\n",
      "                0.09288859367370605,\n",
      "                0.4684792459011078,\n",
      "                0.8787157535552979,\n",
      "            ],\n",
      "            'mask': None,\n",
      "            'confidence': 0.6010023951530457,\n",
      "            'index': None,\n",
      "        }>,\n",
      "        <Detection: {\n",
      "            'id': '6533f6eba0573a0b4f03a825',\n",
      "            'attributes': {},\n",
      "            'tags': [],\n",
      "            'label': 'person',\n",
      "            'bounding_box': [\n",
      "                0.0033426880836486816,\n",
      "                0.037370502948760986,\n",
      "                0.4971860647201538,\n",
      "                0.9558652639389038,\n",
      "            ],\n",
      "            'mask': None,\n",
      "            'confidence': 0.5358169078826904,\n",
      "            'index': None,\n",
      "        }>,\n",
      "    ],\n",
      "}>\n",
      "   0% |/------------|     0/31702 [2.2s elapsed, ? remaining, ? samples/s]   \n"
     ]
    }
   ],
   "source": [
    "for sample in dataset.iter_samples(autosave=True, progress=True): \n",
    "    with s3.open(f's3://{bucket_name}/{annotations_prefix}coco_masks.json', 'rb') as file:\n",
    "        coco_masks = json.load(file)\n",
    "    cmas = coco_masks[\"annotations\"]\n",
    "\n",
    "    CAT_TO_LABEL = {cat[\"id\"]: cat[\"name\"] for cat in coco_masks[\"categories\"]}\n",
    "\n",
    "\n",
    "    FILENAME_TO_ID = {\n",
    "        img[\"file_name\"]: img[\"id\"]\n",
    "        for img in coco_masks[\"images\"]\n",
    "    }\n",
    "    fn = sample.filename\n",
    "\n",
    "    if fn not in FILENAME_TO_ID:\n",
    "        continue\n",
    "\n",
    "    img_id = FILENAME_TO_ID[fn]\n",
    "    img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "    sample_annos = [a for a in cmas if a[\"image_id\"] == img_id]\n",
    "    for ann in sample_annos:\n",
    "        label = CAT_TO_LABEL[ann[\"category_id\"]]\n",
    "        bbox = ann['bbox']\n",
    "        ann_id = ann['ann_id']\n",
    "        person_id = ann['facet_person_id']\n",
    "    print(sample['yolov5m'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58129a44-ffc1-4a0c-b021-29413f63af9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        FACET13\n",
       "Media type:  image\n",
       "Num samples: 31702\n",
       "Persistent:  True\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:           fiftyone.core.fields.ObjectIdField\n",
       "    filepath:     fiftyone.core.fields.StringField\n",
       "    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    coco_masks:   fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    yolov5m:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c75af-b506-4a2f-9aec-b359ec10a580",
   "metadata": {},
   "source": [
    "# Bullshit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a0d54ca9-a88e-4e33-bfe4-1696bc68fcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the S3 bucket name and prefixes\n",
    "bucket_name = 'w210facetdata'\n",
    "annotations_prefix = 'annotations/'\n",
    "image_dir = 'images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9a64c1f3-a025-4fd7-be01-91db037e1bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting unique class labels from the 'class1' column\n",
    "classes = df['class1'].unique()\n",
    "\n",
    "# Creating a mapping from index to class label\n",
    "idx_to_class = {i: j for i, j in enumerate(classes)}\n",
    "\n",
    "# Creating a reverse mapping from class label to index\n",
    "class_to_idx = {value: key for key, value in idx_to_class.items()}\n",
    "\n",
    "# Creating a mapping from index to annotation column name starting from the 7th column\n",
    "idx_to_annot = {i: j for i, j in enumerate(df.columns[6:])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "cb172056-e5f4-4348-97ac-e56b55858fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "88308a2a-518c-41c7-88ba-fc854a0e14c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ae50afb7-0c76-498f-9ed6-577a1bc197e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toggle on/off subsetting\n",
    "# Define the number of samples you want in your subset\n",
    "subset_size = 1000  # Adjust the size as needed\n",
    "\n",
    "# Create a smaller subset of your dataset\n",
    "train_data = train_data[:subset_size]\n",
    "test_data = test_data[:subset_size]\n",
    "val_data = val_data[:subset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e0291f04-c6d2-44ec-ada0-74fba4e9d882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, split_data, image_dir, transform=None):\n",
    "        self.data = split_data\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx, 2]\n",
    "        image_key = f'{images_prefix}{img_name}'  # Construct S3 object key\n",
    "        # Load image from S3\n",
    "        with s3.open(f'{bucket_name}/{image_key}', 'rb') as file:\n",
    "            img_data = file.read()\n",
    "        \n",
    "        # Open the image directly from the byte stream using PIL\n",
    "        image = Image.open(BytesIO(img_data))\n",
    "        annotations = self.data.iloc[idx, 6:].values.astype(np.float16).reshape(-1, 1)\n",
    "        label = class_to_idx[self.data.iloc[idx, 3]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label), torch.from_numpy(annotations)\n",
    "\n",
    "# Create custom datasets and data loaders\n",
    "train_dataset = CustomDataset(split_data=train_data, image_dir=image_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(split_data=test_data, image_dir=image_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = CustomDataset(split_data=val_data, image_dir=image_dir, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9d874202-c161-442a-9e7c-63b825026017",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNN(nn.Module):\n",
    "    def __init__(self, num_classes=52):\n",
    "        super(DeepNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(65536, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "50d4e859-3df1-449a-a01a-638b9b5dc698",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightNN(nn.Module):\n",
    "    def __init__(self, num_classes=52):\n",
    "        super(LightNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(8, 8, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32768, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "255e2590-a645-449b-99aa-b4ee67d2d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_teacher = nn.CrossEntropyLoss()  # Loss for teacher model\n",
    "criterion_student = nn.KLDivLoss()  # Knowledge distillation loss\n",
    "\n",
    "# Instantiate the teacher and student models\n",
    "teacher_model = DeepNN(num_classes=52).to('cuda')\n",
    "\n",
    "student_model = LightNN(num_classes=52).to('cuda')\n",
    "\n",
    "\n",
    "# Define optimizer for the student model\n",
    "optimizer_student = optim.Adam(student_model.parameters(), lr=.01)\n",
    "optimizer_teacher = optim.Adam(teacher_model.parameters(), lr=.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8bad1d9e-bfe0-49be-8b2e-e298ad273b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "fine_tune_learning_rate = learning_rate / 10\n",
    "num_classes = 52\n",
    "num_epochs = 2\n",
    "fine_tune_epochs = 2\n",
    "disparity_weight = 0.1\n",
    "alpha = 0.5\n",
    "temperature = 5.0\n",
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "efceb6b2-380c-476a-8d52-c31e1daa2160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   3%|â–ˆ                                | 1/32 [00:05<02:55,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07575056701898575\n",
      "CE Loss: 3.835609197616577\n",
      "Total Loss: 1.9556798934936523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   6%|â–ˆâ–ˆ                               | 2/32 [00:11<02:47,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07576655596494675\n",
      "CE Loss: 3.831313133239746\n",
      "Total Loss: 1.9535398483276367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   9%|â–ˆâ–ˆâ–ˆ                              | 3/32 [00:16<02:34,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07576114684343338\n",
      "CE Loss: 3.84684419631958\n",
      "Total Loss: 1.961302638053894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–                            | 4/32 [00:21<02:28,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07574716955423355\n",
      "CE Loss: 3.8261120319366455\n",
      "Total Loss: 1.9509296417236328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 5/32 [00:27<02:26,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07574664801359177\n",
      "CE Loss: 3.837665557861328\n",
      "Total Loss: 1.9567060470581055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 6/32 [00:32<02:20,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07574234902858734\n",
      "CE Loss: 3.892853021621704\n",
      "Total Loss: 1.9842976331710815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 7/32 [00:38<02:17,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07572442293167114\n",
      "CE Loss: 3.8511030673980713\n",
      "Total Loss: 1.9634137153625488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 8/32 [00:44<02:17,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07569122314453125\n",
      "CE Loss: 3.834386110305786\n",
      "Total Loss: 1.9550386667251587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 9/32 [00:49<02:10,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07567045837640762\n",
      "CE Loss: 3.7780959606170654\n",
      "Total Loss: 1.9268832206726074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 10/32 [00:55<02:04,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07564491033554077\n",
      "CE Loss: 3.824186086654663\n",
      "Total Loss: 1.9499155282974243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 11/32 [01:01<01:58,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07561714202165604\n",
      "CE Loss: 3.8629965782165527\n",
      "Total Loss: 1.9693068265914917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 12/32 [01:06<01:51,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07560870796442032\n",
      "CE Loss: 3.8251452445983887\n",
      "Total Loss: 1.9503769874572754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 13/32 [01:12<01:48,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07562104612588882\n",
      "CE Loss: 3.7494709491729736\n",
      "Total Loss: 1.9125460386276245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 14/32 [01:18<01:44,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07554510980844498\n",
      "CE Loss: 3.8004207611083984\n",
      "Total Loss: 1.9379829168319702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 15/32 [01:24<01:36,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07555358856916428\n",
      "CE Loss: 3.845147132873535\n",
      "Total Loss: 1.9603503942489624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 16/32 [01:29<01:28,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07551729679107666\n",
      "CE Loss: 3.8983640670776367\n",
      "Total Loss: 1.986940622329712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 17/32 [01:34<01:23,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07552877813577652\n",
      "CE Loss: 3.860450506210327\n",
      "Total Loss: 1.9679896831512451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 18/32 [01:40<01:17,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.0754910334944725\n",
      "CE Loss: 3.74328351020813\n",
      "Total Loss: 1.909387230873108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 19/32 [01:45<01:11,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07542981207370758\n",
      "CE Loss: 3.7849109172821045\n",
      "Total Loss: 1.9301704168319702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 20/32 [01:51<01:06,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07542406022548676\n",
      "CE Loss: 3.8466713428497314\n",
      "Total Loss: 1.961047649383545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 21/32 [01:57<01:01,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07537750154733658\n",
      "CE Loss: 3.74963641166687\n",
      "Total Loss: 1.9125069379806519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 22/32 [02:02<00:55,  5.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07534224539995193\n",
      "CE Loss: 3.7137622833251953\n",
      "Total Loss: 1.894552230834961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 23/32 [02:08<00:49,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07536887377500534\n",
      "CE Loss: 3.7586307525634766\n",
      "Total Loss: 1.9169998168945312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 24/32 [02:13<00:44,  5.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07531052082777023\n",
      "CE Loss: 3.871209144592285\n",
      "Total Loss: 1.9732598066329956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 25/32 [02:18<00:38,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07528430223464966\n",
      "CE Loss: 3.790571689605713\n",
      "Total Loss: 1.9329279661178589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 26/32 [02:24<00:32,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07526466995477676\n",
      "CE Loss: 3.8684041500091553\n",
      "Total Loss: 1.971834421157837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/32 [02:29<00:27,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07524985074996948\n",
      "CE Loss: 3.8233213424682617\n",
      "Total Loss: 1.949285626411438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 28/32 [02:35<00:21,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.0751715824007988\n",
      "CE Loss: 3.720762014389038\n",
      "Total Loss: 1.897966742515564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/32 [02:40<00:16,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07517286390066147\n",
      "CE Loss: 3.7702221870422363\n",
      "Total Loss: 1.9226975440979004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 30/32 [02:45<00:10,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07511971145868301\n",
      "CE Loss: 3.7488913536071777\n",
      "Total Loss: 1.9120055437088013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 31/32 [02:51<00:05,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07507412880659103\n",
      "CE Loss: 3.7974960803985596\n",
      "Total Loss: 1.936285138130188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [02:52<00:00,  5.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07503533363342285\n",
      "CE Loss: 3.8674070835113525\n",
      "Total Loss: 1.9712212085723877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:   3%|â–ˆ                                | 1/32 [00:04<02:32,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07489293813705444\n",
      "CE Loss: 3.8670456409454346\n",
      "Total Loss: 1.970969319343567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:   6%|â–ˆâ–ˆ                               | 2/32 [00:09<02:24,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07497315108776093\n",
      "CE Loss: 3.967163324356079\n",
      "Total Loss: 2.0210683345794678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:   9%|â–ˆâ–ˆâ–ˆ                              | 3/32 [00:14<02:22,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07491344213485718\n",
      "CE Loss: 3.9554479122161865\n",
      "Total Loss: 2.0151805877685547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–                            | 4/32 [00:19<02:17,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07490310817956924\n",
      "CE Loss: 3.7575180530548096\n",
      "Total Loss: 1.9162105321884155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 5/32 [00:24<02:13,  4.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07480747997760773\n",
      "CE Loss: 3.7729973793029785\n",
      "Total Loss: 1.9239023923873901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 6/32 [00:29<02:08,  4.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07484254986047745\n",
      "CE Loss: 3.718601703643799\n",
      "Total Loss: 1.8967220783233643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 7/32 [00:34<02:07,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07472284138202667\n",
      "CE Loss: 3.6102800369262695\n",
      "Total Loss: 1.8425014019012451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                        | 8/32 [00:40<02:02,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07474402338266373\n",
      "CE Loss: 3.741503953933716\n",
      "Total Loss: 1.9081239700317383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                       | 9/32 [00:44<01:56,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07471528649330139\n",
      "CE Loss: 3.6361494064331055\n",
      "Total Loss: 1.855432391166687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 10/32 [00:49<01:50,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07460927963256836\n",
      "CE Loss: 3.7156620025634766\n",
      "Total Loss: 1.8951356410980225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 11/32 [00:55<01:46,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07461842149496078\n",
      "CE Loss: 3.6235053539276123\n",
      "Total Loss: 1.8490618467330933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 12/32 [01:00<01:40,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07463361322879791\n",
      "CE Loss: 3.9023094177246094\n",
      "Total Loss: 1.988471508026123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 13/32 [01:05<01:39,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07447613775730133\n",
      "CE Loss: 3.529726266860962\n",
      "Total Loss: 1.8021012544631958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 14/32 [01:10<01:32,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07451015710830688\n",
      "CE Loss: 3.6906323432922363\n",
      "Total Loss: 1.8825712203979492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 15/32 [01:15<01:26,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07441609352827072\n",
      "CE Loss: 3.773712635040283\n",
      "Total Loss: 1.9240643978118896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 16/32 [01:20<01:22,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07444705069065094\n",
      "CE Loss: 3.7624740600585938\n",
      "Total Loss: 1.9184606075286865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 17/32 [01:26<01:18,  5.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07438552379608154\n",
      "CE Loss: 3.805724620819092\n",
      "Total Loss: 1.9400551319122314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 18/32 [01:31<01:13,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07435647398233414\n",
      "CE Loss: 3.788719415664673\n",
      "Total Loss: 1.9315379858016968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 19/32 [01:36<01:07,  5.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07419939339160919\n",
      "CE Loss: 3.7303884029388428\n",
      "Total Loss: 1.9022939205169678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ            | 20/32 [01:42<01:05,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.0741916298866272\n",
      "CE Loss: 3.707576274871826\n",
      "Total Loss: 1.8908839225769043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 21/32 [01:47<00:58,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07410577684640884\n",
      "CE Loss: 3.5515799522399902\n",
      "Total Loss: 1.812842845916748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 22/32 [01:52<00:51,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07411327958106995\n",
      "CE Loss: 3.7443933486938477\n",
      "Total Loss: 1.9092533588409424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 23/32 [01:57<00:45,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07406619936227798\n",
      "CE Loss: 3.800365924835205\n",
      "Total Loss: 1.93721604347229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 24/32 [02:02<00:41,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07410313189029694\n",
      "CE Loss: 3.584074020385742\n",
      "Total Loss: 1.829088568687439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 25/32 [02:08<00:36,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07392004132270813\n",
      "CE Loss: 3.7399942874908447\n",
      "Total Loss: 1.9069571495056152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 26/32 [02:14<00:33,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07375221699476242\n",
      "CE Loss: 3.607635021209717\n",
      "Total Loss: 1.8406935930252075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 27/32 [02:19<00:27,  5.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07378381490707397\n",
      "CE Loss: 3.702383041381836\n",
      "Total Loss: 1.8880834579467773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 28/32 [02:24<00:21,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07384244352579117\n",
      "CE Loss: 3.6572325229644775\n",
      "Total Loss: 1.8655375242233276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/32 [02:30<00:16,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.0737229585647583\n",
      "CE Loss: 3.783812999725342\n",
      "Total Loss: 1.9287679195404053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 30/32 [02:36<00:11,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07380897551774979\n",
      "CE Loss: 3.7269773483276367\n",
      "Total Loss: 1.9003931283950806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 31/32 [02:41<00:05,  5.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07373419404029846\n",
      "CE Loss: 3.7030210494995117\n",
      "Total Loss: 1.8883776664733887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [02:42<00:00,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KD Loss: 0.07344287633895874\n",
      "CE Loss: 3.6801598072052\n",
      "Total Loss: 1.8768013715744019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f6b66d7f250>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# Lists to store loss values\n",
    "kd_loss_values = []\n",
    "ce_loss_values = []\n",
    "disparity_loss_values = []\n",
    "total_loss_values = []\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    student_model.train()\n",
    "    teacher_model.train()\n",
    "\n",
    "    for images, labels, annotations in pbar:\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        \n",
    "        optimizer_student.zero_grad()\n",
    "        optimizer_teacher.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        teacher_outputs = teacher_model(images)\n",
    "        student_outputs = student_model(images)\n",
    "\n",
    "        # Calculate additional metrics including recall\n",
    "        y_true = labels.cpu().numpy()\n",
    "        y_pred_student = torch.argmax(student_outputs, dim=1).cpu().numpy()\n",
    "        y_pred_teacher = torch.argmax(teacher_outputs, dim=1).cpu().numpy()\n",
    "\n",
    "        # Calculate the Knowledge Distillation loss and Cross Entropy loss\n",
    "        kd_loss = criterion_student(\n",
    "            F.log_softmax(student_outputs / temperature, dim=1),  # Apply temperature scaling\n",
    "            F.softmax(teacher_outputs / temperature, dim=1)+eps  # Apply temperature scaling\n",
    "        )\n",
    "        ce_loss = criterion_teacher(student_outputs, labels)\n",
    "        \n",
    "        # Append the loss values for plotting\n",
    "        kd_loss_values.append(kd_loss.item())\n",
    "        ce_loss_values.append(ce_loss.item())\n",
    "\n",
    "\n",
    "        # Combine the losses\n",
    "        total_loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "        \n",
    "        # Append the total loss value for plotting\n",
    "        total_loss_values.append(total_loss.item())\n",
    "\n",
    "        # Perform the backward pass\n",
    "        total_loss.backward()\n",
    "\n",
    "        # Optimize the models\n",
    "        optimizer_student.step()\n",
    "        optimizer_teacher.step()\n",
    "        \n",
    "        # Output the loss values\n",
    "        print(f'KD Loss: {kd_loss.item()}')\n",
    "        print(f'CE Loss: {ce_loss.item()}')\n",
    "        print(f'Total Loss: {total_loss.item()}')\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    optimizer_student.step()\n",
    "    optimizer_teacher.step()\n",
    "\n",
    "# Disable anomaly detection when done\n",
    "torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45db397a-05c7-4561-b9c7-5be84f992179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
