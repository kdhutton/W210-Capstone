{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c928e2-c18b-457c-8640-f03b4b11b979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "import boto3\n",
    "import s3fs\n",
    "import io\n",
    "import time\n",
    "import botocore.exceptions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import torch.nn.functional as F\n",
    "import getpass\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, utils, models, datasets\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from skimage import io, transform\n",
    "from PIL import Image\n",
    "from PIL import Image\n",
    "from pycocotools import mask as maskUtils\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f329f00-bc92-4a2c-9f76-9d7a8f3aff44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your access:  路路路路路路路路\n",
      "Enter your secret:  路路路路路路路路\n"
     ]
    }
   ],
   "source": [
    "access_key = getpass.getpass(\"Enter your access: \")\n",
    "\n",
    "secret_key = password = getpass.getpass(\"Enter your secret: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04df1e08-51f4-42ee-9459-9f3fa86b1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'w210facetdata'\n",
    "annotations_prefix = 'annotations/'\n",
    "images_prefix = '/home/ubuntu/W210-Capstone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6d45210-7dcb-402d-a7d1-a3719f9a180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(key=access_key, secret=secret_key)\n",
    "\n",
    "# Use s3.open to open the CSV file and read its content into a Pandas DataFrame\n",
    "with s3.open(f's3://{bucket_name}/{annotations_prefix}annotations.csv', 'rb') as file:\n",
    "    gt_df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d50a822-72ce-49e4-9c5e-846f3c2ef96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use relative paths to your image dirs\n",
    "# dataset = fo.Dataset(name = \"FACET14\", persistent=True)\n",
    "dataset = fo.load_dataset('FACET14')\n",
    "# dataset.add_images_dir(images_prefix)\n",
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c721ef-146f-4f48-a411-7fb93bca9d84",
   "metadata": {},
   "source": [
    "# Object Detection Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be99a8b0-a415-4560-a578-2b9f8bba27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOLEAN_PERSONAL_ATTRS = (\n",
    "    \"has_facial_hair\",\n",
    "    \"has_tattoo\",\n",
    "    \"has_cap\",\n",
    "    \"has_mask\",\n",
    "    \"has_headscarf\",\n",
    "    \"has_eyeware\",\n",
    ")\n",
    "def add_boolean_person_attributes(detection, row_index):\n",
    "    for attr in BOOLEAN_PERSONAL_ATTRS:\n",
    "        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "892ad9c7-cb94-49ee-bbe6-caf72e3e7bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hairtype(row_index):\n",
    "    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hairtype')]\n",
    "    hairtype = hair_info[hair_info == 1]\n",
    "    if len(hairtype) == 0:\n",
    "        return None\n",
    "    return hairtype.index[0].split('_')[1]\n",
    "\n",
    "def get_haircolor(row_index):\n",
    "    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hair_color')]\n",
    "    haircolor = hair_info[hair_info == 1]\n",
    "    if len(haircolor) == 0:\n",
    "        return None\n",
    "    return haircolor.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8fc5510-2fd8-42f9-a58a-c301286b3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_person_attributes(detection, row_index):\n",
    "    detection[\"hairtype\"] = get_hairtype(row_index)\n",
    "    detection[\"haircolor\"] = get_haircolor(row_index)\n",
    "    add_boolean_person_attributes(detection, row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0091b7eb-d570-4b21-8290-17fee15d226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perceived_gender_presentation(row_index):\n",
    "    gender_info = gt_df.loc[row_index, gt_df.columns.str.startswith('gender')]\n",
    "    pgp = gender_info[gender_info == 1]\n",
    "    if len(pgp) == 0:\n",
    "        return None\n",
    "    return pgp.index[0].replace(\"gender_presentation_\", \"\").replace(\"_\", \" \")\n",
    "\n",
    "def get_perceived_age_presentation(row_index):\n",
    "    age_info = gt_df.loc[row_index, gt_df.columns.str.startswith('age')]\n",
    "    pap = age_info[age_info == 1]\n",
    "    if len(pap) == 0:\n",
    "        return None\n",
    "    return pap.index[0].split('_')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c49856f0-9c40-44f1-b8dc-afc8a0c80786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_skintone(row_index):\n",
    "    skin_info = gt_df.loc[row_index, gt_df.columns.str.startswith('skin_tone')]\n",
    "    return skin_info.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a702e8b-93a2-4e8b-879c-f82be93830ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_protected_attributes(detection, row_index):\n",
    "    detection[\"perceived_age_presentation\"] = get_perceived_age_presentation(row_index)\n",
    "    detection[\"perceived_gender_presentation\"] = get_perceived_gender_presentation(row_index)\n",
    "    detection[\"skin_tone\"] = get_skintone(row_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01f3a95d-e491-4de4-be5a-d53de547f0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "VISIBILITY_ATTRS = (\"visible_torso\", \"visible_face\", \"visible_minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc9ed1ed-c71d-4583-963f-bfa23563309c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lighting(row_index):\n",
    "    lighting_info = gt_df.loc[row_index, gt_df.columns.str.startswith('lighting')]\n",
    "    lighting = lighting_info[lighting_info == 1]\n",
    "    if len(lighting) == 0:\n",
    "        return None\n",
    "    lighting = lighting.index[0].replace(\"lighting_\", \"\").replace(\"_\", \" \")\n",
    "    return lighting\n",
    "\n",
    "def add_other_attributes(detection, row_index):\n",
    "    detection[\"lighting\"] = get_lighting(row_index)\n",
    "    for attr in VISIBILITY_ATTRS:\n",
    "        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3de3c768-bfa3-4727-95ed-59346a5788a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_detection(row_index, sample):\n",
    "    bbox_dict = json.loads(gt_df.loc[row_index, \"bounding_box\"])\n",
    "    x, y, w, h = bbox_dict[\"x\"], bbox_dict[\"y\"], bbox_dict[\"width\"], bbox_dict[\"height\"]\n",
    "    cat1, cat2 = bbox_dict[\"dict_attributes\"][\"cat1\"], bbox_dict[\"dict_attributes\"][\"cat2\"]\n",
    "\n",
    "    person_id = gt_df.loc[row_index, \"person_id\"]\n",
    "\n",
    "    img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "\n",
    "    bounding_box = [x/img_width, y/img_height, w/img_width, h/img_height]\n",
    "    detection = fo.Detection(\n",
    "        label=cat1, \n",
    "        bounding_box=bounding_box,\n",
    "        person_id=person_id,\n",
    "        )\n",
    "    if cat2 != 'none':\n",
    "        detection[\"class2\"] = cat2\n",
    "\n",
    "    add_person_attributes(detection, row_index)\n",
    "    add_protected_attributes(detection, row_index)\n",
    "    add_other_attributes(detection, row_index)\n",
    "\n",
    "    return detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1869986-c9eb-454e-b164-cd5f8b598286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ground_truth_labels(dataset):\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        sample_annos = gt_df[gt_df['filename'] == sample.filename]\n",
    "        detections = []\n",
    "        for row in sample_annos.iterrows():\n",
    "            row_index = row[0]\n",
    "            detection = create_detection(row_index, sample)\n",
    "            detections.append(detection)\n",
    "        sample[\"ground_truth\"] = fo.Detections(detections=detections)\n",
    "    dataset.add_dynamic_sample_fields()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b7888c9f-92b3-425b-96e5-d7a16692a0f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name:        FACET14\n",
       "Media type:  image\n",
       "Num samples: 31702\n",
       "Persistent:  True\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:              fiftyone.core.fields.ObjectIdField\n",
       "    filepath:        fiftyone.core.fields.StringField\n",
       "    tags:            fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:        fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    ground_truth:    fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    coco_masks:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    yolov5m:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    eval_yolov5m_tp: fiftyone.core.fields.IntField\n",
       "    eval_yolov5m_fp: fiftyone.core.fields.IntField\n",
       "    eval_yolov5m_fn: fiftyone.core.fields.IntField"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec896ec-9aa5-4033-839d-ced0b1ae54d6",
   "metadata": {},
   "source": [
    "# Add labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d068d1d0-38e0-43b7-b827-5006503182d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% || 31702/31702 [5.5m elapsed, 0s remaining, 93.8 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "## add all of the ground truth labels\n",
    "add_ground_truth_labels(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e53d3-c90d-467e-83c2-6b1bc5717fdd",
   "metadata": {},
   "source": [
    "# Add Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b485560-887e-47f9-b980-a9460a481942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% || 31702/31702 [26.0m elapsed, 0s remaining, 16.5 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "def add_coco_masks_to_dataset(dataset):\n",
    "    with s3.open(f's3://{bucket_name}/{annotations_prefix}coco_masks.json', 'rb') as file:\n",
    "        coco_masks = json.load(file)\n",
    "    cmas = coco_masks[\"annotations\"]\n",
    "\n",
    "    FILENAME_TO_ID = {\n",
    "        img[\"file_name\"]: img[\"id\"]\n",
    "        for img in coco_masks[\"images\"]\n",
    "    }\n",
    "\n",
    "    CAT_TO_LABEL = {cat[\"id\"]: cat[\"name\"] for cat in coco_masks[\"categories\"]}\n",
    "\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        fn = sample.filename\n",
    "\n",
    "        if fn not in FILENAME_TO_ID:\n",
    "            continue\n",
    "\n",
    "        img_id = FILENAME_TO_ID[fn]\n",
    "        img_width, img_height = sample.metadata.width, sample.metadata.height\n",
    "        sample_annos = [a for a in cmas if a[\"image_id\"] == img_id]\n",
    "        if len(sample_annos) == 0:\n",
    "            continue\n",
    "\n",
    "        coco_detections = []\n",
    "        for ann in sample_annos:\n",
    "            label = CAT_TO_LABEL[ann[\"category_id\"]]\n",
    "            bbox = ann['bbox']\n",
    "            ann_id = ann['ann_id']\n",
    "            person_id = ann['facet_person_id']\n",
    "\n",
    "            mask = maskUtils.decode(ann[\"segmentation\"])\n",
    "            mask = Image.fromarray(255*mask)\n",
    "\n",
    "            ## Change bbox to be in the format [x, y, x, y]\n",
    "            bbox[2] = bbox[0] + bbox[2]\n",
    "            bbox[3] = bbox[1] + bbox[3]\n",
    "\n",
    "            ## Get the cropped image\n",
    "            cropped_mask = np.array(mask.crop(bbox)).astype(bool)\n",
    "\n",
    "            ## Convert to relative [x, y, w, h] coordinates\n",
    "            bbox[2] = bbox[2] - bbox[0]\n",
    "            bbox[3] = bbox[3] - bbox[1]\n",
    "\n",
    "            bbox[0] = bbox[0]/img_width\n",
    "            bbox[1] = bbox[1]/img_height\n",
    "            bbox[2] = bbox[2]/img_width\n",
    "            bbox[3] = bbox[3]/img_height\n",
    "\n",
    "            new_detection = fo.Detection(\n",
    "                label=label, \n",
    "                bounding_box=bbox,\n",
    "                person_id=person_id,\n",
    "                ann_id=ann_id,\n",
    "                mask=cropped_mask,\n",
    "                )\n",
    "            coco_detections.append(new_detection)\n",
    "        sample[\"coco_masks\"] = fo.Detections(detections=coco_detections)\n",
    "\n",
    "## add the masks\n",
    "add_coco_masks_to_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a7cb6d-c307-434a-9500-15ae6a4ccadb",
   "metadata": {},
   "source": [
    "# Import Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57ed90e2-cd2c-4b69-a7a3-d63e9effea28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5  2023-10-21 Python-3.10.9 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m.pt to yolov5m.pt...\n",
      "100%|| 40.8M/40.8M [00:00<00:00, 143MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients, 48.9 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "yolov5 = foz.load_zoo_model('yolov5m-coco-torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5be206d9-d465-4094-8aff-7b8ede9ccf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING 锔 NMS time limit 0.550s exceeded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% || 31702/31702 [24.5m elapsed, 0s remaining, 21.9 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "dataset.apply_model(yolov5, label_field=\"yolov5m\")\n",
    "### Just retain the \"person\" detections\n",
    "people_view_values = dataset.filter_labels(\"yolov5m\", F(\"label\") == \"person\").values(\"yolov5m\")\n",
    "dataset.set_values(\"yolov5m\", people_view_values)\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22f25c-8c22-4c4a-ae09-05e07ec88e17",
   "metadata": {},
   "source": [
    "# Clip classification model --> Replace with teacher/student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7880f2c8-3d3a-4744-89cb-b69ca608520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get a list of all 52 classes\n",
    "facet_classes = dataset.distinct(\"ground_truth.detections.label\")\n",
    "\n",
    "## instantiate a CLIP model with these classes\n",
    "clip = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    text_prompt=\"A photo of a\",\n",
    "    classes=facet_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "58a70100-8fa6-4b34-a754-f86b4b3a52b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% || 49551/49551 [24.6m elapsed, 0s remaining, 34.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "patch_view = dataset.to_patches(\"ground_truth\")\n",
    "patch_view.apply_model(clip, label_field=\"clip\")\n",
    "dataset.save_view(\"patch_view\", patch_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "339f2b7d-5058-4278-81be-32515887d1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "IOU_THRESHS = np.round(np.arange(0.5, 1.0, 0.05), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14507c5d-78b9-4895-9232-a277c0b61aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_detection_model(dataset, label_field):\n",
    "    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n",
    "    dataset.evaluate_detections(label_field, \"ground_truth\", eval_key=eval_key, classwise=False)\n",
    "    \n",
    "    for sample in dataset.iter_samples(progress=True):\n",
    "        for pred in sample[label_field].detections:\n",
    "            iou_field = f\"{eval_key}_iou\"\n",
    "            if iou_field not in pred:\n",
    "                continue\n",
    "\n",
    "            iou = pred[iou_field]\n",
    "            for it in IOU_THRESHS:\n",
    "                pred[f\"{iou_field}_{str(it).replace('.', '')}\"] = iou >= it\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "02f4e194-37cc-4de9-a76b-51ed0de18ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n",
      " 100% || 31702/31702 [4.6m elapsed, 0s remaining, 65.7 samples/s]       \n",
      " 100% || 31702/31702 [8.1m elapsed, 0s remaining, 49.7 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "_evaluate_detection_model(dataset, 'yolov5m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e682ce1b-787e-47db-95b8-fb17ed5d9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_detection_mAR(sample_collection, label_field):\n",
    "    \"\"\"Computes the mean average recall of the specified detection field.\n",
    "    -- computed as the average over iou thresholds of the recall at\n",
    "    each threshold.\n",
    "    \"\"\"\n",
    "    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n",
    "    iou_recalls = []\n",
    "    for it in IOU_THRESHS:\n",
    "        field_str = f\"{label_field}.detections.{eval_key}_iou_{str(it).replace('.', '')}\"\n",
    "        counts = sample_collection.count_values(field_str)\n",
    "        tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "        recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "        iou_recalls.append(recall)\n",
    "\n",
    "    return np.mean(iou_recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8eb73ee8-28ac-4416-8d7a-791959206e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_detection_mAR(dataset, label_field, concept, attributes):\n",
    "    sub_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_view = sub_view.filter_labels(\"ground_truth\", F(f\"skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_view = sub_view.filter_labels(\"ground_truth\", F(attribute[0]) == attribute[1])\n",
    "    return _compute_detection_mAR(sub_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d651365b-d969-456b-aa02-78a47f6d4d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4259259259259259"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept = 'lawman'\n",
    "attributes = {\"hairtype\": \"straight\", \"haircolor\": \"brown\"}\n",
    "get_concept_attr_detection_mAR(dataset, \"yolov5m\", concept, attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "320aa339-ddd0-4e4d-800e-aa6ac8d31b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_classification_model(dataset, prediction_field):\n",
    "    patch_view = dataset.load_saved_view(\"patch_view\")\n",
    "    eval_key = \"eval_\" + prediction_field\n",
    "    \n",
    "    for sample in patch_view.iter_samples(progress=True):\n",
    "        sample[eval_key] = (\n",
    "            sample.ground_truth.label == sample[prediction_field].label\n",
    "        )\n",
    "        sample.save()\n",
    "    dataset.save_view(\"patch_view\", patch_view, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "42ffe5b3-dfa1-42ad-bb88-52b04622bdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% || 49551/49551 [2.5m elapsed, 0s remaining, 329.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "_evaluate_classification_model(dataset, 'clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f830a851-7ff9-4ee1-9082-3810dc511cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_recall(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field.split(\"_\")[0]\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ae5f107-e21f-46de-a8d3-3a329aa88916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_classification_recall(dataset, label_field, concept, attributes):\n",
    "    patch_view = dataset.load_saved_view(\"patch_view\")\n",
    "    sub_patch_view = patch_view.match(F(\"ground_truth.label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_patch_view = sub_patch_view.match(F(f\"ground_truth.skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_patch_view = sub_patch_view.match(F(f\"ground_truth.{attribute[0]}\") == attribute[1])\n",
    "    return _compute_classification_recall(sub_patch_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62bf03f9-f46d-4ffd-bb0f-e868ad9116ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "attribute = {'hairtype': 'curly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60239f5d-239e-4b3d-af81-a81936cab0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_concept_attr_classification_recall(dataset, \"clip\", concept, attribute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c238e25-c163-4774-82dd-f9a96bb3e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_recall(dataset, label_field, concept, attribute):\n",
    "    if label_field in dataset.get_field_schema().keys():\n",
    "        return get_concept_attr_detection_mAR(dataset, label_field, concept, attribute)\n",
    "    else:\n",
    "        return get_concept_attr_classification_recall(dataset, label_field, concept, attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9936a1d6-090a-4310-9001-de6b5b39c2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5362318840579711"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_concept_attr_classification_recall(dataset, 'clip', 'singer', attrs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e153b95-8f68-4a3c-a562-f1eee4136004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_disparity(dataset, label_field, concept, attribute1, attribute2):\n",
    "    recall1 = get_concept_attr_recall(dataset, label_field, concept, attribute1)\n",
    "    recall2 = get_concept_attr_recall(dataset, label_field, concept, attribute2)\n",
    "    return recall1 - recall2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55758f91-25ff-4b29-80b2-8e82878221af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astronaut: -0.8269230769230769\n",
      "singer: -0.0008051529790660261\n",
      "judge: -0.06666666666666667\n",
      "student: 0.16279069767441856\n"
     ]
    }
   ],
   "source": [
    "attrs1 = {\"perceived_gender_presentation\": \"fem\"}\n",
    "attrs2 = {\"hairtype\": \"straight\"}\n",
    "for concept in [\"astronaut\", \"singer\", \"judge\", \"student\"]:\n",
    "    disparity = compute_disparity(dataset, \"clip\", concept, attrs1, attrs2)     \n",
    "    print(f\"{concept}: {disparity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434a7918-51d0-41f4-a53f-a2324a250437",
   "metadata": {},
   "source": [
    "# Experimenting with uploading custom models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c9a1dc98-17cc-4059-ae0a-df8e32a22ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.utils.torch as fout\n",
    "from torchvision.models import resnet50\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cebd47cf-baf9-45d5-8f69-a4fd3f0fbb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet50(pretrained=True)\n",
    "num_new_classes = 52\n",
    "\n",
    "# Modify the final fully connected (fc) layer\n",
    "# Get the number of input features to the fc layer\n",
    "in_features = model.fc.in_features\n",
    "\n",
    "# Replace the fc layer with a new one for the desired number of classes\n",
    "model.fc = nn.Linear(in_features, num_new_classes)\n",
    "\n",
    "def make_data_loader(image_paths, sample_ids, batch_size):\n",
    "    mean = [0.4914, 0.4822, 0.4465]\n",
    "    std = [0.2023, 0.1994, 0.2010]\n",
    "    transforms = torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.Resize((256, 256)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(mean, std),\n",
    "        ]\n",
    "    )\n",
    "    dataset = fout.TorchImageDataset(\n",
    "        image_paths, sample_ids=sample_ids, transform=transforms\n",
    "    )\n",
    "    return DataLoader(dataset, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fff460c1-0bdf-46f4-949d-a620be5f2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, imgs):\n",
    "    logits = model(imgs).detach().cpu().numpy()\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    odds = np.exp(logits)\n",
    "    confidences = np.max(odds, axis=1) / np.sum(odds, axis=1)\n",
    "    return predictions, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d813418-51c8-442b-b2da-5e58d5d32886",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "batch_size = 5\n",
    "\n",
    "view = dataset.take(num_samples, seed=51)\n",
    "classes = []\n",
    "for i in view.iter_samples():\n",
    "    classes.append(i.ground_truth.detections[0].label)\n",
    "\n",
    "image_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view])\n",
    "data_loader = make_data_loader(image_paths, sample_ids, batch_size)\n",
    "\n",
    "#\n",
    "# Perform prediction and store results in dataset\n",
    "#\n",
    "\n",
    "for imgs, sample_ids in data_loader:\n",
    "    predictions, confidences = predict(model, imgs)\n",
    "\n",
    "    # Add predictions to your FiftyOne dataset\n",
    "    for sample_id, prediction, confidence in zip(\n",
    "        sample_ids, predictions, confidences\n",
    "    ):\n",
    "        sample = dataset[sample_id]\n",
    "        sample[\"resnet_pred\"] = fo.Classification(\n",
    "            label=classes[prediction],\n",
    "            confidence=confidence,\n",
    "        )\n",
    "        sample.save()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "364c1cc6-e500-4a32-9b5b-6425e21864db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_classification_modelr(dataset, prediction_field):\n",
    "    eval_key = \"eval_\" + prediction_field\n",
    "    \n",
    "    for sample in dataset.iter_samples(progress=True):\n",
    "        sample[eval_key] = (\n",
    "            sample.ground_truth.detections[0].label == sample[prediction_field].label\n",
    "        )\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1067a92a-6d53-4d62-b512-cb43083d3777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% || 1000/1000 [15.8s elapsed, 0s remaining, 62.6 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "_evaluate_classification_modelr(view, 'resnet_pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f7f87bbe-6a3c-40da-86b3-0baee1ba3cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_classification_recall(patch_collection, label_field):\n",
    "    eval_key = \"eval_\" + label_field\n",
    "    counts = patch_collection.count_values(eval_key)\n",
    "    tp, fn = counts.get(True, 0), counts.get(False, 0)\n",
    "    recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "204e607b-164f-4326-9971-f09bbd4ce18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_attr_classification_recall(dataset, label_field, concept, attributes):\n",
    "    sub_patch_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == concept)\n",
    "    for attribute in attributes.items():\n",
    "        if \"skin_tone\" in attribute[0]:\n",
    "            sub_patch_view = sub_patch_view.filter_labels('ground_truth', F(f\"skin_tone.{attribute[0]}\") != 0)\n",
    "        else:\n",
    "            sub_patch_view = sub_patch_view.filter_labels('ground_truth', F(f\"{attribute[0]}\") == attribute[1])\n",
    "    return _compute_classification_recall(sub_patch_view, label_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c2980f61-f140-4a86-9e41-fee7161fb3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05194805194805195"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concept = 'lawman'\n",
    "attributes = {\"perceived_gender_presentation\": \"masc\"}\n",
    "\n",
    "get_concept_attr_classification_recall(view, 'resnet_pred', concept, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8fd16a3a-4212-486d-9107-5011a92feae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset:     FACET14\n",
       "Media type:  image\n",
       "Num samples: 25\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    coco_masks:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    yolov5m:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)\n",
       "    eval_yolov5m_tp:  fiftyone.core.fields.IntField\n",
       "    eval_yolov5m_fp:  fiftyone.core.fields.IntField\n",
       "    eval_yolov5m_fn:  fiftyone.core.fields.IntField\n",
       "    resnet_pred:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
       "    eval_resnet_pred: fiftyone.core.fields.BooleanField\n",
       "View stages:\n",
       "    1. Take(size=25, seed=51)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611fae7e-3be8-491b-89f2-ebf589118494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
