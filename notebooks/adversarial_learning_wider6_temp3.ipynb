{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f79206-feac-4302-abb1-d699b6354a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck, ResNet18_Weights, ResNet34_Weights, resnet18\n",
    "from torchvision.datasets import ImageFolder\n",
    "from utils.loss_functions import tkd_kdloss\n",
    "\n",
    "from models_package.models import Teacher, Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3c0ec8-98dc-4933-b9a3-e96fce39c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.001379 # 0.096779\n",
    "num_epochs = 1 # 200\n",
    "temperature = 4.0\n",
    "alpha = 0.9\n",
    "momentum = 0.9\n",
    "num_classes = 30\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "patience = 7  # for early stopping\n",
    "lmda = 0.1 #3\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "\n",
    "class_labels = [0, 1, 3, 4, 6, 7, 11, 15, 17, 18, 19, 20, 22, 25, 27, 28, 30, 31, 33, 35, 36, 37, 39, 43, 44, 50, 51, 54, 57, 58]\n",
    "class_labels_new = torch.tensor([i for i in range(len(class_labels))])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2770eac-10b7-47de-ba06-9aa63cd7ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c922f94-bf69-40d5-bc88-1e6fab4b4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StratifiedBatchSampler:\n",
    "#     \"\"\"Stratified batch sampling\n",
    "#     Provides equal representation of target classes in each batch\n",
    "#     \"\"\"\n",
    "#     def __init__(self, y, batch_size, shuffle=True):\n",
    "#         if torch.is_tensor(y):\n",
    "#             y = y.numpy()\n",
    "#         assert len(y.shape) == 1, 'label array must be 1D'\n",
    "#         n_batches = int(len(y) / batch_size)\n",
    "#         self.skf = StratifiedKFold(n_splits=n_batches, shuffle=shuffle)\n",
    "#         self.X = torch.randn(len(y),1).numpy()\n",
    "#         self.y = y\n",
    "#         self.shuffle = shuffle\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         if self.shuffle:\n",
    "#             self.skf.random_state = torch.randint(0,int(1e8),size=()).item()\n",
    "#         for train_idx, test_idx in self.skf.split(self.X, self.y):\n",
    "#             yield test_idx\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.y)//self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1c84514-66c9-4543-8448-cafa751730ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, ann_files, augs, img_size, dataset):\n",
    "        # Define the original class labels\n",
    "\n",
    "        # Create a mapping from old labels to new labels\n",
    "        self.label_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted(class_labels))}\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.ann_files = ann_files\n",
    "        self.augment = self.augs_function(augs, img_size)\n",
    "        # Initialize transformations directly\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "            ] \n",
    "        )\n",
    "        if self.dataset == \"wider\":\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])                ] \n",
    "            )        \n",
    "\n",
    "        self.anns = []\n",
    "        self.load_anns()\n",
    "        print(self.augment)\n",
    "\n",
    "    def augs_function(self, augs, img_size):            \n",
    "        t = []\n",
    "        if 'randomflip' in augs:\n",
    "            t.append(transforms.RandomHorizontalFlip())\n",
    "        if 'ColorJitter' in augs:\n",
    "            t.append(transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0))\n",
    "        if 'resizedcrop' in augs:\n",
    "            t.append(transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0)))\n",
    "        if 'RandAugment' in augs:\n",
    "            t.append(transforms.RandAugment())\n",
    "\n",
    "        t.append(transforms.Resize((img_size, img_size)))\n",
    "\n",
    "        return transforms.Compose(t)\n",
    "    \n",
    "    def load_anns(self):\n",
    "        self.anns = []\n",
    "        for ann_file in self.ann_files:\n",
    "            json_data = json.load(open(ann_file, \"r\"))\n",
    "            self.anns += json_data['images']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Make sure the index is within bounds\n",
    "        idx = idx % len(self)\n",
    "        ann = self.anns[idx]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to open the image file\n",
    "            img = Image.open(f'WIDER/Image/{ann[\"file_name\"]}').convert(\"RGB\")\n",
    "\n",
    "            # If this is the wider dataset, proceed with specific processing\n",
    "            # x, y, w, h = ann['bbox']\n",
    "            # img_area = img.crop([x, y, x+w, y+h])\n",
    "            img_area = self.augment(img)\n",
    "            img_area = self.transform(img_area)\n",
    "            attributes_list = [target['attribute'] for target in ann['targets']]\n",
    "            num_people = len(attributes_list)\n",
    "            attributes_distribution = [max(sum(attribute), 0)/num_people for attribute in zip(*attributes_list)]\n",
    "            # Extract label from image path\n",
    "            img_path = f'WIDER/Image/{ann[\"file_name\"]}'\n",
    "            label = self.extract_label(img_path)  # You might need to implement this method\n",
    "            \n",
    "            return {\n",
    "                \"label\": label,\n",
    "                \"target\": torch.tensor([attributes_distribution[0]], dtype=torch.float32),\n",
    "                \"img\": img_area\n",
    "            }\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            # If any error occurs during the processing of an image, log the error and the index\n",
    "            print(f\"Error processing image at index {idx}: {e}\")\n",
    "            # Instead of returning None, raise the exception\n",
    "            raise\n",
    "\n",
    "    def extract_label(self, img_path):\n",
    "        original_label = None\n",
    "    \n",
    "        if \"WIDER/Image/train\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/train/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/test\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/test/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/val\" in img_path:  # Handle validation images\n",
    "            label_str = img_path.split(\"WIDER/Image/val/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "    \n",
    "        if original_label is not None:\n",
    "            remapped_label = self.label_mapping[original_label]\n",
    "            return remapped_label\n",
    "        else:\n",
    "            raise ValueError(f\"Label could not be extracted from path: {img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2490c2-046a-4a19-9717-642adbabb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = ['data/wider/trainval_wider.json']\n",
    "test_file = ['data/wider/test_wider.json']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "012d6bd8-61d5-4a20-845b-689234718508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # Filter out any None items in the batch\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    # If after filtering the batch is empty, handle this case by either returning an empty tensor or raising an exception\n",
    "    if len(batch) == 0:\n",
    "        # Option 1: Return a placeholder tensor (adapt the shape to match your data)\n",
    "        # return torch.tensor([]), torch.tensor([])\n",
    "        # Option 2: Raise an exception\n",
    "        raise ValueError(\"Batch is empty after filtering out None items.\")\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe2adb5-687d-4055-a108-a9c041836b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = DataSet(train_file, augs = ['RandAugment'], img_size = 226, dataset = 'wider')\n",
    "# test_dataset = DataSet(test_file, augs = [], img_size = 226, dataset = 'wider')\n",
    "\n",
    "\n",
    "# trainloader = DataLoader(train_dataset, \n",
    "#                          batch_sampler=StratifiedBatchSampler(torch.tensor([train_dataset[i]['label'] for i in range(len(train_dataset))]), batch_size=batch_size),\n",
    "#                          num_workers=num_workers, collate_fn=custom_collate)\n",
    "# testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ed9413-2d40-49b9-ba36-f1d43cc95ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandAugment(num_ops=2, magnitude=9, num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataSet(train_file, augs = ['RandAugment'], img_size = 226, dataset = 'wider')\n",
    "test_dataset = DataSet(test_file, augs = [], img_size = 226, dataset = 'wider')\n",
    "\n",
    "\n",
    "trainloader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=custom_collate)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e14c7fe5-f229-4463-9bd0-de519dbdf2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 class counts:\n",
      "  Class 0: 21 instances\n",
      "  Class 1: 2 instances\n",
      "  Class 2: 8 instances\n",
      "  Class 3: 10 instances\n",
      "  Class 4: 8 instances\n",
      "  Class 5: 8 instances\n",
      "  Class 6: 5 instances\n",
      "  Class 7: 7 instances\n",
      "  Class 8: 6 instances\n",
      "  Class 9: 11 instances\n",
      "  Class 10: 3 instances\n",
      "  Class 11: 8 instances\n",
      "  Class 12: 3 instances\n",
      "  Class 13: 3 instances\n",
      "  Class 14: 2 instances\n",
      "  Class 15: 7 instances\n",
      "  Class 16: 5 instances\n",
      "  Class 17: 10 instances\n",
      "  Class 18: 6 instances\n",
      "  Class 19: 19 instances\n",
      "  Class 20: 5 instances\n",
      "  Class 21: 18 instances\n",
      "  Class 22: 14 instances\n",
      "  Class 23: 10 instances\n",
      "  Class 24: 13 instances\n",
      "  Class 25: 9 instances\n",
      "  Class 26: 9 instances\n",
      "  Class 27: 11 instances\n",
      "  Class 28: 9 instances\n",
      "  Class 29: 6 instances\n",
      "------------------------------\n",
      "Batch 2 class counts:\n",
      "  Class 0: 18 instances\n",
      "  Class 1: 10 instances\n",
      "  Class 2: 7 instances\n",
      "  Class 3: 8 instances\n",
      "  Class 4: 10 instances\n",
      "  Class 5: 4 instances\n",
      "  Class 6: 3 instances\n",
      "  Class 7: 1 instances\n",
      "  Class 8: 10 instances\n",
      "  Class 9: 7 instances\n",
      "  Class 10: 4 instances\n",
      "  Class 11: 7 instances\n",
      "  Class 12: 5 instances\n",
      "  Class 13: 7 instances\n",
      "  Class 14: 2 instances\n",
      "  Class 15: 8 instances\n",
      "  Class 16: 3 instances\n",
      "  Class 17: 8 instances\n",
      "  Class 18: 7 instances\n",
      "  Class 19: 18 instances\n",
      "  Class 20: 14 instances\n",
      "  Class 21: 5 instances\n",
      "  Class 22: 17 instances\n",
      "  Class 23: 11 instances\n",
      "  Class 24: 13 instances\n",
      "  Class 25: 11 instances\n",
      "  Class 26: 16 instances\n",
      "  Class 27: 9 instances\n",
      "  Class 28: 8 instances\n",
      "  Class 29: 5 instances\n",
      "------------------------------\n",
      "Batch 3 class counts:\n",
      "  Class 0: 15 instances\n",
      "  Class 1: 3 instances\n",
      "  Class 2: 12 instances\n",
      "  Class 3: 6 instances\n",
      "  Class 4: 9 instances\n",
      "  Class 5: 3 instances\n",
      "  Class 6: 7 instances\n",
      "  Class 7: 1 instances\n",
      "  Class 8: 5 instances\n",
      "  Class 9: 12 instances\n",
      "  Class 10: 2 instances\n",
      "  Class 11: 13 instances\n",
      "  Class 12: 8 instances\n",
      "  Class 13: 8 instances\n",
      "  Class 14: 2 instances\n",
      "  Class 15: 2 instances\n",
      "  Class 16: 5 instances\n",
      "  Class 17: 9 instances\n",
      "  Class 18: 7 instances\n",
      "  Class 19: 31 instances\n",
      "  Class 20: 5 instances\n",
      "  Class 21: 11 instances\n",
      "  Class 22: 14 instances\n",
      "  Class 23: 10 instances\n",
      "  Class 24: 8 instances\n",
      "  Class 25: 8 instances\n",
      "  Class 26: 13 instances\n",
      "  Class 27: 10 instances\n",
      "  Class 28: 9 instances\n",
      "  Class 29: 8 instances\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Iterate through batches in the trainloader\n",
    "for batch_idx, batch in enumerate(trainloader):\n",
    "    # Only process the first 3 batches\n",
    "    if batch_idx >= 3:\n",
    "        break\n",
    "\n",
    "    # Extract labels for the current batch\n",
    "    batch_labels = batch['label']\n",
    "\n",
    "    # Convert batch_labels to a list if it's not already (e.g., if it's a tensor)\n",
    "    if not isinstance(batch_labels, list):\n",
    "        batch_labels = batch_labels.tolist()\n",
    "\n",
    "    # Count the frequency of each label in this batch\n",
    "    label_counts = Counter(batch_labels)\n",
    "\n",
    "    # Print the label counts for this batch\n",
    "    print(f\"Batch {batch_idx + 1} class counts:\")\n",
    "    for label in range(30):  # Assuming classes are labeled from 0 to 29\n",
    "        print(f\"  Class {label}: {label_counts[label]} instances\")\n",
    "    print(\"-\" * 30)  # Just a separator for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f42f0ac9-3af2-4990-a8c1-f7f2af96c03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10324\n",
      "3453\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66cdd881-3047-4bab-84c8-9ceaf347b958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 0,\n",
       " 'target': tensor([0.6250]),\n",
       " 'img': tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          ...,\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       " \n",
       "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       " \n",
       "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          ...,\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ead33f3-32b2-46f1-96aa-feb4aac0128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_labels = []\n",
    "\n",
    "# # Iterate through the DataLoader to collect all labels\n",
    "# for batch in trainloader:\n",
    "#     labels = batch['label'].numpy()  # Convert to NumPy array if not already\n",
    "#     all_train_labels.extend(labels)\n",
    "\n",
    "# # Convert the list to a NumPy array\n",
    "# all_train_labels = np.array(all_train_labels)\n",
    "\n",
    "# # Find the unique classes and their frequencies\n",
    "# unique_classes, class_frequencies = np.unique(all_train_labels, return_counts=True)\n",
    "# normalized_class_weights = torch.from_numpy(class_frequencies).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f761db95-0936-4f2e-8005-7ed3079237e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# def plot_label_frequencies(train_labels, class_names):\n",
    "#     # Count the occurrences of each label in the training set\n",
    "#     train_label_counts = np.bincount(train_labels)\n",
    "\n",
    "#     # Count the occurrences of each label in the test set\n",
    "\n",
    "#     # Create a bar plot\n",
    "#     fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#     bar_width = 0.35\n",
    "#     index = np.arange(len(class_names))\n",
    "\n",
    "#     # Plot training set frequencies\n",
    "#     train_bars = ax.bar(index, train_label_counts, bar_width, label='Train Set')\n",
    "\n",
    "#     # Plot test set frequencies\n",
    "\n",
    "#     # Add labels, title, and legend\n",
    "#     ax.set_xlabel('Class')\n",
    "#     ax.set_ylabel('Frequency')\n",
    "#     ax.set_title('Label Frequencies in Train Sets')\n",
    "#     ax.set_xticks(index + bar_width / 2)\n",
    "#     ax.set_xticklabels(class_names)\n",
    "#     ax.legend()\n",
    "\n",
    "#     # Show the plot\n",
    "#     plt.show()\n",
    "\n",
    "# # Extract labels from the data\n",
    "# # train_labels = [train_dataset[i]['label'] for i in range(len(train_dataset))]\n",
    "# # test_labels = [test_dataset[i]['label'] for i in range(len(test_dataset))]\n",
    "\n",
    "# # List of class names\n",
    "# class_names = [f'Class {i+1}' for i in range(1, len(set(all_train_labels)) + 1)]\n",
    "\n",
    "# # Plot label frequencies\n",
    "# plot_label_frequencies(all_train_labels, class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d18d4b-a3c0-4c5b-ad75-e74b6b1b7296",
   "metadata": {},
   "source": [
    "# Start Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c941e9-954c-4d7b-932e-06d3297c02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    data = next(dataiter)\n",
    "    inputs = data['img']\n",
    "    \n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher(inputs)\n",
    "    teacher_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        student_outputs = student(inputs)\n",
    "    student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n",
    "\n",
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs)\n",
    "            student_outputs = student(inputs)\n",
    "            \n",
    "        teacher_preds = torch.argmax(teacher_outputs, dim=1).cpu().numpy()\n",
    "        student_preds = torch.argmax(student_outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(teacher_preds)\n",
    "        all_student_preds.append(student_preds)\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'all_labels': all_labels,\n",
    "        'all_teacher_preds': all_teacher_preds,\n",
    "        'all_student_preds': all_student_preds\n",
    "    }\n",
    "    \n",
    "def plot_comparison(labels, teacher_values, student_values, title, ylabel):\n",
    "    # Convert parameter count to millions\n",
    "    if 'Parameter Count' in title or 'Parameter Count' in ylabel:\n",
    "        teacher_values = [value / 1e6 for value in teacher_values]\n",
    "        student_values = [value / 1e6 for value in student_values]\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, teacher_values, width, label='Teacher')\n",
    "    rects2 = ax.bar(x + width/2, student_values, width, label='Student')\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c37c867-c1d1-438c-ac22-755904ce121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive_parity(actual_labels, y_pred, gender_labels, num_classes):\n",
    "    # Ensure actual_labels is an array of integers representing class labels\n",
    "    actual_labels = np.array(actual_labels, dtype=int)\n",
    "\n",
    "    # Convert gender_labels to binary (0 for female, 1 for male)\n",
    "    binary_gender_labels = (gender_labels >= 0.5).astype(int)\n",
    "\n",
    "    # Initialize arrays to store true positive rates for each gender and class\n",
    "    female_true_positive_rates = np.zeros(num_classes)\n",
    "    male_true_positive_rates = np.zeros(num_classes)\n",
    "\n",
    "    for class_label in range(num_classes):\n",
    "        # Identify indices corresponding to female and male for the current class\n",
    "        female_indices = np.logical_and(binary_gender_labels == 0, actual_labels == class_label)\n",
    "        male_indices = np.logical_and(binary_gender_labels == 1, actual_labels == class_label)\n",
    "\n",
    "        # Calculate true positive rates for the current class\n",
    "        female_true_positive_rates[class_label] = (\n",
    "            np.sum((y_pred[female_indices] == class_label) & (actual_labels[female_indices] == class_label)) /\n",
    "            np.sum(actual_labels == class_label)\n",
    "        )\n",
    "\n",
    "        male_true_positive_rates[class_label] = (\n",
    "            np.sum((y_pred[male_indices] == class_label) & (actual_labels[male_indices] == class_label)) /\n",
    "            np.sum(actual_labels == class_label)\n",
    "        )\n",
    "\n",
    "    # Calculate the absolute difference in true positive rates between male and female for each class\n",
    "    true_positive_parity_per_class = np.abs(female_true_positive_rates - male_true_positive_rates)\n",
    "\n",
    "    # Average over all classes to get an overall measure\n",
    "    true_positive_parity_average = np.mean(true_positive_parity_per_class)\n",
    "\n",
    "    return true_positive_parity_average\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b780bf22-388c-425f-9e6a-27b59ff35f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecallDifferenceCalculator:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cumulative_true_positives = np.zeros((self.num_classes, 2))  # Two columns for male and female\n",
    "        self.cumulative_actual_positives = np.zeros((self.num_classes, 2))\n",
    "\n",
    "    def update(self, actual_labels, y_pred, gender_labels, batch_number):\n",
    "        actual_labels = np.array(actual_labels.cpu(), dtype=int)\n",
    "        y_pred = np.array(y_pred.cpu(), dtype=int)\n",
    "        gender_labels = np.array(gender_labels.cpu())  # Gender labels are continuous\n",
    "\n",
    "        # Classify gender based on threshold\n",
    "        female_indices = gender_labels <= 0.5\n",
    "        male_indices = gender_labels > 0.5\n",
    "\n",
    "        print(f\"Batch {batch_number}: True Positives by Class\")\n",
    "        for class_label in range(self.num_classes):\n",
    "            # Identifying indices for female and male for the specific class\n",
    "            indices_female_class = np.logical_and(female_indices, actual_labels == class_label)\n",
    "            indices_male_class = np.logical_and(male_indices, actual_labels == class_label)\n",
    "\n",
    "            # Calculating true positives\n",
    "            true_positives_female = np.logical_and(indices_female_class, y_pred == class_label)\n",
    "            true_positives_male = np.logical_and(indices_male_class, y_pred == class_label)\n",
    "\n",
    "            # Debugging prints\n",
    "            print(f\"Batch {batch_number}, Class {class_label}:\")\n",
    "            print(f\"  Female: TP - {np.sum(true_positives_female)}, Total - {np.sum(indices_female_class)}\")\n",
    "            print(f\"  Male: TP - {np.sum(true_positives_male)}, Total - {np.sum(indices_male_class)}\")\n",
    "\n",
    "            # Update cumulative counts\n",
    "            self.cumulative_true_positives[class_label, 0] += np.sum(true_positives_female)\n",
    "            self.cumulative_true_positives[class_label, 1] += np.sum(true_positives_male)\n",
    "            self.cumulative_actual_positives[class_label, 0] += np.sum(indices_female_class)\n",
    "            self.cumulative_actual_positives[class_label, 1] += np.sum(indices_male_class)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def compute_recall_difference(self):\n",
    "        recalls = np.divide(\n",
    "            self.cumulative_true_positives,\n",
    "            self.cumulative_actual_positives,\n",
    "            out=np.zeros_like(self.cumulative_true_positives, dtype=float),\n",
    "            where=self.cumulative_actual_positives != 0\n",
    "        )\n",
    "\n",
    "        # Calculate the difference in recall between the demographic groups for each class\n",
    "        recall_differences = recalls[:, 1] - recalls[:, 0]  # Assuming 1 is male and 0 is female\n",
    "\n",
    "        # Pair each class label with its recall difference\n",
    "        recall_diff_by_class = {label: diff for label, diff in enumerate(recall_differences)}\n",
    "\n",
    "        return recall_diff_by_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a6b084d-9c3f-43cf-8460-8de4ac776a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RecallDifferenceCalculator:\n",
    "#     def __init__(self, num_classes):\n",
    "#         self.num_classes = num_classes\n",
    "#         self.reset()\n",
    "\n",
    "#     def reset(self):\n",
    "#         self.cumulative_true_positives = np.zeros(self.num_classes)\n",
    "#         self.cumulative_actual_positives = np.zeros(self.num_classes)\n",
    "#         self.cumulative_predicted_positives = np.zeros(self.num_classes)\n",
    "\n",
    "#     def update(self, actual_labels, y_pred, gender_labels):\n",
    "#         # Ensure actual_labels is an array of integers representing class labels\n",
    "#         actual_labels = np.array(actual_labels.cpu(), dtype=float)\n",
    "#         y_pred = np.array(y_pred.cpu(), dtype=int)\n",
    "#         # Convert gender_labels to binary (0 for female, 1 for male)\n",
    "#         gender_labels = np.array(gender_labels.cpu(), dtype=float)\n",
    "#         binary_gender_labels = (gender_labels >= 0.5).astype(int)\n",
    "\n",
    "#         for class_label in range(self.num_classes):\n",
    "#             # Identify indices corresponding to female and male for the current class\n",
    "#             female_indices = np.logical_and(binary_gender_labels == 0, actual_labels == class_label)\n",
    "#             male_indices = np.logical_and(binary_gender_labels == 1, actual_labels == class_label)\n",
    "\n",
    "#             # Update cumulative statistics\n",
    "#             self.cumulative_true_positives[class_label] += (\n",
    "#                 np.sum(np.where(np.logical_and(female_indices, actual_labels == class_label),\n",
    "#                                 y_pred, 0) == class_label) +\n",
    "#                 np.sum(np.where(np.logical_and(male_indices, actual_labels == class_label),\n",
    "#                                 y_pred, 0) == class_label)\n",
    "#             )\n",
    "\n",
    "\n",
    "#             self.cumulative_actual_positives[class_label] += np.sum(actual_labels == class_label)\n",
    "#             self.cumulative_predicted_positives[class_label] += np.sum(y_pred == class_label)\n",
    "\n",
    "#     def compute_recall_difference(self):\n",
    "#         # Calculate recall for each class\n",
    "#         class_recall = np.divide(self.cumulative_true_positives, self.cumulative_actual_positives, \n",
    "#                                  where=self.cumulative_actual_positives != 0, out=np.zeros_like(self.cumulative_true_positives))\n",
    "    \n",
    "#         # Calculate the difference in recall between male and female for each class\n",
    "#         recall_difference_per_class = class_recall[1] - class_recall[0]\n",
    "    \n",
    "#         # Compute the average recall difference across all classes\n",
    "#         # average_recall_difference = np.mean(recall_difference_per_class)\n",
    "    \n",
    "#         return recall_difference_per_class\n",
    "\n",
    "\n",
    "# recalc = RecallDifferenceCalculator(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c6b93cb-37c9-448f-a8d9-424d3c4d40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "###################### Testing 1 ######################\n",
    "# Create instances of your models\n",
    "# teacher_model = torchvision.models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1).to(device)\n",
    "# teacher_model.fc = nn.Linear(512,30)\n",
    "student_model = torchvision.models.resnet18(weights=None).to(device)\n",
    "student_model.fc = nn.Linear(512,30)\n",
    "\n",
    "\n",
    "# Load teacher\n",
    "teacher_model = torch.load('teacher_model_ckd_prof.pth')\n",
    "teacher_model.load_state_dict(torch.load('teacher_model_weights_ckd_prof_checkpoint.pth'))\n",
    "teacher_model = teacher_model.to(device)\n",
    "\n",
    "# # Load the studnet\n",
    "# student_model = torch.load('student_model_ckd_prof.pth')\n",
    "# student_model.load_state_dict(torch.load('student_model_weights_ckd_prof_checkpoint.pth'))\n",
    "# student_model = student_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "836f2993-0505-47c6-85ee-2ad2f45db946",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adversary(nn.Module):\n",
    "    def __init__(self, identity_labels=1):\n",
    "        super(Adversary, self).__init__()\n",
    "\n",
    "        self.a1 = nn.Linear(512, 64)\n",
    "        self.a2 = nn.Linear(64, identity_labels)\n",
    "        nn.init.xavier_normal_(self.a1.weight)\n",
    "        nn.init.xavier_normal_(self.a2.weight)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        input_ids = input_ids.view(input_ids.size(0), -1)\n",
    "        adversary = F.relu(self.a1(input_ids))\n",
    "        adversary_output = self.a2(adversary)\n",
    "        return adversary_output\n",
    "\n",
    "adv = Adversary()\n",
    "adv = adv.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9de9c1c-649c-475f-9f52-c85a19d61bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modules=list(student_model.children())[:-1]\n",
    "student_features=nn.Sequential(*modules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7b4731c-f105-4192-9650-6587c46f6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler for the student model\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Optimizer and scheduler for the teacher model\n",
    "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "teacher_scheduler = torch.optim.lr_scheduler.StepLR(teacher_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "optimizer_adv = optim.Adam(adv.parameters(), lr=learning_rate)\n",
    "\n",
    "# Instantiate the model and the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "adv_criterion = nn.MSELoss()\n",
    "# print(\"Adversarial Criterion:\", adv_criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc1d2fe6-54a9-46eb-9db0-21691da79f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_student(student, teacher, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs, patience=5):\n",
    "    teacher.eval()\n",
    "    teacher.to(device)\n",
    "    best_val_loss = float('inf')  \n",
    "    patience_counter = 0 \n",
    "    student_epoch_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        student.train()\n",
    "        student.to(device)\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        # epoch_disparity = 0.0\n",
    "        \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student(inputs)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "\n",
    "            \n",
    "            ce_loss = criterion(student_outputs, labels)\n",
    "            kd_loss = tkd_kdloss(student_outputs, teacher_outputs, temperature=temperature)  # Make sure this returns a scalar\n",
    "            \n",
    "            # If not scalar, sum up to make sure the loss is scalar\n",
    "            if kd_loss.ndim != 0:\n",
    "                kd_loss = kd_loss.sum()\n",
    "                \n",
    "            # Now combine the losses\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        print(f'*******Epoch {epoch}: loss - {epoch_loss}')\n",
    "        student_epoch_losses.append(epoch_loss)\n",
    "\n",
    "    return student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbefb9c9-e67a-4585-9d33-e807c114267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_adversary(adv, student, optimizer, trainloader, adv_criterion, epochs):\n",
    "  \n",
    "  pretrain_adversary_loss = 0\n",
    "  steps = 0\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_batches = 0\n",
    "    for i, data in enumerate(tqdm(trainloader)): # starting from the 0th batch\n",
    "        # get the inputs and labels\n",
    "        inputs = data['img'].to(device)\n",
    "        labels = data['label'].to(device)\n",
    "        targets = data['target'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        classifier_prev_output = student_features(inputs)\n",
    "        adversary_output = adv(classifier_prev_output)\n",
    "        # print(\"Adversary output sample:\", adversary_output[0])\n",
    "        adversary_loss = adv_criterion(adversary_output, targets)\n",
    "        adversary_loss.backward() # back prop\n",
    "        optimizer.step()\n",
    "        pretrain_adversary_loss += adversary_loss.item()\n",
    "        epoch_loss += adversary_loss.item()\n",
    "        epoch_batches += 1\n",
    "        steps += 1\n",
    "\n",
    "    print(\"Average Pretrain Adversary epoch loss: \", epoch_loss/epoch_batches)\n",
    "\n",
    "  return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b877954b-483b-455d-b5a2-211887f49d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain_student(student_model, teacher_model, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs=3, patience=5)\n",
    "# pretrain_adversary(adv, student_model, optimizer, trainloader, adv_criterion, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3b7719f-4d65-4601-8e1d-1015de2201cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### finding the optimal learning rate\n",
    "# def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, num_epochs=5, lr_range=(1e-4, 1e-1), plot_loss=True):\n",
    "#     model.train()\n",
    "#     model.to(device)\n",
    "#     lr_values = np.logspace(np.log10(lr_range[0]), np.log10(lr_range[1]), num_epochs * len(trainloader))  # Generate learning rates for each batch\n",
    "#     lr_iter = iter(lr_values)\n",
    "#     losses = []\n",
    "#     lrs = []\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         for i, batch in enumerate(tqdm(trainloader)):\n",
    "#             lr = next(lr_iter)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = lr  # Set new learning rate\n",
    "            \n",
    "#             inputs, labels = batch['img'].to(device), batch['label'].to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             losses.append(loss.item())\n",
    "#             lrs.append(lr)\n",
    "    \n",
    "#     # Calculate the derivative of the loss\n",
    "#     loss_derivative = np.gradient(losses)\n",
    "    \n",
    "#     # Find the learning rate corresponding to the minimum derivative (steepest decline)\n",
    "#     best_lr_index = np.argmin(loss_derivative)\n",
    "#     best_lr = lrs[best_lr_index]\n",
    "    \n",
    "#     if plot_loss:\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.figure()\n",
    "#         plt.plot(lrs, losses)\n",
    "#         plt.xscale('log')\n",
    "#         plt.xlabel('Learning Rate')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title('Learning Rate Range Test')\n",
    "#         plt.axvline(x=best_lr, color='red', linestyle='--', label=f'Best LR: {best_lr}')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "    \n",
    "#     print(f'Best learning rate: {best_lr}')\n",
    "#     return best_lr\n",
    "\n",
    "# ############# input ############## \n",
    "# best_lr = train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs=3)  \n",
    "# print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "411c4f7a-d0b4-478f-9fa5-5e152e08ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(losses):\n",
    "    epochs = range(1, len(losses) + 1)\n",
    "    plt.plot(epochs, losses)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "286386b2-d6e9-447e-95b5-b3dd8f18010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the teacher model\n",
    "def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, epochs, patience=5):\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    epoch_losses = [] \n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        model.to(device)\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "\n",
    "        epoch_loss /= num_batches  \n",
    "        epoch_losses.append(epoch_loss)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_val_loss = 0.0\n",
    "        num_batches = 0  \n",
    "        with torch.no_grad():\n",
    "            for val_data in tqdm(testloader):\n",
    "                val_inputs = val_data['img'].to(device)\n",
    "                val_labels = val_data['label'].to(device)\n",
    "    \n",
    "                # Forward pass for validation\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = criterion(val_outputs, val_labels)\n",
    "                total_val_loss += val_loss.item()\n",
    "    \n",
    "                # Compute the validation accuracy\n",
    "                _, predicted = torch.max(val_outputs, 1)\n",
    "                total_samples += val_labels.size(0)\n",
    "                total_correct += (predicted == val_labels).sum().item()\n",
    "                num_batches += 1\n",
    "            total_val_loss /= num_batches\n",
    "            val_losses.append(total_val_loss)\n",
    "            accuracy = total_correct / total_samples\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss: .6f} Val Loss: {total_val_loss: .6f}| Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "            \n",
    "        # Check for early stopping\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            patience_counter = 0 \n",
    "            torch.save(model.state_dict(), f'teacher_model_weights_ckd_prof_checkpoint.pth')\n",
    "            torch.save(model, f'teacher_model_ckd_prof_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break  \n",
    "\n",
    "        scheduler.step()\n",
    "    plot_loss_curve(val_losses)\n",
    "    print(\"Finished Training Teacher\")\n",
    "\n",
    "\n",
    "def train_adversary(adv, student, optimizer, trainloader, adv_criterion, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_batches = 0\n",
    "        for i, data in enumerate(tqdm(trainloader)): \n",
    "            inputs = data['img'].to(device)\n",
    "            targets = data['target'].to(device)  # Ensure targets are in the range [0, 1] for BCEWithLogitsLoss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Assuming student_features is a function or part of the student model\n",
    "            # that extracts the necessary features for the adversary\n",
    "            classifier_prev_output = student_features(inputs)\n",
    "            adversary_output = adv(classifier_prev_output)\n",
    "\n",
    "            adversary_loss = adv_criterion(adversary_output, targets)\n",
    "            adversary_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += adversary_loss.item()\n",
    "            epoch_batches += 1\n",
    "\n",
    "        print(f\"Average Adversary epoch loss: {epoch_loss/epoch_batches}\")\n",
    "    return adv\n",
    "\n",
    "\n",
    "\n",
    "# Function to train the student model with knowledge distillation\n",
    "def train_student_with_distillation_disparity(student, teacher, adv, trainloader, testloader, criterion, adv_criterion, optimizer, scheduler, device, alpha, temperature, epochs, lmda, patience=5):\n",
    "    teacher.eval()\n",
    "    teacher.to(device)\n",
    "    \n",
    "    # Create an instance of RecallDifferenceCalculator\n",
    "    num_classes = len(class_labels)\n",
    "    recalc = RecallDifferenceCalculator(num_classes)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    student_epoch_losses = []  # List to store loss per epoch\n",
    "    val_losses = []  # List to store validation losses\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        student.to(device)\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for index, data in tqdm(enumerate(trainloader), total=len(trainloader), desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            targets = data['target'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student(inputs)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "                \n",
    "            classifier_prev_output = student_features(inputs)\n",
    "            adversary_output = adv(classifier_prev_output)\n",
    "            adversary_loss = criterion(adversary_output, targets)\n",
    "                \n",
    "            ce_loss = criterion(student_outputs, labels)\n",
    "            kd_loss = tkd_kdloss(student_outputs, teacher_outputs, temperature=temperature)\n",
    "            if kd_loss.ndim != 0:\n",
    "                kd_loss = kd_loss.sum()\n",
    "            \n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss - lmda * adversary_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        student_epoch_losses.append(epoch_loss)\n",
    "\n",
    "        # Validation Loop\n",
    "        student.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        total_val_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for index, val_data in tqdm(enumerate(testloader), total=len(testloader), desc=\"Validation\"):\n",
    "                val_inputs = val_data['img'].to(device)\n",
    "                val_labels = val_data['label'].to(device)\n",
    "                val_targets = val_data['target'].to(device)\n",
    "                \n",
    "                val_student_outputs = student(val_inputs)\n",
    "                with torch.no_grad():\n",
    "                    val_teacher_outputs = teacher(val_inputs)\n",
    "\n",
    "                val_classifier_prev_output = student_features(val_inputs)\n",
    "                val_adversary_output = adv(val_classifier_prev_output)\n",
    "                val_adversary_loss = criterion(val_adversary_output, val_targets)\n",
    "                val_ce_loss = criterion(val_student_outputs, val_labels)\n",
    "                val_kd_loss = tkd_kdloss(val_student_outputs, val_teacher_outputs, temperature=temperature)\n",
    "                if val_kd_loss.ndim != 0:\n",
    "                    val_kd_loss = val_kd_loss.sum()\n",
    "\n",
    "                val_loss = alpha * val_kd_loss + (1 - alpha) * val_ce_loss - lmda * val_adversary_loss\n",
    "                total_val_loss += val_loss.item()\n",
    "    \n",
    "                _, predicted = torch.max(val_student_outputs, 1)\n",
    "                total_samples += val_labels.size(0)\n",
    "                total_correct += (predicted == val_labels).sum().item()\n",
    "                num_batches += 1\n",
    "                recalc.update(val_labels, predicted, val_targets, index)\n",
    "\n",
    "            total_val_loss /= num_batches\n",
    "            val_losses.append(total_val_loss)\n",
    "            accuracy = total_correct / total_samples\n",
    "            recall_difference_epoch = recalc.compute_recall_difference()\n",
    "            print(f\"Epoch {epoch + 1}: Difference in Recall by Class: {recall_difference_epoch}\")\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {epoch_loss: .6f} Val Loss: {total_val_loss: .6f}| Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "        if total_val_loss < best_val_loss:\n",
    "            best_val_loss = total_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(student.state_dict(), f'student_model_weights_ckd_prof_checkpoint.pth')\n",
    "            torch.save(student, f'student_model_ckd_prof_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    plot_loss_curve(val_losses)\n",
    "    print(\"Finished Training Student\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "540397f4-0bb9-4f55-88a5-bdd7671a913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41/41 [01:48<00:00,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Adversary epoch loss: 0.30108657697351965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Adversary(\n",
       "  (a1): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (a2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs=num_epochs)\n",
    "train_adversary(adv, student_model, optimizer, trainloader, adv_criterion, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a06902-9ad4-47a6-9a67-181e2dc43218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  78%|                     | 32/41 [01:35<00:21,  2.43s/it]"
     ]
    }
   ],
   "source": [
    "train_student_with_distillation_disparity(student_model, teacher_model, adv, trainloader, testloader, criterion, adv_criterion, optimizer, scheduler, device, alpha, temperature, num_epochs, lmda=0.25, patience=5)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30aad75-7eb3-4a59-b1ac-dc3582cfa65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Testing 1 ######################\n",
    "# Save the student and teacher model weights and architecture\n",
    "torch.save(student_model.state_dict(), 'student_model_weights_ckd_prof.pth')\n",
    "torch.save(student_model, 'student_model_ckd_prof.pth')\n",
    "print('student weights and architecture saved and exported')\n",
    "\n",
    "torch.save(teacher_model.state_dict(), 'teacher_model_weights_ckd_prof.pth')\n",
    "torch.save(teacher_model, 'teacher_model_ckd_prof.pth')\n",
    "print('teacher weights and architecture saved and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995827c-1726-44e2-89aa-5f346a3d4987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the comparison and plotting functions after training\n",
    "teacher_params, student_params = compare_model_size(teacher_model, student_model)\n",
    "teacher_time, student_time = compare_inference_time(teacher_model, student_model, testloader)\n",
    "performance_metrics = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "\n",
    "# Extracting the metric values for plotting\n",
    "performance_labels = ['accuracy', 'precision', 'recall', 'f1']\n",
    "teacher_performance_values = [performance_metrics['metrics'][metric][0] for metric in performance_labels]\n",
    "student_performance_values = [performance_metrics['metrics'][metric][1] for metric in performance_labels]\n",
    "\n",
    "# Plotting the comparison for performance metrics\n",
    "plot_comparison(performance_labels, teacher_performance_values, student_performance_values, 'Performance Comparison', 'Score')\n",
    "\n",
    "# Plotting the comparison for model size\n",
    "model_size_labels = ['Model Size']\n",
    "teacher_model_size_values = [teacher_params]\n",
    "student_model_size_values = [student_params]\n",
    "plot_comparison(model_size_labels, teacher_model_size_values, student_model_size_values, 'Model Size Comparison', 'Parameter Count (millions)')\n",
    "\n",
    "# Plotting the comparison for inference time\n",
    "inference_time_labels = ['Inference Time']\n",
    "teacher_inference_time_values = [teacher_time]\n",
    "student_inference_time_values = [student_time]\n",
    "plot_comparison(inference_time_labels, teacher_inference_time_values, student_inference_time_values, 'Inference Time Comparison', 'Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289f549-42fc-4c17-ad60-415775d9d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(preds, targets, condition):\n",
    "    \"\"\"\n",
    "    Calculate recall for a given condition in a multi-class setting.\n",
    "\n",
    "    :param preds: Predicted classes.\n",
    "    :param targets: True classes.\n",
    "    :param condition: Boolean tensor indicating the condition (subset) for which to calculate recall.\n",
    "    :return: Recall value.\n",
    "    \"\"\"\n",
    "    if condition.sum() == 0:  # No samples meet the condition\n",
    "        return 0.0\n",
    "\n",
    "    filtered_preds = preds[condition]\n",
    "    filtered_targets = targets[condition]\n",
    "\n",
    "    true_positive = (filtered_preds == filtered_targets).sum().float()\n",
    "    condition_positive = filtered_targets.size(0)\n",
    "\n",
    "    recall = true_positive / condition_positive if condition_positive > 0 else 0.0\n",
    "    return recall\n",
    "    \n",
    "def calculate_weighted_disparity(disparity_sums, counts):\n",
    "    \"\"\"\n",
    "    Calculate weighted disparity for each class-attribute pair.\n",
    "    \"\"\"\n",
    "    weighted_disparities = torch.zeros_like(disparity_sums)\n",
    "    for class_idx in range(disparity_sums.size(0)):\n",
    "        for attr_idx in range(disparity_sums.size(1)):\n",
    "            if counts[class_idx][attr_idx] > 0:\n",
    "                weighted_disparities[class_idx][attr_idx] = disparity_sums[class_idx][attr_idx] / counts[class_idx][attr_idx]\n",
    "            else:\n",
    "                weighted_disparities[class_idx][attr_idx] = 0.0\n",
    "    return weighted_disparities\n",
    "\n",
    "def evaluate_disparity(model, dataloader, num_classes, device):\n",
    "    \"\"\"\n",
    "    Evaluate the disparity on the test data with weighted consideration.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    disparity_sums = None\n",
    "    counts = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['img'].to(device)\n",
    "            targets = batch['label'].to(device)\n",
    "            attributes = batch['target'].to(device)\n",
    "\n",
    "            if disparity_sums is None:\n",
    "                disparity_sums = torch.zeros(num_classes, attributes.size(1), device=device)\n",
    "                counts = torch.zeros(num_classes, attributes.size(1), device=device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for class_idx in range(num_classes):\n",
    "                for attr_idx in range(attributes.size(1)):\n",
    "                    condition_present = (attributes[:, attr_idx] == 1) & (targets == class_idx)\n",
    "                    condition_absent = (attributes[:, attr_idx] == 0) & (targets == class_idx)\n",
    "\n",
    "                    if condition_present.sum() > 0 or condition_absent.sum() > 0:\n",
    "                        recall_present = calculate_recall(preds, targets, condition_present)\n",
    "                        recall_absent = calculate_recall(preds, targets, condition_absent)\n",
    "\n",
    "                        disparity = abs(recall_present - recall_absent)\n",
    "                        count = condition_present.sum() + condition_absent.sum()\n",
    "                        disparity_sums[class_idx][attr_idx] += disparity * count\n",
    "                        counts[class_idx][attr_idx] += count\n",
    "\n",
    "    weighted_disparities = calculate_weighted_disparity(disparity_sums, counts)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        for attr_idx in range(attributes.size(1)):\n",
    "            print(f\"Class: {class_idx}, Attr: {attr_idx}, Weighted Disparity: {weighted_disparities[class_idx][attr_idx]}\")\n",
    "\n",
    "    weighted_average = weighted_disparities.flatten()\n",
    "    weighted_average = weighted_average.sum()/weighted_average.numel()\n",
    "    return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3dd66-85c6-4a91-9c4d-f20fbc011342",
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity = evaluate_disparity(student_model, testloader, num_classes=num_classes, device=device)\n",
    "print(f'Average recall disparity across all attributes and classes: {disparity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa18cfd-8f83-4c3f-bbe9-c834403adf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names_new = [f\"Class {label}\" for label in range(30)]\n",
    "def plot_prediction_distribution_and_confusion_matrix(labels, preds, class_names):\n",
    "    # Plotting the distribution of predictions\n",
    "    sns.countplot(x=preds)\n",
    "    plt.title('Distribution of Predictions')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Computing the confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names_new, columns=class_names_new)\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='g')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report for detailed metrics\n",
    "    print(classification_report(labels, preds, target_names=class_names, zero_division=0))\n",
    "\n",
    "performance_metrics_teacher = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "all_labels = performance_metrics_teacher['all_labels']\n",
    "all_teacher_preds = performance_metrics_teacher['all_teacher_preds']\n",
    "all_student_preds = performance_metrics_teacher['all_student_preds']\n",
    "\n",
    "# For the Teacher Model\n",
    "plot_prediction_distribution_and_confusion_matrix(all_labels, all_teacher_preds, class_names_new)\n",
    "\n",
    "# For the Student Model\n",
    "plot_prediction_distribution_and_confusion_matrix(all_labels, all_student_preds, class_names_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ccf26-26ea-468d-877a-d8c24d8b45a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf4430b-9213-444d-9288-b03b6011266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef97ca7-c21c-4463-8845-1d4cf2e45927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules=list(student_model.children())[:-1]\n",
    "# student_features=nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd9fea-f2c5-469c-aaf1-5a045dfb0ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e634a-992d-4ae1-8883-fb8065970cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in student_model.named_parameters():\n",
    "#     if \"weight\" in name:  # assuming the last layer has a weight parameter\n",
    "#         last_layer_hidden_size = param.size(0)  # The first dimension is the hidden size\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05762185-9db6-46c5-93f1-3eb7e52402ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_layer_hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8f21a-ed8f-42bb-9b91-fce9fe3c1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# student_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d76a0f-a3e9-416d-861b-52c733cf9d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
