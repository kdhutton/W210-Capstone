{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50f79206-feac-4302-abb1-d699b6354a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import tarfile\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import s3fs\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset, Dataset, DataLoader\n",
    "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck, ResNet18_Weights, ResNet34_Weights, resnet18\n",
    "from torchvision.datasets import ImageFolder\n",
    "from data.data_loader import load_cifar10, load_cifar100, load_imagenet, load_prof\n",
    "from utils.loss_functions import tkd_kdloss\n",
    "\n",
    "from models_package.models import Teacher, Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dbaca6c-d58e-4940-8897-de1eb935a366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access_key = getpass.getpass(\"Enter your access: \")\n",
    "\n",
    "# secret_key = password = getpass.getpass(\"Enter your secret: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0d4329-0939-445e-98a6-685ab0d95632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run once to get images onto EC2\n",
    "\n",
    "# wider_dir = './WIDER'\n",
    "# if not os.path.exists(wider_dir):\n",
    "#     os.makedirs(wider_dir)\n",
    "\n",
    "# # Specify your S3 bucket and file path\n",
    "# bucket_name = '210bucket'\n",
    "# s3_file_path = 'wider_attribute_image.tgz'\n",
    "\n",
    "# # Initialize an S3 filesystem\n",
    "# s3 = s3fs.S3FileSystem(key=access_key, secret=secret_key)\n",
    "\n",
    "# # Download the .tgz file from S3\n",
    "# with s3.open(f\"{bucket_name}/{s3_file_path}\", 'rb') as s3_file:\n",
    "#     with tarfile.open(fileobj=s3_file, mode=\"r:gz\") as tar:\n",
    "#         # Specify the destination directory where you want to store the extracted contents\n",
    "#         extract_dir = wider_dir # Change this to your desired directory\n",
    "#         tar.extractall(path=extract_dir)\n",
    "\n",
    "# print(\"File downloaded and extracted successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15ac2268-adff-4ea5-beb6-e954594bd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Specify your S3 bucket and directory path\n",
    "# s3_directory_path = 'wider_attribute_annotation/'\n",
    "\n",
    "# local_directory = './WIDER/Annotations'  # Change this to your desired directory\n",
    "\n",
    "# s3_files = s3.ls(f\"{bucket_name}/{s3_directory_path}\")\n",
    "\n",
    "\n",
    "# # Create the local directory if it doesn't exist\n",
    "# os.makedirs(local_directory, exist_ok=True)\n",
    "\n",
    "# # Download each file from the S3 directory to the local directory\n",
    "# for s3_file in s3_files:\n",
    "#     # Get the filename from the S3 file path\n",
    "#     filename = os.path.basename(s3_file)\n",
    "    \n",
    "#     # Download the file to the local directory\n",
    "#     local_path = os.path.join(local_directory, filename)\n",
    "#     with s3.open(s3_file, 'rb') as s3_file_obj:\n",
    "#         with open(local_path, 'wb') as local_file:\n",
    "#             local_file.write(s3_file_obj.read())\n",
    "\n",
    "# print(\"Files downloaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1ab952-fdf3-4abd-b899-5a7e6c68ee68",
   "metadata": {},
   "source": [
    "# Load WIDER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5caa8a7b-2a7e-4a8e-a1ae-2a3c60675232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_wider(tag, value, data_path):\n",
    "#     img_path = os.path.join(data_path, \"Image\")\n",
    "#     ann_path = os.path.join(data_path, \"Annotations\")\n",
    "#     ann_file = os.path.join(ann_path, \"wider_attribute_{}.json\".format(tag))\n",
    "\n",
    "#     data = json.load(open(ann_file, \"r\"))\n",
    "\n",
    "#     final = []\n",
    "#     image_list = data['images']\n",
    "#     for image in image_list:\n",
    "#         for person in image[\"targets\"]: # iterate over each person\n",
    "#             tmp = {}\n",
    "#             tmp['img_path'] = os.path.join(img_path, image['file_name'])\n",
    "#             tmp['bbox'] = person['bbox']\n",
    "#             attr = person[\"attribute\"]\n",
    "#             for i, item in enumerate(attr):\n",
    "#                 if item == -1:\n",
    "#                     attr[i] = 0\n",
    "#                 if item == 0:\n",
    "#                     attr[i] = value  # pad un-specified samples\n",
    "#                 if item == 1:\n",
    "#                     attr[i] = 1\n",
    "#             tmp[\"target\"] = attr\n",
    "#             final.append(tmp)\n",
    "\n",
    "#     json.dump(final, open(\"data/wider/{}_wider.json\".format(tag), \"w\"))\n",
    "#     print(\"data/wider/{}_wider.json\".format(tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4aa8bdb-873b-4fed-95f6-fb79b3c14b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #run once\n",
    "# if not os.path.exists(\"data/wider\"):\n",
    "#     os.makedirs(\"data/wider\")\n",
    "\n",
    "# # 0 (zero) means negative, we treat un-specified attribute as negative in the trainval set\n",
    "# make_wider(tag='trainval', value=0, data_path='WIDER') \n",
    "# make_wider(tag='test', value=99, data_path='WIDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c0ec8-98dc-4933-b9a3-e96fce39c6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01379 # 0.096779\n",
    "num_epochs = 1 # 200\n",
    "temperature = 4.0\n",
    "alpha = 0.9\n",
    "momentum = 0.9\n",
    "# num_classes = 61\n",
    "num_classes = 30\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "beta = 0.25\n",
    "\n",
    "# new parameters\n",
    "# lr_input = 0.1\n",
    "# momentum_input = 0.9\n",
    "weight_decay_input = 5e-4\n",
    "# epochs = 20\n",
    "# T = 4.0 # temperatureture\n",
    "# alpha = 0.9\n",
    "patience = 5  # for early stopping\n",
    "\n",
    "batch_size = 372\n",
    "num_workers = 4\n",
    "\n",
    "class_labels = [0, 1, 3, 4, 6, 7, 11, 15, 17, 18, 19, 20, 22, 25, 27, 28, 30, 31, 33, 35, 36, 37, 39, 43, 44, 50, 51, 54, 57, 58]\n",
    "class_frequencies = np.array([3782, 462, 892, 854, 1160, 1013, 1162, 302, 901, 611, 317, 2272, 563, 546, 155, 762, 346, 520, 363, 2894, 1067, 1164, 950, 477, 1211, 938, 734, 994, 418, 1349])\n",
    "\n",
    "# Total number of samples\n",
    "total_samples = sum(class_frequencies)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = total_samples / class_frequencies\n",
    "\n",
    "# Normalize class weights\n",
    "normalized_class_weights = class_weights / sum(class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2770eac-10b7-47de-ba06-9aa63cd7ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "normalized_class_weights = torch.from_numpy(normalized_class_weights).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1c84514-66c9-4543-8448-cafa751730ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, ann_files, augs, img_size, dataset, undersample=False):\n",
    "        # Define the original class labels\n",
    "\n",
    "        # Create a mapping from old labels to new labels\n",
    "        self.label_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted(class_labels))}\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.ann_files = ann_files\n",
    "        self.augment = self.augs_function(augs, img_size)\n",
    "        # Initialize transformations directly\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0, 0, 0], std=[1, 1, 1])\n",
    "            ] \n",
    "        )\n",
    "        if self.dataset == \"wider\":\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "                ] \n",
    "            )        \n",
    "\n",
    "        self.anns = []\n",
    "        self.load_anns()\n",
    "        if undersample: \n",
    "            self.undersample_anns()\n",
    "        print(self.augment)\n",
    "\n",
    "    def augs_function(self, augs, img_size):            \n",
    "        t = []\n",
    "        if 'randomflip' in augs:\n",
    "            t.append(transforms.RandomHorizontalFlip())\n",
    "        if 'ColorJitter' in augs:\n",
    "            t.append(transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0))\n",
    "        if 'resizedcrop' in augs:\n",
    "            t.append(transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0)))\n",
    "        if 'RandAugment' in augs:\n",
    "            t.append(RandAugment())\n",
    "\n",
    "        t.append(transforms.Resize((img_size, img_size)))\n",
    "\n",
    "        return transforms.Compose(t)\n",
    "    \n",
    "    def load_anns(self):\n",
    "        self.anns = []\n",
    "        for ann_file in self.ann_files:\n",
    "            json_data = json.load(open(ann_file, \"r\"))\n",
    "            self.anns += json_data\n",
    "\n",
    "    def undersample_anns(self):\n",
    "        # Shuffle annotations before undersampling\n",
    "        random.shuffle(self.anns)\n",
    "\n",
    "        # Count the instances per class\n",
    "        class_counts = {}\n",
    "        for ann in self.anns:\n",
    "            label = self.extract_label(ann['img_path'])  # Assuming this method returns the class label\n",
    "            class_counts[label] = class_counts.get(label, 0) + 1\n",
    "\n",
    "        # Find the minimum class count\n",
    "        min_count = min(class_counts.values())\n",
    "\n",
    "        # Perform undersampling\n",
    "        undersampled_anns = []\n",
    "        current_counts = {label: 0 for label in class_counts}\n",
    "        for ann in self.anns:\n",
    "            label = self.extract_label(ann['img_path'])\n",
    "            if current_counts[label] < min_count:\n",
    "                undersampled_anns.append(ann)\n",
    "                current_counts[label] += 1\n",
    "\n",
    "        # Update the annotations to the undersampled list\n",
    "        self.anns = undersampled_anns\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.anns)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Make sure the index is within bounds\n",
    "        idx = idx % len(self)\n",
    "        ann = self.anns[idx]\n",
    "        \n",
    "        try:\n",
    "            # Attempt to open the image file\n",
    "            img = Image.open(ann[\"img_path\"]).convert(\"RGB\")\n",
    "\n",
    "            # If this is the wider dataset, proceed with specific processing\n",
    "            if self.dataset == \"wider\":\n",
    "                x, y, w, h = ann['bbox']\n",
    "                img_area = img.crop([x, y, x+w, y+h])\n",
    "                img_area = self.augment(img_area)\n",
    "                img_area = self.transform(img_area)\n",
    "                \n",
    "                # Extract label from image path\n",
    "                img_path = ann['img_path']\n",
    "                label = self.extract_label(img_path)  # You might need to implement this method\n",
    "                \n",
    "                return {\n",
    "                    \"label\": label,\n",
    "                    \"target\": torch.Tensor(ann['target']),\n",
    "                    \"img\": img_area\n",
    "                }\n",
    "            \n",
    "            # Else, add handling for other dataset types\n",
    "            else:\n",
    "                # Handle other dataset types here\n",
    "                pass\n",
    "\n",
    "        except Exception as e:\n",
    "            # If any error occurs during the processing of an image, log the error and the index\n",
    "            print(f\"Error processing image at index {idx}: {e}\")\n",
    "            # Instead of returning None, raise the exception\n",
    "            raise\n",
    "\n",
    "    def extract_label(self, img_path):\n",
    "        original_label = None\n",
    "    \n",
    "        if \"WIDER/Image/train\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/train/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/test\" in img_path:\n",
    "            label_str = img_path.split(\"WIDER/Image/test/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "        elif \"WIDER/Image/val\" in img_path:  # Handle validation images\n",
    "            label_str = img_path.split(\"WIDER/Image/val/\")[1].split(\"/\")[0]\n",
    "            original_label = int(label_str.split(\"--\")[0])\n",
    "    \n",
    "        if original_label is not None:\n",
    "            remapped_label = self.label_mapping[original_label]\n",
    "            return remapped_label\n",
    "        else:\n",
    "            raise ValueError(f\"Label could not be extracted from path: {img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb1ebc0e-fdab-409a-91df-e8d013b8f46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_images_per_class(annotation_files):\n",
    "#     label_counts = {}\n",
    "\n",
    "#     for ann_file in annotation_files:\n",
    "#         with open(ann_file, 'r') as file:\n",
    "#             annotations = json.load(file)\n",
    "#             for ann in annotations:\n",
    "#                 label_str = ann['img_path'].split('/')[1]  # Extract the label part from the file path\n",
    "#                 label = int(label_str.split(\"--\")[0])  # Extract the numeric part of the label\n",
    "#                 label_counts[label] = label_counts.get(label, 0) + 1\n",
    "\n",
    "#     return label_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea2490c2-046a-4a19-9717-642adbabb732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = ['data/wider/trainval_wider.json']\n",
    "test_file = ['data/wider/test_wider.json']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "012d6bd8-61d5-4a20-845b-689234718508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    # Filter out any None items in the batch\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    # If after filtering the batch is empty, handle this case by either returning an empty tensor or raising an exception\n",
    "    if len(batch) == 0:\n",
    "        # Option 1: Return a placeholder tensor (adapt the shape to match your data)\n",
    "        # return torch.tensor([]), torch.tensor([])\n",
    "        # Option 2: Raise an exception\n",
    "        raise ValueError(\"Batch is empty after filtering out None items.\")\n",
    "    return torch.utils.data.dataloader.default_collate(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fe2adb5-687d-4055-a108-a9c041836b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n",
      "Compose(\n",
      "    Resize(size=(226, 226), interpolation=bilinear, max_size=None, antialias=warn)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = DataSet(train_file, augs = ['randomflip'], img_size = 226, dataset = 'wider', undersample=False)\n",
    "test_dataset = DataSet(test_file, augs = [], img_size = 226, dataset = 'wider', undersample=False)\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=custom_collate)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b89f04c2-de33-431c-bf55-eedb38e8f178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28345"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d18d4b-a3c0-4c5b-ad75-e74b6b1b7296",
   "metadata": {},
   "source": [
    "# Start Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c941e9-954c-4d7b-932e-06d3297c02c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    dataiter = iter(dataloader)\n",
    "    data = next(dataiter)\n",
    "    inputs = data['img']\n",
    "    \n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher(inputs)\n",
    "    teacher_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        student_outputs = student(inputs)\n",
    "    student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n",
    "\n",
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        inputs = batch['img'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(inputs)\n",
    "            student_outputs = student(inputs)\n",
    "            \n",
    "        teacher_preds = torch.argmax(teacher_outputs, dim=1).cpu().numpy()\n",
    "        student_preds = torch.argmax(student_outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Debug: Check if all predictions are the same\n",
    "        print(\"Teacher predictions:\", np.unique(teacher_preds))\n",
    "        print(\"Student predictions:\", np.unique(student_preds))\n",
    "        \n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(teacher_preds)\n",
    "        all_student_preds.append(student_preds)\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    # Debug: Check final label distribution\n",
    "    print(\"Label distribution:\", np.unique(all_labels, return_counts=True))\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'all_labels': all_labels,\n",
    "        'all_teacher_preds': all_teacher_preds,\n",
    "        'all_student_preds': all_student_preds\n",
    "    }\n",
    "    \n",
    "def plot_comparison(labels, teacher_values, student_values, title, ylabel):\n",
    "    # Convert parameter count to millions\n",
    "    if 'Parameter Count' in title or 'Parameter Count' in ylabel:\n",
    "        teacher_values = [value / 1e6 for value in teacher_values]\n",
    "        student_values = [value / 1e6 for value in student_values]\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    rects1 = ax.bar(x - width/2, teacher_values, width, label='Teacher')\n",
    "    rects2 = ax.bar(x + width/2, student_values, width, label='Student')\n",
    "\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4102246d-d7c8-4230-ae3b-735081a608eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load IdenProf dataset\n",
    "# train_path = '/home/ubuntu/capstone/W210-Capstone/notebooks/idenprof/train'\n",
    "# test_path = '/home/ubuntu/capstone/W210-Capstone/notebooks/idenprof/test'\n",
    "# trainloader, testloader  = load_prof(train_path, test_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c6b93cb-37c9-448f-a8d9-424d3c4d40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "###################### Testing 1 ######################\n",
    "# Create instances of your models\n",
    "teacher_model = torchvision.models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1, ).to(device)\n",
    "teacher_model.fc = nn.Linear(512,30)\n",
    "student_model = torchvision.models.resnet18(weights=None).to(device)\n",
    "student_model.fc = nn.Linear(512,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7b4731c-f105-4192-9650-6587c46f6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler for the student model\n",
    "optimizer = optim.SGD(student_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Optimizer and scheduler for the teacher model\n",
    "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "teacher_scheduler = torch.optim.lr_scheduler.StepLR(teacher_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=normalized_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "477780cd-e318-4e47-a35d-103b6e4506ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_disparity_loss(outputs, targets, attributes, class_labels):\n",
    "    \"\"\"\n",
    "    Compute the recall disparity loss.\n",
    "\n",
    "    :param outputs: Tensor of shape (batch_size, num_classes), model's class probabilities or logits.\n",
    "    :param targets: Tensor of shape (batch_size,), true class indices.\n",
    "    :param attributes: Tensor of shape (batch_size, num_attributes), binary attributes for each instance.\n",
    "    :param num_classes: int, number of classes.\n",
    "    \"\"\"\n",
    "    # Ensure we're working with probabilities\n",
    "    probs = torch.softmax(outputs, dim=1)\n",
    "    preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "    # Initialize tensors to store recall for when attributes are present and absent\n",
    "    recall_when_present = torch.zeros(num_classes, attributes.size(1))\n",
    "    recall_when_absent = torch.zeros(num_classes, attributes.size(1))\n",
    "\n",
    "    # Initialize the disparity matrix\n",
    "    disparity = torch.zeros(num_classes, attributes.size(1))\n",
    "    \n",
    "    for class_idx in class_labels:\n",
    "        for attr_idx in range(attributes.size(1)):\n",
    "            # Indices of instances with the current class and attribute present/absent\n",
    "            class_and_attr_present = (targets == class_idx) & (attributes[:, attr_idx] == 1)\n",
    "            class_and_attr_absent = (targets == class_idx) & (attributes[:, attr_idx] == 0)\n",
    "    \n",
    "            # True positives for current class when attribute is present/absent\n",
    "            true_positive_present = ((preds == class_idx) & class_and_attr_present).sum().float()\n",
    "            true_positive_absent = ((preds == class_idx) & class_and_attr_absent).sum().float()\n",
    "    \n",
    "            # Condition positives for current class when attribute is present/absent\n",
    "            condition_positive_present = class_and_attr_present.sum().float()\n",
    "            condition_positive_absent = class_and_attr_absent.sum().float()\n",
    "    \n",
    "            # Check if both denominators are non-zero before calculating recall\n",
    "            if condition_positive_present != 0 and condition_positive_absent != 0:\n",
    "                recall_when_present[class_idx, attr_idx] = true_positive_present / condition_positive_present\n",
    "                recall_when_absent[class_idx, attr_idx] = true_positive_absent / condition_positive_absent\n",
    "    \n",
    "                # Disparity is the absolute difference between recall when attribute is present and when it's absent\n",
    "                disparity[class_idx, attr_idx] = (recall_when_present[class_idx, attr_idx] - recall_when_absent[class_idx, attr_idx]).abs()\n",
    "    \n",
    "    # Compute the average disparity\n",
    "    average_disparity = disparity.mean().item()\n",
    "    \n",
    "    return average_disparity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3b7719f-4d65-4601-8e1d-1015de2201cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### finding the optimal learning rate\n",
    "# def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, num_epochs=5, lr_range=(1e-4, 1e-1), plot_loss=True):\n",
    "#     model.train()\n",
    "#     model.to(device)\n",
    "#     lr_values = np.logspace(np.log10(lr_range[0]), np.log10(lr_range[1]), num_epochs * len(trainloader))  # Generate learning rates for each batch\n",
    "#     lr_iter = iter(lr_values)\n",
    "#     losses = []\n",
    "#     lrs = []\n",
    "    \n",
    "#     for epoch in range(num_epochs):\n",
    "#         for i, (inputs, labels, annotation) in enumerate(tqdm(trainloader)):\n",
    "#             lr = next(lr_iter)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = lr  # Set new learning rate\n",
    "            \n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             losses.append(loss.item())\n",
    "#             lrs.append(lr)\n",
    "    \n",
    "#     # Calculate the derivative of the loss\n",
    "#     loss_derivative = np.gradient(losses)\n",
    "    \n",
    "#     # Find the learning rate corresponding to the minimum derivative (steepest decline)\n",
    "#     best_lr_index = np.argmin(loss_derivative)\n",
    "#     best_lr = lrs[best_lr_index]\n",
    "    \n",
    "#     if plot_loss:\n",
    "#         import matplotlib.pyplot as plt\n",
    "#         plt.figure()\n",
    "#         plt.plot(lrs, losses)\n",
    "#         plt.xscale('log')\n",
    "#         plt.xlabel('Learning Rate')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.title('Learning Rate Range Test')\n",
    "#         plt.axvline(x=best_lr, color='red', linestyle='--', label=f'Best LR: {best_lr}')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "    \n",
    "#     print(f'Best learning rate: {best_lr}')\n",
    "#     return best_lr\n",
    "\n",
    "# ############# input ############## \n",
    "# batch_size = 16  #to find the optimal learning rate\n",
    "# best_lr = train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs=3)  \n",
    "# print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "411c4f7a-d0b4-478f-9fa5-5e152e08ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(losses):\n",
    "    epochs = range(1, len(losses) + 1)\n",
    "    plt.plot(epochs, losses)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "286386b2-d6e9-447e-95b5-b3dd8f18010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the teacher model\n",
    "def train_teacher(model, trainloader, criterion, optimizer, scheduler, device, num_epochs=1, patience=5):\n",
    "\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    best_train_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    epoch_losses = [] \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            if index % 100 == 99:  # Print every 100 mini-batches\n",
    "                print(f\"[{epoch + 1}, {index + 1}] loss: {running_loss / 100:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        epoch_loss /= num_batches  \n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if epoch_loss < best_train_loss:\n",
    "            best_train_loss = epoch_loss\n",
    "            patience_counter = 0 \n",
    "            # checkpoint\n",
    "            torch.save(model.state_dict(), f'teacher_model_weights_ckd_prof_checkpoint.pth')\n",
    "            torch.save(model, f'teacher_model_ckd_prof_checkpoint.pth')\n",
    "\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    print(\"Finished Training Teacher\")\n",
    "    return epoch_losses\n",
    "\n",
    "\n",
    "# Function to train the student model with knowledge distillation\n",
    "def train_student_with_distillation_disparity(student, teacher, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs, patience=5):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    student.to(device)\n",
    "    teacher.to(device)\n",
    "    best_train_loss = float('inf')  \n",
    "    patience_counter = 0 \n",
    "    student_epoch_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0 \n",
    "        epoch_loss = 0.0  \n",
    "        num_batches = 0  \n",
    "        epoch_disparity = 0.0\n",
    "        running_recall_with = 0.0\n",
    "        running_recall_without = 0.0\n",
    "        student_epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        for index, data in enumerate(tqdm(trainloader)):\n",
    "\n",
    "            inputs = data['img'].to(device)\n",
    "            labels = data['label'].to(device)\n",
    "            annot = data['target'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student(inputs)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher(inputs)\n",
    "\n",
    "            ce_loss = criterion(student_outputs, labels)\n",
    "            kd_loss = tkd_kdloss(student_outputs, teacher_outputs, temperature=temperature)  # Make sure this returns a scalar\n",
    "            \n",
    "            disparity = recall_disparity_loss(student_outputs, labels, annot, class_labels)\n",
    "            # If not scalar, sum up to make sure the loss is scalar\n",
    "            if kd_loss.ndim != 0:\n",
    "                kd_loss = kd_loss.sum()\n",
    "            # if disparity.ndim != 0:\n",
    "            #     disparity = disparity.sum()\n",
    "            \n",
    "            # Now combine the losses\n",
    "            loss = alpha * kd_loss + (1 - alpha) * ce_loss + beta * disparity\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            epoch_disparity += disparity\n",
    "    \n",
    "            if index % 100 == 99:  \n",
    "                print(f\"[{epoch + 1}, {index + 1}] loss: {running_loss / 100:.3f}\")\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        epoch_loss /= num_batches\n",
    "        # print(f'*******Epoch {epoch}: running_recall_with - {running_recall_with/num_batches}  |  running_recall_without - {running_recall_without/num_batches}  |  disparity - {epoch_disparity/num_batches}******')\n",
    "        student_epoch_losses.append(epoch_loss)\n",
    "        # Check for early stopping\n",
    "        if epoch_loss < best_train_loss:\n",
    "            best_train_loss = epoch_loss\n",
    "            patience_counter = 0 \n",
    "            torch.save(student.state_dict(), f'student_model_weights_ckd_prof_checkpoint.pth')\n",
    "            torch.save(student, f'student_model_ckd_prof_checkpoint.pth')\n",
    "        else:\n",
    "            patience_counter += 1 \n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print('Early stopping')\n",
    "            break  \n",
    "\n",
    "        scheduler.step() \n",
    "\n",
    "    print(\"Finished Training Student\")\n",
    "    plot_loss_curve(student_epoch_losses)\n",
    "    return student_epoch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "540397f4-0bb9-4f55-88a5-bdd7671a913e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 77/77 [45:00<00:00, 35.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Teacher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 77/77 [41:07<00:00, 32.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training Student\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 0.4391830892531903]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the function to train the teacher model\n",
    "train_teacher(teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, device, num_epochs=num_epochs)\n",
    "\n",
    "# Call the function to train the student model with knowledge distillation\n",
    "train_student_with_distillation_disparity(student_model, teacher_model, trainloader, criterion, optimizer, scheduler, device, alpha, temperature, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c30aad75-7eb3-4a59-b1ac-dc3582cfa65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "student weights and architecture saved and exported\n",
      "teacher weights and architecture saved and exported\n"
     ]
    }
   ],
   "source": [
    "###################### Testing 1 ######################\n",
    "# Save the student and teacher model weights and architecture\n",
    "torch.save(student_model.state_dict(), 'student_model_weights_ckd_prof.pth')\n",
    "torch.save(student_model, 'student_model_ckd_prof.pth')\n",
    "print('student weights and architecture saved and exported')\n",
    "\n",
    "torch.save(teacher_model.state_dict(), 'teacher_model_weights_ckd_prof.pth')\n",
    "torch.save(teacher_model, 'teacher_model_ckd_prof.pth')\n",
    "print('teacher weights and architecture saved and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995827c-1726-44e2-89aa-5f346a3d4987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher predictions: [ 0  1  2  4  5  7  8  9 10 11 13 16 18 19 20 21 23 25 26 27 28 29]\n",
      "Student predictions: [ 0  1  2  5  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  4  5  7  8  9 10 11 12 16 18 19 20 21 22 23 25 26 27 28 29]\n",
      "Student predictions: [ 0  1  5  8  9 10 13 16 19 21 22 29]\n",
      "Teacher predictions: [ 0  1  2  4  5  6  7  8  9 10 11 12 19 20 21 22 23 24 25 26 27 28 29]\n",
      "Student predictions: [ 0  1  5  8  9 16 19 21 22 29]\n",
      "Teacher predictions: [ 0  1  3  4  5  7  8  9 10 11 12 16 19 20 21 22 23 25 26 27 28 29]\n",
      "Student predictions: [ 0  1  5  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  7  8  9 10 11 12 13 16 19 20 21 23 24 25 26 27 28 29]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 13 16 19 21 22 23 28 29]\n",
      "Teacher predictions: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 18 19 20 21 22 23 25 26 27\n",
      " 28 29]\n",
      "Student predictions: [ 0  1  5  8  9 10 16 19 21 22 29]\n",
      "Teacher predictions: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 16 17 18 19 20 21 22 23 25 26 27\n",
      " 28 29]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 18 19 20 21 22 23 25 26\n",
      " 27 28 29]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 13 16 19 21 22 29]\n",
      "Teacher predictions: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 16 18 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 14 16 17 19 20 21 22 23 25 28 29]\n",
      "Student predictions: [ 1  4  5  8  9 10 16 19 21 22 28]\n",
      "Teacher predictions: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 16 18 19 21 22 23 25 26 27 28\n",
      " 29]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 16 19 21 22]\n",
      "Teacher predictions: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 25\n",
      " 26 27 28]\n",
      "Student predictions: [ 0  1  5  8  9 10 16 19 21 22 28]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 18 19 21 22 23 25 26 27\n",
      " 28]\n",
      "Student predictions: [ 0  1  5  8  9 10 13 16 19 21 22 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 22 23 25\n",
      " 26 27 28]\n",
      "Student predictions: [ 0  1  2  4  5  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "Student predictions: [ 0  1  2  4  5  8  9 10 13 16 19 21 22 23 28 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 21 22 23 24 25\n",
      " 26 27 28]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 13 16 19 20 21 22 28 29]\n",
      "Teacher predictions: [ 1  2  3  4  5  7  8  9 10 11 12 13 14 15 16 18 19 20 22 24 25 26 28]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "Student predictions: [ 0  1  4  5  7  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 16 18 19 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "Student predictions: [ 1  4  5  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 16 18 19 20 21 22 23 25 26 28]\n",
      "Student predictions: [ 0  1  2  4  5  8  9 10 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 16 18 19 20 21 22 23 25 26 28\n",
      " 29]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 16 18 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "Student predictions: [ 0  1  4  5  7  8  9 10 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 16 18 20 21 22 23 24 25 26 27\n",
      " 28]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  3  4  5  6  8  9 10 11 12 13 14 16 17 18 19 20 21 23 24 25 26 27\n",
      " 28]\n",
      "Student predictions: [ 0  1  5  8  9 10 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 18 19 20 21 23 25 26 27\n",
      " 28]\n",
      "Student predictions: [ 0  1  2  4  5  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  4  7  8  9 10 11 13 14 16 18 22 23 25 26 27 28]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 13 16 19 21 22 28]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 13 14 15 16 19 20 21 22 23 24 25 26 27\n",
      " 28 29]\n",
      "Student predictions: [ 0  1  2  4  5  8  9 10 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 25 26\n",
      " 27 28 29]\n",
      "Student predictions: [ 0  1  2  4  5  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 18 19 20 21 22 23 24 25\n",
      " 26 27 28 29]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 0  1  4  5  6  7  8  9 10 11 13 14 16 17 18 19 21 22 23 24 25 26 27 28]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 13 16 19 21 22 28 29]\n",
      "Teacher predictions: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 16 17 18 19 20 21 23 25 26 27\n",
      " 28]\n",
      "Student predictions: [ 0  1  4  5  8  9 10 13 16 19 20 21 22 26 29]\n",
      "Teacher predictions: [ 0  1  2  3  4  5  6  7  8  9 10 11 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28]\n",
      "Student predictions: [ 0  1  5  8  9 10 16 19 21 22 29]\n"
     ]
    }
   ],
   "source": [
    "# Call the comparison and plotting functions after training\n",
    "teacher_params, student_params = compare_model_size(teacher_model, student_model)\n",
    "teacher_time, student_time = compare_inference_time(teacher_model, student_model, testloader)\n",
    "performance_metrics = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "\n",
    "# Extracting the metric values for plotting\n",
    "performance_labels = ['accuracy', 'precision', 'recall', 'f1']\n",
    "teacher_performance_values = [performance_metrics[metric][0] for metric in performance_labels]\n",
    "student_performance_values = [performance_metrics[metric][1] for metric in performance_labels]\n",
    "\n",
    "# Plotting the comparison for performance metrics\n",
    "plot_comparison(performance_labels, teacher_performance_values, student_performance_values, 'Performance Comparison', 'Score')\n",
    "\n",
    "# Plotting the comparison for model size\n",
    "model_size_labels = ['Model Size']\n",
    "teacher_model_size_values = [teacher_params]\n",
    "student_model_size_values = [student_params]\n",
    "plot_comparison(model_size_labels, teacher_model_size_values, student_model_size_values, 'Model Size Comparison', 'Parameter Count (millions)')\n",
    "\n",
    "# Plotting the comparison for inference time\n",
    "inference_time_labels = ['Inference Time']\n",
    "teacher_inference_time_values = [teacher_time]\n",
    "student_inference_time_values = [student_time]\n",
    "plot_comparison(inference_time_labels, teacher_inference_time_values, student_inference_time_values, 'Inference Time Comparison', 'Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de4fbd0-9771-4790-ae5a-f576a7132fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9859d-e3e5-401a-afd5-cd8bbbed5483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_model = torchvision.models.resnet34(weights=None)\n",
    "\n",
    "# weights_path = 'teacher_model_weights_ckd_prof.pth'\n",
    "\n",
    "# teacher_model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "# student_model = torchvision.models.resnet18(weights=None)\n",
    "\n",
    "# weights_path = 'student_model_weights_ckd_prof.pth'\n",
    "\n",
    "# student_model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "# Call the comparison and plotting functions after training\n",
    "teacher_params, student_params = compare_model_size(teacher_model, student_model)\n",
    "teacher_time, student_time = compare_inference_time(teacher_model, student_model, testloader)\n",
    "performance_metrics = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "\n",
    "# Extracting the metric values for plotting\n",
    "performance_labels = ['accuracy', 'precision', 'recall', 'f1']\n",
    "teacher_performance_values = [performance_metrics[metric][0] for metric in performance_labels]\n",
    "student_performance_values = [performance_metrics[metric][1] for metric in performance_labels]\n",
    "\n",
    "# Plotting the comparison for performance metrics\n",
    "plot_comparison(performance_labels, teacher_performance_values, student_performance_values, 'Performance Comparison', 'Score')\n",
    "\n",
    "# Plotting the comparison for model size\n",
    "model_size_labels = ['Model Size']\n",
    "teacher_model_size_values = [teacher_params]\n",
    "student_model_size_values = [student_params]\n",
    "plot_comparison(model_size_labels, teacher_model_size_values, student_model_size_values, 'Model Size Comparison', 'Parameter Count (millions)')\n",
    "\n",
    "# Plotting the comparison for inference time\n",
    "inference_time_labels = ['Inference Time']\n",
    "teacher_inference_time_values = [teacher_time]\n",
    "student_inference_time_values = [student_time]\n",
    "plot_comparison(inference_time_labels, teacher_inference_time_values, student_inference_time_values, 'Inference Time Comparison', 'Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a15892-747e-40ed-9d6a-ed5160b58757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(preds, targets, condition):\n",
    "    \"\"\"\n",
    "    Calculate recall for a given condition in a multi-class setting.\n",
    "\n",
    "    :param preds: Predicted classes.\n",
    "    :param targets: True classes.\n",
    "    :param condition: Boolean tensor indicating the condition (subset) for which to calculate recall.\n",
    "    :return: Recall value.\n",
    "    \"\"\"\n",
    "    if condition.sum() == 0:  # No samples meet the condition\n",
    "        return 0.0\n",
    "\n",
    "    filtered_preds = preds[condition]\n",
    "    filtered_targets = targets[condition]\n",
    "\n",
    "    true_positive = (filtered_preds == filtered_targets).sum().float()\n",
    "    condition_positive = filtered_targets.size(0)\n",
    "\n",
    "    recall = true_positive / condition_positive if condition_positive > 0 else 0.0\n",
    "    return recall\n",
    "\n",
    "\n",
    "def evaluate_disparity(model, dataloader, num_classes, device):\n",
    "    model.eval()\n",
    "    initialized = False\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['img'].to(device)\n",
    "            targets = batch['label'].to(device)\n",
    "            attributes = batch['target'].to(device)\n",
    "\n",
    "            # Initialize sums and counts after attributes is defined and on the same device\n",
    "            if not initialized:\n",
    "                disparity_sums = torch.zeros(num_classes, attributes.size(1), device=device)\n",
    "                counts = torch.zeros(num_classes, attributes.size(1), device=device)\n",
    "                initialized = True\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for class_idx in range(num_classes):\n",
    "                for attr_idx in range(attributes.size(1)):\n",
    "                    condition_present = (attributes[:, attr_idx] == 1) & (targets == class_idx)\n",
    "                    condition_absent = (attributes[:, attr_idx] == 0) & (targets == class_idx)\n",
    "\n",
    "                    recall_present = calculate_recall(preds, targets, condition_present)\n",
    "                    recall_absent = calculate_recall(preds, targets, condition_absent)\n",
    "\n",
    "                    disparity = abs(recall_present - recall_absent)\n",
    "                    disparity_sums[class_idx][attr_idx] += disparity\n",
    "                    counts[class_idx][attr_idx] += 1\n",
    "\n",
    "    average_disparities = disparity_sums / counts\n",
    "    for class_idx in range(num_classes):\n",
    "        for attr_idx in range(attributes.size(1)):\n",
    "            print(f\"Class: {class_idx}, Attr: {attr_idx}, Avg Disparity: {average_disparities[class_idx][attr_idx]}\")\n",
    "\n",
    "    return average_disparities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2be58-1645-404b-95e8-439591731303",
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity = evaluate_disparity(student_model, testloader, num_classes=num_classes, device=device)\n",
    "# print(f'Average recall disparity across all attributes and classes: {disparity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289f549-42fc-4c17-ad60-415775d9d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_disparity(disparity_sums, counts):\n",
    "    \"\"\"\n",
    "    Calculate weighted disparity for each class-attribute pair.\n",
    "    \"\"\"\n",
    "    weighted_disparities = torch.zeros_like(disparity_sums)\n",
    "    for class_idx in range(disparity_sums.size(0)):\n",
    "        for attr_idx in range(disparity_sums.size(1)):\n",
    "            if counts[class_idx][attr_idx] > 0:\n",
    "                weighted_disparities[class_idx][attr_idx] = disparity_sums[class_idx][attr_idx] / counts[class_idx][attr_idx]\n",
    "            else:\n",
    "                weighted_disparities[class_idx][attr_idx] = 0.0\n",
    "    return weighted_disparities\n",
    "\n",
    "def evaluate_disparity(model, dataloader, num_classes, device):\n",
    "    \"\"\"\n",
    "    Evaluate the disparity on the test data with weighted consideration.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    disparity_sums = None\n",
    "    counts = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['img'].to(device)\n",
    "            targets = batch['label'].to(device)\n",
    "            attributes = batch['target'].to(device)\n",
    "\n",
    "            if disparity_sums is None:\n",
    "                disparity_sums = torch.zeros(num_classes, attributes.size(1), device=device)\n",
    "                counts = torch.zeros(num_classes, attributes.size(1), device=device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for class_idx in range(num_classes):\n",
    "                for attr_idx in range(attributes.size(1)):\n",
    "                    condition_present = (attributes[:, attr_idx] == 1) & (targets == class_idx)\n",
    "                    condition_absent = (attributes[:, attr_idx] == 0) & (targets == class_idx)\n",
    "\n",
    "                    if condition_present.sum() > 0 or condition_absent.sum() > 0:\n",
    "                        recall_present = calculate_recall(preds, targets, condition_present)\n",
    "                        recall_absent = calculate_recall(preds, targets, condition_absent)\n",
    "\n",
    "                        disparity = abs(recall_present - recall_absent)\n",
    "                        count = condition_present.sum() + condition_absent.sum()\n",
    "                        disparity_sums[class_idx][attr_idx] += disparity * count\n",
    "                        counts[class_idx][attr_idx] += count\n",
    "\n",
    "    weighted_disparities = calculate_weighted_disparity(disparity_sums, counts)\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        for attr_idx in range(attributes.size(1)):\n",
    "            print(f\"Class: {class_idx}, Attr: {attr_idx}, Weighted Disparity: {weighted_disparities[class_idx][attr_idx]}\")\n",
    "\n",
    "    weighted_average = weighted_disparities.flatten()\n",
    "    weighted_average = weighted_average.sum()/weighted_average.numel()\n",
    "    return weighted_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3dd66-85c6-4a91-9c4d-f20fbc011342",
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity = evaluate_disparity(student_model, testloader, num_classes=num_classes, device=device)\n",
    "print(f'Average recall disparity across all attributes and classes: {disparity}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f6ea21-b999-4dbb-bfca-496354aeeda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa18cfd-8f83-4c3f-bbe9-c834403adf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [f\"Class {label}\" for label in class_labels]\n",
    "\n",
    "def plot_prediction_distribution_and_confusion_matrix(labels, preds, class_names):\n",
    "    # Plotting the distribution of predictions\n",
    "    sns.countplot(x=preds)\n",
    "    plt.title('Distribution of Predictions')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(len(class_names)), class_names, rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "    # Computing the confusion matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm_df, annot=True, fmt='g')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report for detailed metrics\n",
    "    print(classification_report(labels, preds, target_names=class_names, zero_division=0))\n",
    "\n",
    "performance_metrics_teacher = compare_performance_metrics(teacher_model, student_model, testloader)\n",
    "all_labels = performance_metrics_teacher['all_labels']\n",
    "all_teacher_preds = performance_metrics_teacher['all_teacher_preds']\n",
    "all_student_preds = performance_metrics_teacher['all_student_preds']\n",
    "\n",
    "# For the Teacher Model\n",
    "plot_prediction_distribution_and_confusion_matrix(all_labels, all_teacher_preds, class_names)\n",
    "\n",
    "# For the Student Model\n",
    "plot_prediction_distribution_and_confusion_matrix(all_labels, all_student_preds, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ccf26-26ea-468d-877a-d8c24d8b45a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f69e679-d431-4c79-a071-dbd2fdfe4f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
