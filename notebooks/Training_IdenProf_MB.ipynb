{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d692c1e4-7222-46be-a9c4-ffd1331863b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.loss_functions import DKDLoss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from models_package.models import Teacher, Student\n",
    "from torchvision import datasets, transforms, models\n",
    "import models_package\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# new libraries\n",
    "from data.data_loader import load_cifar10, load_cifar100, load_imagenet, load_prof\n",
    "import boto3\n",
    "import io\n",
    "from utils.compare_tools import compare_model_size, compare_inference_time, compare_performance_metrics, plot_comparison\n",
    "from utils.misc_tools import best_LR, train_teacher, retrieve_teacher_class_weights, new_teacher_class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa78713-3fc2-461a-bed6-e4e500e0d133",
   "metadata": {},
   "source": [
    "## Find best LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94bb557-3c3d-45c0-9b41-ccad6ca0ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.003  # 0.01 for resnet34x2 & 0.1 for resnet8 & 0.003 for resnet 8x4\n",
    "num_epochs = 200\n",
    "num_workers = 2\n",
    "batch_size = 128\n",
    "temperature = 4.0\n",
    "alpha = 0.9\n",
    "momentum = 0.9\n",
    "num_classes = 10\n",
    "step_size = 30\n",
    "gamma = 0.1\n",
    "\n",
    "# new parameters\n",
    "# lr_input = 0.1\n",
    "# momentum_input = 0.9\n",
    "weight_decay_input = 5e-4\n",
    "# epochs = 20\n",
    "# T = 4.0 # temperatureture\n",
    "# alpha = 0.9\n",
    "patience = 5  # for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f4ec2-e075-43a4-8889-67d11543463a",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e47e3a-3889-4b4b-b552-f16efff3aac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IdenProf dataset\n",
    "train_path = '/home/ubuntu/W210-Capstone/notebooks/idenprof/train'\n",
    "test_path = '/home/ubuntu/W210-Capstone/notebooks/idenprof/test'\n",
    "trainloader, testloader  = load_prof(train_path, test_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b17689-868d-48c0-a54b-955cd8e49067",
   "metadata": {},
   "source": [
    "## Load in models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ba578-9fee-4f80-a4f3-c7f8277c41bc",
   "metadata": {},
   "source": [
    "### resnet32x4_idenprof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08995195-43a8-4d2e-a181-af291b5282c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "\n",
    "# Create instances of your models\n",
    "# teacher_model = torchvision.models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1).cuda()\n",
    "# teacher_model.eval()  # Set teacher model to evaluation mode\n",
    "# student_model = torchvision.models.resnet18(weights=None).cuda()\n",
    "\n",
    "teacher_name = 'resnet32x4_idenprof'\n",
    "teacher_model = models_package.__dict__[teacher_name](num_class=10)\n",
    "teacher_model.fc = nn.Linear(teacher_model.fc.in_features, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52d20e-74bc-47de-98fe-459fffd5a4ff",
   "metadata": {},
   "source": [
    "### resnet8_idenprof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "351776d3-928e-41ae-bdcf-67077724f25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher_name = 'resnet8_idenprof'\n",
    "# teacher_model = models_package.__dict__[teacher_name](num_class=10)\n",
    "# teacher_model.fc = nn.Linear(teacher_model.fc.in_features, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f15cba-6773-47ad-bf3c-3a168a4ad3e0",
   "metadata": {},
   "source": [
    "### resnet8x4_idenprof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d207ab1-84fd-4a3c-9d3b-5ea16e136325",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_name = 'resnet8x4_idenprof'\n",
    "student_model = models_package.__dict__[student_name](num_class=10)\n",
    "student_model.fc = nn.Linear(teacher_model.fc.in_features, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a98f7e7-bff8-4f28-9db2-72301bb8a920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=12544, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "918fa4ed-2b3e-4b40-be75-1a6500c55eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler for the student model\n",
    "student_optimizer = optim.SGD(student_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "student_scheduler = torch.optim.lr_scheduler.StepLR(student_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Optimizer and scheduler for the teacher model\n",
    "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "teacher_scheduler = torch.optim.lr_scheduler.StepLR(teacher_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Assuming the device is a CUDA device if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38683df-c675-4462-af47-bc4c418ab11b",
   "metadata": {},
   "source": [
    "## Best LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef9c911-8961-4f19-bf38-50f5fd897712",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                 | 0/71 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacty of 22.19 GiB of which 1.47 GiB is free. Including non-PyTorch memory, this process has 20.71 GiB memory in use. Of the allocated memory 20.37 GiB is allocated by PyTorch, and 47.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m teacher_lr \u001b[38;5;241m=\u001b[39m \u001b[43mbest_LR\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet32x4_lr_test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m teacher_lr\n",
      "File \u001b[0;32m~/W210-Capstone/notebooks/utils/misc_tools.py:40\u001b[0m, in \u001b[0;36mbest_LR\u001b[0;34m(save_name, model, trainloader, criterion, optimizer, scheduler, num_epochs, emb, lr_range, plot_loss)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# the Norm and Direction models give 2 outputs - feature embeddings and output\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m emb:\n\u001b[0;32m---> 40\u001b[0m     _, outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/W210-Capstone/notebooks/models_package/resnet_idenprof.py:165\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x, embed)\u001b[0m\n\u001b[1;32m    162\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[1;32m    163\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)    \u001b[38;5;66;03m# 32x32\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 32x32\u001b[39;00m\n\u001b[1;32m    166\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)  \u001b[38;5;66;03m# 16x16\u001b[39;00m\n\u001b[1;32m    167\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)  \u001b[38;5;66;03m# 8x8\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/W210-Capstone/notebooks/models_package/resnet_idenprof.py:44\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m     43\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[0;32m---> 44\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2476\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2478\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacty of 22.19 GiB of which 1.47 GiB is free. Including non-PyTorch memory, this process has 20.71 GiB memory in use. Of the allocated memory 20.37 GiB is allocated by PyTorch, and 47.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "teacher_lr = best_LR('resnet32x4_lr_test', teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, num_epochs=3, emb = True)\n",
    "teacher_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5fbe5-581e-4af0-aeef-c2bd0368ed9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_lr = best_LR('resnet8x4_lr', student_model, trainloader, criterion, optimizer, scheduler, num_epochs=3, emb = True)\n",
    "student_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "912793a1-0a69-474f-8470-3b2114d7c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_lr = 0.00036685719526150065\n",
    "student_lr = 0.0016510167498967254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c54efd-b7d9-44ea-a269-d0d12e87258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler for the student model\n",
    "student_optimizer = optim.SGD(student_model.parameters(), lr=student_lr, momentum=momentum)\n",
    "student_scheduler = torch.optim.lr_scheduler.StepLR(student_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# Optimizer and scheduler for the teacher model\n",
    "teacher_optimizer = optim.SGD(teacher_model.parameters(), lr=teacher_lr, momentum=momentum)\n",
    "teacher_scheduler = torch.optim.lr_scheduler.StepLR(teacher_optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Assuming the device is a CUDA device if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adff0123-c39b-442f-b806-d879d38f466c",
   "metadata": {},
   "source": [
    "## Train Leaderboard Teacher Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dd2e635-6af8-485c-8e1f-54dc2cca58a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 100] loss: 1.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 200] loss: 1.255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 100] loss: 1.132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 200] loss: 1.143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 100] loss: 1.004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 200] loss: 1.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 100] loss: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 200] loss: 0.993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 100] loss: 0.886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 200] loss: 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 100] loss: 0.794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 200] loss: 0.865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 100] loss: 0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 200] loss: 0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 100] loss: 0.737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 200] loss: 0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 100] loss: 0.718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 200] loss: 0.722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 100] loss: 0.631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 200] loss: 0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 100] loss: 0.611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 200] loss: 0.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 100] loss: 0.563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 200] loss: 0.567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 100] loss: 0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 200] loss: 0.543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 100] loss: 0.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 200] loss: 0.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 100] loss: 0.479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 200] loss: 0.520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 100] loss: 0.425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 200] loss: 0.405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 100] loss: 0.400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 200] loss: 0.398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 100] loss: 0.382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 200] loss: 0.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 100] loss: 0.318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 200] loss: 0.345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 100] loss: 0.290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 200] loss: 0.310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 100] loss: 0.291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21, 200] loss: 0.286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 100] loss: 0.288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 200] loss: 0.257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 100] loss: 0.215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 200] loss: 0.240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 100] loss: 0.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 200] loss: 0.240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 100] loss: 0.188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 200] loss: 0.213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 100] loss: 0.162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26, 200] loss: 0.155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 100] loss: 0.177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27, 200] loss: 0.164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 100] loss: 0.180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28, 200] loss: 0.154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 100] loss: 0.096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 200] loss: 0.075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 100] loss: 0.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 200] loss: 0.062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 100] loss: 0.064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 200] loss: 0.069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 100] loss: 0.065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 200] loss: 0.066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 100] loss: 0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 200] loss: 0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 100] loss: 0.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34, 200] loss: 0.060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 100] loss: 0.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35, 200] loss: 0.058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36, 100] loss: 0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36, 200] loss: 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 100] loss: 0.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 200] loss: 0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 100] loss: 0.054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 200] loss: 0.051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 100] loss: 0.055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39, 200] loss: 0.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 100] loss: 0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 200] loss: 0.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 100] loss: 0.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41, 200] loss: 0.048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 100] loss: 0.057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 200] loss: 0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 100] loss: 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 200] loss: 0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 100] loss: 0.049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 200] loss: 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45, 100] loss: 0.048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45, 200] loss: 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 100] loss: 0.052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 200] loss: 0.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 100] loss: 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 200] loss: 0.050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 100] loss: 0.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 200] loss: 0.048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49, 100] loss: 0.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49, 200] loss: 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 100] loss: 0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 200] loss: 0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 100] loss: 0.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51, 200] loss: 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 100] loss: 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 200] loss: 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 100] loss: 0.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53, 200] loss: 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54, 100] loss: 0.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54, 200] loss: 0.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 100] loss: 0.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55, 200] loss: 0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 100] loss: 0.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 200] loss: 0.046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 100] loss: 0.043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 200] loss: 0.037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 100] loss: 0.039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58, 200] loss: 0.040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59, 100] loss: 0.039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59, 200] loss: 0.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60, 100] loss: 0.035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60, 200] loss: 0.034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 100] loss: 0.045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61, 200] loss: 0.040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 100] loss: 0.042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 200] loss: 0.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63, 100] loss: 0.037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63, 200] loss: 0.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 100] loss: 0.039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 200] loss: 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]\n",
      " 35%|██████████████████████████████▍                                                       | 100/282 [01:27<02:38,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 100] loss: 0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████████████████████████████████████▉                         | 200/282 [02:54<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65, 200] loss: 0.041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 282/282 [04:05<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "Finished Training Teacher\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "teacher_resnet32x4 = train_teacher('resnet_32x4', teacher_model, trainloader, criterion, teacher_optimizer, teacher_scheduler, num_epochs=260, patience=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100ebbb-97d0-459a-a26f-3e62299fd6ff",
   "metadata": {},
   "source": [
    "## Extract Class Weights for Norm and Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f15c2d-b937-4214-9964-0a3e0e75e5d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Load in model and weights\n",
    "model_path = './weights/resnet_32x4/checkpoint.pth'\n",
    "weights_path = './weights/resnet_32x4/weights.pth'\n",
    "test_path = './weights/resnet_32x4/test.pth'\n",
    "# idenprof_resnet32x4_model = torch.load(weights_path)\n",
    "# # idenprof_resnet32x4_model.load_state_dict(torch.load(weights_path))\n",
    "# # idenprof_resnet32x4_model.eval()\n",
    "# # idenprof_resnet32x4_model.items()\n",
    "\n",
    "# # import torch, torchvision.models\n",
    "# # model = torchvision.models.vgg16()\n",
    "# # path = 'test.pth'\n",
    "# torch.save(idenprof_resnet32x4_model.state_dict(), test_path) # nothing else here\n",
    "# idenprof_resnet32x4_model.load_state_dict(torch.load(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87edb587-ea4f-4155-b20f-38ed0d9240d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdb61e73-3833-484a-8021-9f0fb61ace0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (4): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
      "  (fc): Linear(in_features=12544, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(idenprof_resnet32x4_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade374e-af03-49f7-b88d-b1308479820e",
   "metadata": {},
   "source": [
    "## Train Leaderboard Student Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3372ef8-48e4-44d6-920d-a2529a5fa199",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Need studnet model loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33dfc81a-80c7-4909-ac9d-c321d19274c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Studnet Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c2041-5f41-47cc-9f2b-3729a8f1100d",
   "metadata": {},
   "source": [
    "## Save Models and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9b7201-9308-43b8-a7ad-aea5898e6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## backup\n",
    "# Save the student and teacher model weights and architecture\n",
    "torch.save(teacher_model.state_dict(), 'teacher_model_weights_resnet8_4.pth')\n",
    "torch.save(teacher_model, 'testing_teacher_model_resnet8_4.pth')\n",
    "print('student weights and architecture saved and exported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c6094-8e8b-4c15-a886-3222640db9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### Saving weights and movel using s3 bucket ######################\n",
    "\n",
    "session = boto3.session.Session()\n",
    "s3 = session.client('s3')\n",
    "\n",
    "bucket_name = '210bucket' \n",
    "\n",
    "# Teacher Model\n",
    "#### IMPORTANT!!!!! Change the file name so that you do not overwrite the existing files\n",
    "teacher_model_weights_path = 'weights/teacher_model_weights_resnet8_4.pth'\n",
    "teacher_model_path = 'models/testing_teacher_model_resnet8_4.pth'\n",
    "\n",
    "# Save state dict to buffer\n",
    "teacher_model_weights_buffer = io.BytesIO()\n",
    "torch.save(teacher_model.state_dict(), teacher_model_weights_buffer)\n",
    "teacher_model_weights_buffer.seek(0)\n",
    "\n",
    "# Save entire model to buffer\n",
    "teacher_model_buffer = io.BytesIO()\n",
    "torch.save(teacher_model, teacher_model_buffer)\n",
    "teacher_model_buffer.seek(0)\n",
    "\n",
    "# Upload to S3\n",
    "s3.put_object(Bucket=bucket_name, Key=teacher_model_weights_path, Body=teacher_model_weights_buffer)\n",
    "s3.put_object(Bucket=bucket_name, Key=teacher_model_path, Body=teacher_model_buffer)\n",
    "print('teacher weights and architecture saved and exported to S3')\n",
    "\n",
    "# # Student Model\n",
    "# #### IMPORTANT!!!!! Change the file name so that you do not overwrite the existing files\n",
    "# student_model_weights_path = 'weights/student_model_weights.pth' \n",
    "# student_model_path = 'models/student_model.pth'\n",
    "\n",
    "# # Save state dict to buffer\n",
    "# student_model_weights_buffer = io.BytesIO()\n",
    "# torch.save(student_model.state_dict(), student_model_weights_buffer)\n",
    "# student_model_weights_buffer.seek(0)\n",
    "\n",
    "# # Save entire model to buffer\n",
    "# student_model_buffer = io.BytesIO()\n",
    "# torch.save(student_model, student_model_buffer)\n",
    "# student_model_buffer.seek(0)\n",
    "\n",
    "# # Upload to S3\n",
    "# s3.put_object(Bucket=bucket_name, Key=student_model_weights_path, Body=student_model_weights_buffer)\n",
    "# s3.put_object(Bucket=bucket_name, Key=student_model_path, Body=student_model_buffer)\n",
    "# print('student weights and architecture saved and exported to S3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6731e70-0e45-4569-a2d1-15a459a05e9f",
   "metadata": {},
   "source": [
    "## Read Models and Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0894a3-e04a-4d27-a0b9-7b795e15ed59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
       "  (fc): Linear(in_features=12544, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a session using Boto3 again \n",
    "session = boto3.session.Session()\n",
    "\n",
    "s3 = session.client('s3')\n",
    "bucket_name = '210bucket'  \n",
    "\n",
    "teacher_model_weights_s3_path = 'weights/idenprof_teacher_resnet32x4_weights.pth'\n",
    "# student_model_weights_s3_path = 'weights/testing_student_model_weights_rkd_prof.pth'\n",
    "\n",
    "# Read files directly into memory\n",
    "teacher_model_weights_buffer = io.BytesIO()\n",
    "# student_model_weights_buffer = io.BytesIO()\n",
    "\n",
    "s3.download_fileobj(bucket_name, teacher_model_weights_s3_path, teacher_model_weights_buffer)\n",
    "# s3.download_fileobj(bucket_name, student_model_weights_s3_path, student_model_weights_buffer)\n",
    "\n",
    "# Load the weights into the models\n",
    "teacher_model_weights_buffer.seek(0)  # Move to the beginning of the buffer\n",
    "# student_model_weights_buffer.seek(0)  \n",
    "\n",
    "######## MAKE SURE THAT YOU HAVE THE CORRECT MODELS FOR WEIGHTS ########\n",
    "# Teacher\n",
    "# teacher_name = 'resnet8x4_idenprof'\n",
    "teacher_name = 'resnet32x4_idenprof'\n",
    "teacher_model = models_package.__dict__[teacher_name](num_class=10)\n",
    "teacher_model.fc = nn.Linear(teacher_model.fc.in_features, 10)\n",
    "teacher_model.load_state_dict(torch.load(teacher_model_weights_buffer))\n",
    "teacher_model.eval()\n",
    "# # Student\n",
    "# student_model = CustomResNet18()\n",
    "# student_model.load_state_dict(torch.load(student_model_weights_buffer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb4da13-d912-47e5-bef7-c0f571b343e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the function\n",
    "model_name = 'resnet32x4_idenprof'\n",
    "model_weight_path = 'weights/idenprof_teacher_resnet32x4_weights.pth'\n",
    "num_class = 10\n",
    "data_name = 'idenprof'  \n",
    "batch_size = 32  \n",
    "bucket_name = '210bucket'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0cba112-38ca-4ed0-925e-ca4798f6aa4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in checkpoint: odict_keys(['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'bn1.num_batches_tracked', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.num_batches_tracked', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.num_batches_tracked', 'layer1.0.downsample.0.weight', 'layer1.0.downsample.1.weight', 'layer1.0.downsample.1.bias', 'layer1.0.downsample.1.running_mean', 'layer1.0.downsample.1.running_var', 'layer1.0.downsample.1.num_batches_tracked', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.num_batches_tracked', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.num_batches_tracked', 'layer1.2.conv1.weight', 'layer1.2.bn1.weight', 'layer1.2.bn1.bias', 'layer1.2.bn1.running_mean', 'layer1.2.bn1.running_var', 'layer1.2.bn1.num_batches_tracked', 'layer1.2.conv2.weight', 'layer1.2.bn2.weight', 'layer1.2.bn2.bias', 'layer1.2.bn2.running_mean', 'layer1.2.bn2.running_var', 'layer1.2.bn2.num_batches_tracked', 'layer1.3.conv1.weight', 'layer1.3.bn1.weight', 'layer1.3.bn1.bias', 'layer1.3.bn1.running_mean', 'layer1.3.bn1.running_var', 'layer1.3.bn1.num_batches_tracked', 'layer1.3.conv2.weight', 'layer1.3.bn2.weight', 'layer1.3.bn2.bias', 'layer1.3.bn2.running_mean', 'layer1.3.bn2.running_var', 'layer1.3.bn2.num_batches_tracked', 'layer1.4.conv1.weight', 'layer1.4.bn1.weight', 'layer1.4.bn1.bias', 'layer1.4.bn1.running_mean', 'layer1.4.bn1.running_var', 'layer1.4.bn1.num_batches_tracked', 'layer1.4.conv2.weight', 'layer1.4.bn2.weight', 'layer1.4.bn2.bias', 'layer1.4.bn2.running_mean', 'layer1.4.bn2.running_var', 'layer1.4.bn2.num_batches_tracked', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.num_batches_tracked', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.num_batches_tracked', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.num_batches_tracked', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.num_batches_tracked', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.num_batches_tracked', 'layer2.2.conv1.weight', 'layer2.2.bn1.weight', 'layer2.2.bn1.bias', 'layer2.2.bn1.running_mean', 'layer2.2.bn1.running_var', 'layer2.2.bn1.num_batches_tracked', 'layer2.2.conv2.weight', 'layer2.2.bn2.weight', 'layer2.2.bn2.bias', 'layer2.2.bn2.running_mean', 'layer2.2.bn2.running_var', 'layer2.2.bn2.num_batches_tracked', 'layer2.3.conv1.weight', 'layer2.3.bn1.weight', 'layer2.3.bn1.bias', 'layer2.3.bn1.running_mean', 'layer2.3.bn1.running_var', 'layer2.3.bn1.num_batches_tracked', 'layer2.3.conv2.weight', 'layer2.3.bn2.weight', 'layer2.3.bn2.bias', 'layer2.3.bn2.running_mean', 'layer2.3.bn2.running_var', 'layer2.3.bn2.num_batches_tracked', 'layer2.4.conv1.weight', 'layer2.4.bn1.weight', 'layer2.4.bn1.bias', 'layer2.4.bn1.running_mean', 'layer2.4.bn1.running_var', 'layer2.4.bn1.num_batches_tracked', 'layer2.4.conv2.weight', 'layer2.4.bn2.weight', 'layer2.4.bn2.bias', 'layer2.4.bn2.running_mean', 'layer2.4.bn2.running_var', 'layer2.4.bn2.num_batches_tracked', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.num_batches_tracked', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.num_batches_tracked', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.num_batches_tracked', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.num_batches_tracked', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.num_batches_tracked', 'layer3.2.conv1.weight', 'layer3.2.bn1.weight', 'layer3.2.bn1.bias', 'layer3.2.bn1.running_mean', 'layer3.2.bn1.running_var', 'layer3.2.bn1.num_batches_tracked', 'layer3.2.conv2.weight', 'layer3.2.bn2.weight', 'layer3.2.bn2.bias', 'layer3.2.bn2.running_mean', 'layer3.2.bn2.running_var', 'layer3.2.bn2.num_batches_tracked', 'layer3.3.conv1.weight', 'layer3.3.bn1.weight', 'layer3.3.bn1.bias', 'layer3.3.bn1.running_mean', 'layer3.3.bn1.running_var', 'layer3.3.bn1.num_batches_tracked', 'layer3.3.conv2.weight', 'layer3.3.bn2.weight', 'layer3.3.bn2.bias', 'layer3.3.bn2.running_mean', 'layer3.3.bn2.running_var', 'layer3.3.bn2.num_batches_tracked', 'layer3.4.conv1.weight', 'layer3.4.bn1.weight', 'layer3.4.bn1.bias', 'layer3.4.bn1.running_mean', 'layer3.4.bn1.running_var', 'layer3.4.bn1.num_batches_tracked', 'layer3.4.conv2.weight', 'layer3.4.bn2.weight', 'layer3.4.bn2.bias', 'layer3.4.bn2.running_mean', 'layer3.4.bn2.running_var', 'layer3.4.bn2.num_batches_tracked', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "session = boto3.session.Session()\n",
    "s3 = session.client('s3')\n",
    "\n",
    "teacher_model_weights_buffer = io.BytesIO()\n",
    "s3.download_fileobj(bucket_name, model_weight_path, teacher_model_weights_buffer)\n",
    "teacher_model_weights_buffer.seek(0)  \n",
    "\n",
    "# Load the model\n",
    "model = models_package.__dict__[model_name](num_class=num_class)\n",
    "checkpoint = torch.load(teacher_model_weights_buffer)\n",
    "print(\"Keys in checkpoint:\", checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2eeb83d-334b-4d55-8a38-ce7679c09b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import models_package  \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Function definitions\n",
    "def get_lindsey_emb_fea(model, dataloader, batch_size):\n",
    "    # Define the device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "    embeddings = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            output = model(inputs)\n",
    "\n",
    "            if isinstance(output, tuple):\n",
    "                output = output[0]\n",
    "\n",
    "            embeddings.append(output.cpu().numpy())\n",
    "\n",
    "    embeddings = np.concatenate(embeddings, axis=0).tolist() \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def get_emb_fea(model, dataloader, batch_size):\n",
    "    ''' Used to extract the feature embeddings in a teacher model '''\n",
    "    model.eval()\n",
    "\n",
    "    EMB = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # compute output\n",
    "            emb_fea, logits = model(images, embed=True)\n",
    "\n",
    "            for emb, i in zip(emb_fea, labels):\n",
    "                i = i.item()\n",
    "                emb_size = len(emb) \n",
    "                if str(i) in EMB:\n",
    "                    for j in range(emb_size):\n",
    "                        EMB[str(i)][j].append(round(emb[j].item(), 4))\n",
    "                else:\n",
    "                    EMB[str(i)] = [[] for _ in range(emb_size)]\n",
    "                    for j in range(emb_size):\n",
    "                        EMB[str(i)][j].append(round(emb[j].item(), 4))\n",
    "\n",
    "    for key, value in EMB.items():\n",
    "        for i in range(emb_size):\n",
    "            EMB[key][i] = round(np.array(EMB[key][i]).mean(), 4)\n",
    "\n",
    "    return EMB\n",
    "\n",
    "\n",
    "def retrieve_teacher_class_weights(model_name, model_weight_path, num_class, data_name, dataloader, batch_size, bucket_name):\n",
    "    ''' Use the extracted feature embeddings to create a json of class means for teacher'''\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.client('s3')\n",
    "\n",
    "    teacher_model_weights_buffer = io.BytesIO()\n",
    "    s3.download_fileobj(bucket_name, model_weight_path, teacher_model_weights_buffer)\n",
    "    teacher_model_weights_buffer.seek(0)  \n",
    "\n",
    "    # Load the model\n",
    "    model = models_package.__dict__[model_name](num_class=num_class)\n",
    "    checkpoint = torch.load(teacher_model_weights_buffer)\n",
    "    # print(\"Keys in checkpoint:\", checkpoint.keys())\n",
    "    print(\"model is loaded properly\")\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in checkpoint.items():\n",
    "        name = k[7:] if k.startswith('module.') else k\n",
    "        new_state_dict[name] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model = model.cuda()\n",
    "\n",
    "    # emb = get_emb_fea(model=model, dataloader=dataloader, batch_size=batch_size)\n",
    "    # emb_json = json.dumps(emb, indent=4)\n",
    "    # with open(\"./class_means/{}_embedding_fea/{}.json\".format(data_name, model_name), 'w', encoding='utf-8') as f:\n",
    "    #     f.write(emb_json)\n",
    "\n",
    "    emb = get_emb_fea(model=model, dataloader=dataloader, batch_size=batch_size)\n",
    "    emb_json = json.dumps(emb, indent=4)\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    output_dir = \"./class_means/{}_embedding_fea\".format(data_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(\"{}/{}.json\".format(output_dir, model_name), 'w', encoding='utf-8') as f:\n",
    "        f.write(emb_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac2bde0b-c7d5-4187-b912-f4a5104c4383",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loaded properly\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n",
      "12544\n"
     ]
    }
   ],
   "source": [
    "retrieve_teacher_class_weights(model_name, model_weight_path, num_class, data_name, testloader, batch_size, bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e712850-fc1f-416b-a689-d62bdb71852b",
   "metadata": {},
   "source": [
    "# Knowledge Distillation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f9fbc-e1d5-47bd-8341-14d3a02cbcf2",
   "metadata": {},
   "source": [
    "## KD++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42126720-0eb4-4740-9c46-56cfc2325ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16 Nov 2023 04:20:00 [line:297] \u001b[32mDistribute train, total batch size:128, epoch:240\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Teacher Weights\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 311\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mteacher_weights:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoad Teacher Weights\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 311\u001b[0m     teacher_ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mteacher_weights\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    312\u001b[0m     teacher\u001b[38;5;241m.\u001b[39mload_state_dict(teacher_ckpt)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m teacher\u001b[38;5;241m.\u001b[39mparameters():\n",
      "\u001b[0;31mKeyError\u001b[0m: 'model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import models_package\n",
    "from utils.loss_functions import DKDLoss, DirectNormLoss, KDLoss\n",
    "# from Models.embtrans_cifar import EmbTrans\n",
    "# from Dataset import CIFAR, IDENPROF\n",
    "from utils.misc_tools import colorstr, Save_Checkpoint, AverageMeter\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pdb\n",
    "\n",
    "def train(model, teacher, T_EMB, train_dataloader, optimizer, criterion, kd_loss, nd_loss, args, epoch):\n",
    "    train_loss = AverageMeter()\n",
    "    train_error = AverageMeter()\n",
    "\n",
    "    Cls_loss = AverageMeter()\n",
    "    Div_loss = AverageMeter()\n",
    "    Norm_Dir_loss = AverageMeter()\n",
    "\n",
    "    # Model on train mode\n",
    "    model.train()\n",
    "    teacher.eval()\n",
    "    step_per_epoch = len(train_dataloader)\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_dataloader):\n",
    "        start = time.time()\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # compute output\n",
    "        s_emb, s_logits = model(images, embed=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_emb, t_logits = teacher(images, embed=True)\n",
    "\n",
    "        # cls loss\n",
    "        cls_loss = criterion(s_logits, labels) * args.cls_loss_factor\n",
    "        # KD loss\n",
    "        div_loss = kd_loss(s_logits, t_logits) * min(1.0, epoch/args.warm_up)\n",
    "        # ND loss\n",
    "        norm_dir_loss = nd_loss(s_emb=s_emb, t_emb=t_emb, T_EMB=T_EMB, labels=labels)\n",
    "\n",
    "        loss = cls_loss + div_loss + norm_dir_loss\n",
    "        # measure accuracy and record loss\n",
    "        batch_size = images.size(0)\n",
    "        _, pred = s_logits.data.cpu().topk(1, dim=1)\n",
    "        train_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "        train_loss.update(loss.item(), batch_size)\n",
    "\n",
    "        Cls_loss.update(cls_loss.item(), batch_size)\n",
    "        Div_loss.update(div_loss.item(), batch_size)\n",
    "        Norm_Dir_loss.update(norm_dir_loss.item(), batch_size)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "        s1 = '\\r{} [{}/{}]'.format(t, step+1, step_per_epoch)\n",
    "        s2 = ' - {:.2f}ms/step - nd_loss: {:.3f} - kd_loss: {:.3f} - cls_loss: {:.3f} - train_loss: {:.3f} - train_acc: {:.3f}'.format(\n",
    "             1000 * (time.time() - start), norm_dir_loss.item(), div_loss.item(), cls_loss.item(), train_loss.val, 1-train_error.val)\n",
    "\n",
    "        print(s1+s2, end='', flush=True)\n",
    "\n",
    "    print()\n",
    "    return Norm_Dir_loss.avg, Div_loss.avg, Cls_loss.avg, train_loss.avg, train_error.avg\n",
    "\n",
    "\n",
    "def test(model, test_dataloader, criterion):\n",
    "    test_loss = AverageMeter()\n",
    "    test_error = AverageMeter()\n",
    "\n",
    "    # Model on eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # compute logits\n",
    "            logits = model(images, embed=False)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            batch_size = images.size(0)\n",
    "            _, pred = logits.data.cpu().topk(1, dim=1)\n",
    "            test_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "            test_loss.update(loss.item(), batch_size)\n",
    "\n",
    "    return test_loss.avg, test_error.avg\n",
    "\n",
    "\n",
    "def epoch_loop(model, teacher, args):\n",
    "    # data loaders\n",
    "    # train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=args.workers)\n",
    "    # test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=args.workers)\n",
    "    train_loader, test_loader  = load_prof(train_path, test_path, batch_size=batch_size)\n",
    "    \n",
    "    # model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # model = nn.DataParallel(model, device_ids=args.gpus)\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    # teacher = nn.DataParallel(teacher, device_ids=args.gpus)\n",
    "    teacher = nn.DataParallel(teacher)\n",
    "    teacher.to(device)\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    kd_loss = KDLoss(kl_loss_factor=args.kd_loss_factor, T=args.t).to(device)\n",
    "    nd_loss = DirectNormLoss(num_class=100, nd_loss_factor=args.nd_loss_factor).to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)\n",
    "\n",
    "    # weights\n",
    "    save_dir = Path(args.save_dir)\n",
    "    weights = save_dir / 'weights'\n",
    "    weights.mkdir(parents=True, exist_ok=True)\n",
    "    last = weights / 'last'\n",
    "    best = weights / 'best'\n",
    "\n",
    "    # acc,loss\n",
    "    acc_loss = save_dir / 'acc_loss'\n",
    "    acc_loss.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_acc_savepath = acc_loss / 'train_acc.npy'\n",
    "    train_loss_savepath = acc_loss / 'train_loss.npy'\n",
    "    val_acc_savepath = acc_loss / 'val_acc.npy'\n",
    "    val_loss_savepath = acc_loss / 'val_loss.npy'\n",
    "\n",
    "    # tensorboard\n",
    "    logdir = save_dir / 'logs'\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "    summary_writer = SummaryWriter(logdir, flush_secs=120)\n",
    "\n",
    "    # resume\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_error = checkpoint['best_error']\n",
    "        train_acc = checkpoint['train_acc']\n",
    "        train_loss = checkpoint['train_loss']\n",
    "        test_acc = checkpoint['test_acc']\n",
    "        test_loss = checkpoint['test_loss']\n",
    "        logger.info(colorstr('green', 'Resuming training from {} epoch'.format(start_epoch)))\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_error = 0\n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        test_acc = []\n",
    "        test_loss = []\n",
    "\n",
    "    # Train model\n",
    "    best_error = 1\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        if epoch in [150, 180, 210]:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, args.epochs))\n",
    "        norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error = train(model=model,\n",
    "                                                                                 teacher=teacher,\n",
    "                                                                                 T_EMB=T_EMB,\n",
    "                                                                                 train_dataloader=train_loader,\n",
    "                                                                                 optimizer=optimizer,\n",
    "                                                                                 criterion=criterion,\n",
    "                                                                                 kd_loss=kd_loss,\n",
    "                                                                                 nd_loss=nd_loss,\n",
    "                                                                                 args=args,\n",
    "                                                                                 epoch=epoch)\n",
    "        test_epoch_loss, test_error = test(model=model,\n",
    "                                           test_dataloader=test_loader,\n",
    "                                           criterion=criterion)\n",
    "\n",
    "        s = \"Train Loss: {:.3f}, Train Acc: {:.3f}, Test Loss: {:.3f}, Test Acc: {:.3f}, lr: {:.5f}\".format(\n",
    "            train_epoch_loss, 1-train_error, test_epoch_loss, 1-test_error, optimizer.param_groups[0]['lr'])\n",
    "        logger.info(colorstr('green', s))\n",
    "\n",
    "        # save acc,loss\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        train_acc.append(1-train_error)\n",
    "        test_loss.append(test_epoch_loss)\n",
    "        test_acc.append(1-test_error)\n",
    "\n",
    "        # save model\n",
    "        is_best = test_error < best_error\n",
    "        best_error = min(best_error, test_error)\n",
    "        state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_error': best_error,\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'test_acc': test_acc,\n",
    "                'test_loss': test_loss,\n",
    "            }\n",
    "\n",
    "        last_path = last / 'epoch_{}_loss_{:.3f}_acc_{:.3f}'.format(\n",
    "            epoch + 1, test_epoch_loss, 1-test_error)\n",
    "        best_path = best / 'epoch_{}_acc_{:.3f}'.format(\n",
    "                epoch + 1, 1-best_error)\n",
    "\n",
    "        Save_Checkpoint(state, last, last_path, best, best_path, is_best)\n",
    "\n",
    "        # tensorboard\n",
    "        if epoch == 1:\n",
    "            images, labels = next(iter(train_loader))\n",
    "            img_grid = torchvision.utils.make_grid(images)\n",
    "            summary_writer.add_image('Image', img_grid)\n",
    "        summary_writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "        summary_writer.add_scalar('train_loss', train_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('train_error', train_error, epoch)\n",
    "        summary_writer.add_scalar('val_loss', test_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('val_error', test_error, epoch)\n",
    "\n",
    "        summary_writer.add_scalar('nd_loss', norm_dir_loss, epoch)\n",
    "        summary_writer.add_scalar('kd_loss', div_loss, epoch)\n",
    "        summary_writer.add_scalar('cls_loss', cls_loss, epoch)\n",
    "\n",
    "    summary_writer.close()\n",
    "    if not os.path.exists(train_acc_savepath) or not os.path.exists(train_loss_savepath):\n",
    "        np.save(train_acc_savepath, train_acc)\n",
    "        np.save(train_loss_savepath, train_loss)\n",
    "        np.save(val_acc_savepath, test_acc)\n",
    "        np.save(val_loss_savepath, test_loss)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_names = sorted(name for name in models_package.__dict__\n",
    "                         if name.islower() and not name.startswith(\"__\")\n",
    "                         and callable(models_package.__dict__[name]))\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Cifar Training')\n",
    "    parser.add_argument('-f') # added to make this run in collab\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"resnet8x4_idenprof\", choices=model_names, help=\"model architecture\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default='idenprof')\n",
    "    parser.add_argument(\"--epochs\", type=int, default=240)\n",
    "    # parser.add_argument(\"--epochs\", type=int, default=4)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size per gpu\")\n",
    "    parser.add_argument('--workers', default=32, type=int, help='number of data loading workers')\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.1)\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum')\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=5e-4)\n",
    "\n",
    "    parser.add_argument(\"--teacher\", type=str, default=\"resnet32x4_idenprof\", help=\"teacher architecture\")\n",
    "    parser.add_argument(\"--teacher_weights\", type=str, default=\"./weights/resnet_32x4/weights.pth\", help=\"teacher weights path\")\n",
    "    parser.add_argument(\"--cls_loss_factor\", type=float, default=1.0, help=\"cls loss weight factor\")\n",
    "    parser.add_argument(\"--kd_loss_factor\", type=float, default=1.0, help=\"KD loss weight factor\")\n",
    "    parser.add_argument(\"--t\", type=float, default=4.0, help=\"temperature\")\n",
    "    parser.add_argument(\"--nd_loss_factor\", type=float, default=1.0, help=\"ND loss weight factor\")\n",
    "    parser.add_argument(\"--warm_up\", type=float, default=20.0, help='loss weight warm up epochs')\n",
    "\n",
    "    # parser.add_argument(\"--gpus\", type=list, default=[0, 1])\n",
    "    parser.add_argument('--seed', default=None, type=int, help='seed for initializing training.')\n",
    "    parser.add_argument(\"--resume\", type=str, help=\"best ckpt's path to resume most recent training\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"./run/IdenProf/KD++\", help=\"save path, eg, acc_loss, weights, tensorboard, and so on\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        cudnn.benchmark = False\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s [line:%(lineno)d] %(message)s',\n",
    "                        datefmt='%d %b %Y %H:%M:%S')\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # args.batch_size = args.batch_size * len(args.gpus)\n",
    "    args.batch_size = args.batch_size * 1\n",
    "\n",
    "    # logger.info(colorstr('green', \"Distribute train, gpus:{}, total batch size:{}, epoch:{}\".format(args.gpus, args.batch_size, args.epochs)))\n",
    "    logger.info(colorstr('green', \"Distribute train, total batch size:{}, epoch:{}\".format(args.batch_size, args.epochs)))\n",
    "\n",
    "\n",
    "    # train_set, test_set, num_class = IDENPROF(name=args.dataset)\n",
    "    num_class = 10\n",
    "    model = models_package.__dict__[args.model_name](num_class=num_class)\n",
    "\n",
    "    # if args.model_name in ['wrn40_1_cifar', 'mobilenetv2', 'shufflev1_cifar', 'shufflev2_cifar']:\n",
    "    #     model = EmbTrans(student=model, model_name=args.model_name)\n",
    "\n",
    "    teacher = models_package.__dict__[args.teacher](num_class=num_class)\n",
    "\n",
    "    if args.teacher_weights:\n",
    "        print('Load Teacher Weights')\n",
    "        session = boto3.session.Session()\n",
    "        s3 = session.client('s3')\n",
    "    \n",
    "        teacher_model_weights_buffer = io.BytesIO()\n",
    "        s3.download_fileobj(bucket_name, model_weight_path, teacher_model_weights_buffer)\n",
    "        teacher_model_weights_buffer.seek(0)  \n",
    "    \n",
    "        # Load the model\n",
    "        model = models_package.__dict__[model_name](num_class=num_class)\n",
    "        checkpoint = torch.load(teacher_model_weights_buffer)\n",
    "\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # res56    ./ckpt/teacher/resnet56/center_emb_train.json\n",
    "    # res32x4  ./ckpt/teacher/resnet32x4/center_emb_train.json\n",
    "    # wrn40_2  ./ckpt/teacher/wrn_40_2/center_emb_train.json\n",
    "    # res50    ./ckpt/teacher/resnet50/center_emb_train.json\n",
    "    # class-mean\n",
    "    with open(\"./class_means/idenprof_embedding_fea/resnet32x4_idenprof.json\", 'r') as f:\n",
    "        T_EMB = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    logger.info(colorstr('green', 'Use ' + args.teacher + ' Training ' + args.model_name + ' ...'))\n",
    "    # Train the model\n",
    "    epoch_loop(model=model, teacher=teacher, train_set=train_set, test_set=test_set, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222fcba-7fd3-402b-b7b8-1b90681a8de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
