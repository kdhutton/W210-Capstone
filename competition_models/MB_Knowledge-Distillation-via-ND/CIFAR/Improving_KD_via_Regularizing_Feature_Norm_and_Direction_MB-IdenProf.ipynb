{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VoQdhztlxPU"
   },
   "source": [
    "Paper: https://arxiv.org/abs/2305.17007\n",
    "\n",
    "Code Link: https://github.com/wangyz1608/knowledge-distillation-via-nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboard\n",
      "  Downloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard) (1.24.4)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[K     |████████████████████████████████| 226 kB 122.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.19.6\n",
      "  Downloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
      "\u001b[K     |████████████████████████████████| 311 kB 122.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard) (2.31.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 23.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 119.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard) (45.2.0)\n",
      "Collecting grpcio>=1.48.2\n",
      "  Downloading grpcio-1.59.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 115.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard) (0.34.2)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.23.3-py2.py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 122.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard) (3.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard) (1.25.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/ubuntu/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard) (6.8.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.2)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard) (3.17.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.6.3->tensorboard) (0.4.2)\n",
      "Installing collected packages: tensorboard-data-server, werkzeug, protobuf, markdown, rsa, google-auth, requests-oauthlib, google-auth-oauthlib, absl-py, grpcio, tensorboard\n",
      "Successfully installed absl-py-2.0.0 google-auth-2.23.3 google-auth-oauthlib-1.0.0 grpcio-1.59.0 markdown-3.5 protobuf-4.24.4 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.14.0 tensorboard-data-server-0.7.2 werkzeug-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5196,
     "status": "ok",
     "timestamp": 1698548857358,
     "user": {
      "displayName": "Michelle Bolner",
      "userId": "00246950705303252689"
     },
     "user_tz": 240
    },
    "id": "dG0NcJPYlkyr"
   },
   "outputs": [],
   "source": [
    "# import libaraies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# import local \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_salAm-WD_m"
   },
   "source": [
    "## Pull in Data per README\n",
    "\n",
    "Please put the CIFAR100 dataset in ./Dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1697464658043,
     "user": {
      "displayName": "Michelle Bolner",
      "userId": "00246950705303252689"
     },
     "user_tz": 240
    },
    "id": "3tgZSNd1TvgF",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "c9a1badc-e15b-4581-e4a9-103a4adbfdfd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Edited the cifar.py file to download dataset'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Edited the cifar.py file to download dataset\"\"\"\n",
    "\n",
    "# def CIFAR(name='cifar10', valid_size=None):\n",
    "#     assert name in [\"cifar10\", \"cifar100\"]\n",
    "#     if name == \"cifar10\":\n",
    "#         train_data = datasets.CIFAR10(root=\"Dataset/\", train=True, download=True)\n",
    "#         test_data = datasets.CIFAR10(root=\"Dataset/\", train=False)\n",
    "#         num_class = 10\n",
    "\n",
    "#         mean = [0.4914, 0.4822, 0.4465]\n",
    "#         std = [0.2471, 0.2435, 0.2616]\n",
    "\n",
    "#     elif name == \"cifar100\":\n",
    "#         train_data = datasets.CIFAR100(root=\"Dataset/\", train=True, download=True)\n",
    "#         test_data = datasets.CIFAR100(root=\"Dataset/\", train=False)\n",
    "#         num_class = 100\n",
    "\n",
    "#         mean = [0.5071, 0.4867, 0.4408]\n",
    "#         std = [0.2675, 0.2565, 0.2761]\n",
    "\n",
    "#     train_data.transform = T.Compose([\n",
    "#         T.Pad(4),\n",
    "#         T.RandomCrop(32),\n",
    "#         T.RandomHorizontalFlip(),\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize(mean=mean, std=std),\n",
    "#     ])\n",
    "#     test_data.transform = T.Compose([\n",
    "#         T.ToTensor(),\n",
    "#         T.Normalize(mean=mean, std=std)\n",
    "#     ])\n",
    "\n",
    "#     if valid_size:\n",
    "\n",
    "#         if name == 'cifar10':\n",
    "#             valid_data = datasets.CIFAR10(root=\"Dataset/data\", train=True)\n",
    "#         elif name == 'cifar100':\n",
    "#             valid_data = datasets.CIFAR100(root=\"Dataset/data\", train=True)\n",
    "\n",
    "#         valid_data.transform = T.Compose([\n",
    "#             T.ToTensor(),\n",
    "#             T.Normalize(mean=mean, std=std)\n",
    "#         ])\n",
    "\n",
    "#         indices = torch.randperm(len(train_data))\n",
    "#         train_indices = indices[:len(indices) - int(valid_size * len(indices))]\n",
    "#         valid_indices = indices[len(indices) - int(valid_size * len(indices)):]\n",
    "#         train_data = torch.utils.data.Subset(train_data, train_indices)\n",
    "#         valid_data = torch.utils.data.Subset(valid_data, valid_indices)\n",
    "#     else:\n",
    "#         valid_data = None\n",
    "\n",
    "#     return train_data, test_data, num_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebj1TlYlWipL"
   },
   "source": [
    "Download the teacher checkpoint cifar_teachers.tar at https://github.com/megvii-research/mdistiller/releases/tag/checkpoints and untar it to ./ckpt.\n",
    "\n",
    "**NOTE - run all of the below blocks to download the teacher checkpoints.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-29 20:59:48--  https://github.com/megvii-research/mdistiller/releases/download/checkpoints/cifar_teachers.tar\n",
      "Resolving github.com (github.com)... 192.30.255.113\n",
      "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/470505142/344f57bd-dedb-4f11-971b-b1c5b8946f1c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231029%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231029T205948Z&X-Amz-Expires=300&X-Amz-Signature=2c82846ac8ae14fcf8ee779f7d960b533f284f0665efe941a884e666c4667ef7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=470505142&response-content-disposition=attachment%3B%20filename%3Dcifar_teachers.tar&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-10-29 20:59:48--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/470505142/344f57bd-dedb-4f11-971b-b1c5b8946f1c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231029%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231029T205948Z&X-Amz-Expires=300&X-Amz-Signature=2c82846ac8ae14fcf8ee779f7d960b533f284f0665efe941a884e666c4667ef7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=470505142&response-content-disposition=attachment%3B%20filename%3Dcifar_teachers.tar&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 364390400 (348M) [application/octet-stream]\n",
      "Saving to: ‘cifar_teachers.tar’\n",
      "\n",
      "cifar_teachers.tar  100%[===================>] 347.51M   215MB/s    in 1.6s    \n",
      "\n",
      "2023-10-29 20:59:49 (215 MB/s) - ‘cifar_teachers.tar’ saved [364390400/364390400]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sudo wget -c https://github.com/megvii-research/mdistiller/releases/download/checkpoints/cifar_teachers.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar_teachers.tar: POSIX tar archive (GNU)\n"
     ]
    }
   ],
   "source": [
    "!file cifar_teachers.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo tar -xf  cifar_teachers.tar -C ./ckpt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf cifar_teachers.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgcuAfpIWV9y"
   },
   "source": [
    "# 1. compute the class-mean of teachers on training set\n",
    "We provide the class-mean of teachers in ./ckpt/teacher for the ease of reproducibility.\n",
    "\n",
    "```\n",
    "python3 emb_fea_distribution.py \\\n",
    "      --model_name resnet56_cifar \\\n",
    "      --model_weights 'MODEL WEIGHT PATH' \\\n",
    "      --emb_size 64 \\\n",
    "      --dataset 'cifar100' \\\n",
    "      --batch_size 128\n",
    "```\n",
    "\n",
    "\n",
    " The results, json file, be put in ./ckpt/teacher/. e.g., ./ckpt/teacher/resnet56/center_emb_train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1697464658044,
     "user": {
      "displayName": "Michelle Bolner",
      "userId": "00246950705303252689"
     },
     "user_tz": 240
    },
    "id": "h3Bar6YOZMgR",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Visualized the embedding feature of the pre-train model on the training set.\n",
    "# \"\"\"\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader\n",
    "# from collections import OrderedDict\n",
    "\n",
    "# import Models\n",
    "# from Dataset import CIFAR\n",
    "\n",
    "# import numpy as np\n",
    "# import argparse\n",
    "# import json\n",
    "\n",
    "# def emb_fea(model, data, args):\n",
    "#     # model to evaluate mode\n",
    "#     model.eval()\n",
    "#     dataloader = DataLoader(data, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=args.workers)\n",
    "\n",
    "#     EMB = {}\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in dataloader:\n",
    "#             images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "#             # compute output\n",
    "#             emb_fea, logits = model(images, embed=True)\n",
    "\n",
    "#             for emb, i in zip(emb_fea, labels):\n",
    "#                 i = i.item()\n",
    "#                 assert len(emb) == args.emb_size\n",
    "#                 if str(i) in EMB:\n",
    "#                     for j in range(len(emb)):\n",
    "#                         EMB[str(i)][j].append(round(emb[j].item(), 4))\n",
    "#                 else:\n",
    "#                     EMB[str(i)] = [[] for _ in range(len(emb))]\n",
    "#                     for j in range(len(emb)):\n",
    "#                         EMB[str(i)][j].append(round(emb[j].item(), 4))\n",
    "\n",
    "\n",
    "#     for key, value in EMB.items():\n",
    "#         for i in range(args.emb_size):\n",
    "#             EMB[key][i] = round(np.array(EMB[key][i]).mean(), 4)\n",
    "\n",
    "#     return EMB\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     model_names = sorted(name for name in Models.__dict__\n",
    "#                          if name.islower() and not name.startswith(\"__\")\n",
    "#                          and callable(Models.__dict__[name]))\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='Visualized the embedding feature of the model on the train set.')\n",
    "#     parser.add_argument('-f') # added to make this run in collab\n",
    "#     parser.add_argument(\"--model_name\", type=str, default=\"resnet56_cifar\", choices=model_names, help=\"model architecture\")\n",
    "#     parser.add_argument(\"--model_weights\", type=str, default=\"./ckpt/teacher/resnet56/center_emb_train.json\", help=\"model weights path\")\n",
    "#     parser.add_argument(\"--emb_size\", type=int, default=64, help=\"emb fea size\")\n",
    "#     parser.add_argument(\"--dataset\", type=str, default='cifar10')\n",
    "#     parser.add_argument(\"--batch_size\", type=int, default=64, help=\"total batch size\")\n",
    "#     parser.add_argument('--workers', default=16, type=int, help='number of data loading workers')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     # dataset\n",
    "#     if args.dataset in ['cifar10', 'cifar100']:\n",
    "#         train_set, test_set, num_class = CIFAR(name=args.dataset)\n",
    "#     else:\n",
    "#         print(\"No Dataset!!!\")\n",
    "\n",
    "#     model = Models.__dict__[args.model_name](num_class=num_class)\n",
    "\n",
    "#     if args.model_weights:\n",
    "#         print('Visualized the embedding feature of the {} model on the train set'.format(args.model_name))\n",
    "\n",
    "#         model_ckpt = torch.load(args.model_weights)['model_state_dict']\n",
    "#         new_state_dict = OrderedDict()\n",
    "#         for k, v in model_ckpt.items():\n",
    "#             name = k[7:]   # remove 'module.'\n",
    "#             new_state_dict[name] = v\n",
    "#         model.load_state_dict(new_state_dict)\n",
    "\n",
    "#         for param in model.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#     else:\n",
    "#         print('No load Pre-trained weights!')\n",
    "\n",
    "#     model = model.cuda()\n",
    "\n",
    "#     emb = emb_fea(model=model, data=train_set, args=args)\n",
    "#     emb_json = json.dumps(emb, indent=4)\n",
    "#     with open(\"./run/{}_embedding_fea/{}.json\".format(args.dataset, args.model_name), 'w', encoding='utf-8') as f:\n",
    "#         f.write(emb_json)\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LB75Se2x04p2"
   },
   "source": [
    "### Can skip ^^ they provide trained teacher weights. Not sure where the model weights file is supposed to be (needs to be a pickle file, not the default one listed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmF2q0Ql1F2h"
   },
   "source": [
    "#2. train student by KD++\n",
    "\n",
    "for instance, distillation from resnet-56 to resnet-20 by KD++.\n",
    "\n",
    "\n",
    "```\n",
    "python3 train_cifar_kd.py \\\n",
    "      --model_name resnet20_cifar \\\n",
    "      --teacher resnet56_cifar \\\n",
    "      --teacher_weights 'Teacher WEIGHT PATH' \\\n",
    "      --dataset 'cifar100' \\\n",
    "      --epoch 240 \\\n",
    "      --batch_size 64 \\\n",
    "      --lr 0.1 \\\n",
    "      --cls_loss_factor 1.0 \\\n",
    "      --kd_loss_factor 1.0 \\\n",
    "      --nd_loss_factor 2.0 \\\n",
    "      --save_dir \"./run/CIFAR100/KD++/res56-res20\"\n",
    "\n",
    "```\n",
    "\n",
    "other models, please refer to train_cifar.sh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IdenProf Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-30 04:18:23--  https://github.com/OlafenwaMoses/IdenProf/releases/download/v1.0/idenprof-jpg.zip\n",
      "Resolving github.com (github.com)... 192.30.255.112\n",
      "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/131628975/f5b4b56a-75bc-11e8-9437-cd029632d3dc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231030T041824Z&X-Amz-Expires=300&X-Amz-Signature=38b992312976863a6189b91ca8c4f9973d15b6aedc5bc3d66a5544cb1ad91632&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=131628975&response-content-disposition=attachment%3B%20filename%3Didenprof-jpg.zip&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-10-30 04:18:24--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/131628975/f5b4b56a-75bc-11e8-9437-cd029632d3dc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231030%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231030T041824Z&X-Amz-Expires=300&X-Amz-Signature=38b992312976863a6189b91ca8c4f9973d15b6aedc5bc3d66a5544cb1ad91632&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=131628975&response-content-disposition=attachment%3B%20filename%3Didenprof-jpg.zip&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 154714873 (148M) [application/octet-stream]\n",
      "Saving to: ‘idenprof-jpg.zip’\n",
      "\n",
      "idenprof-jpg.zip    100%[===================>] 147.55M   231MB/s    in 0.6s    \n",
      "\n",
      "2023-10-30 04:18:24 (231 MB/s) - ‘idenprof-jpg.zip’ saved [154714873/154714873]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sudo wget -c https://github.com/OlafenwaMoses/IdenProf/releases/download/v1.0/idenprof-jpg.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIST++\n",
      "Dataset\n",
      "Improving_KD_via_Regularizing_Feature_Norm_and_Direction_MB-IdenProf.ipynb\n",
      "Improving_KD_via_Regularizing_Feature_Norm_and_Direction_MB.ipynb\n",
      "Models\n",
      "README.md\n",
      "ReviewKD++\n",
      "__pycache__\n",
      "ckpt\n",
      "emb_fea_distribution.py\n",
      "emb_visual.sh\n",
      "fiftyone_KD_ND.ipynb\n",
      "idenprof\n",
      "idenprof-jpg.zip\n",
      "run\n",
      "train_cifar.sh\n",
      "train_cifar_baseline.py\n",
      "train_cifar_dkd.py\n",
      "train_cifar_kd.py\n",
      "utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prof(train_path, test_path, batch_size=64):\n",
    "    \n",
    "    # Define data transformations (you can customize this based on your needs)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((226, 226)),  # Resize the images to a consistent size\n",
    "        transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the images\n",
    "    ])\n",
    "    \n",
    "    # Create the ImageFolder dataset\n",
    "    traindataset = datasets.ImageFolder(root=train_path, transform=transform)\n",
    "    testdataset = datasets.ImageFolder(root=test_path, transform=transform)\n",
    "    \n",
    "    \n",
    "    # Create a DataLoader to load the data\n",
    "    # batch_size = 32  # You can adjust this based on your hardware and requirements\n",
    "    train_loader = torch.utils.data.DataLoader(traindataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = torch.utils.data.DataLoader(testdataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = './idenprof/train'\n",
    "test_dir = './idenprof/test'\n",
    "train_dataloader, test_dataloader = load_prof(train_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "HzYUnK17azyK"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 04:21:24 [line:295] \u001b[32mDistribute train, total batch size:128, epoch:240\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 298\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# logger.info(colorstr('green', \"Distribute train, gpus:{}, total batch size:{}, epoch:{}\".format(args.gpus, args.batch_size, args.epochs)))\u001b[39;00m\n\u001b[1;32m    295\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(colorstr(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDistribute train, total batch size:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, epoch:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mbatch_size, args\u001b[38;5;241m.\u001b[39mepochs)))\n\u001b[0;32m--> 298\u001b[0m train_set, test_set, num_class \u001b[38;5;241m=\u001b[39m CIFAR(name\u001b[38;5;241m=\u001b[39m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m)\n\u001b[1;32m    299\u001b[0m model \u001b[38;5;241m=\u001b[39m Models\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[args\u001b[38;5;241m.\u001b[39mmodel_name](num_class\u001b[38;5;241m=\u001b[39mnum_class)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwrn40_1_cifar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmobilenetv2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshufflev1_cifar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshufflev2_cifar\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'dataset'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import Models\n",
    "from Models.embtrans_cifar import EmbTrans\n",
    "from Dataset import CIFAR\n",
    "from utils import colorstr, Save_Checkpoint, AverageMeter, DirectNormLoss, KDLoss\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pdb\n",
    "\n",
    "def train(model, teacher, T_EMB, train_dataloader, optimizer, criterion, kd_loss, nd_loss, args, epoch):\n",
    "    train_loss = AverageMeter()\n",
    "    train_error = AverageMeter()\n",
    "\n",
    "    Cls_loss = AverageMeter()\n",
    "    Div_loss = AverageMeter()\n",
    "    Norm_Dir_loss = AverageMeter()\n",
    "\n",
    "    # Model on train mode\n",
    "    model.train()\n",
    "    teacher.eval()\n",
    "    step_per_epoch = len(train_dataloader)\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_dataloader):\n",
    "        start = time.time()\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # compute output\n",
    "        s_emb, s_logits = model(images, embed=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_emb, t_logits = teacher(images, embed=True)\n",
    "\n",
    "        # cls loss\n",
    "        cls_loss = criterion(s_logits, labels) * args.cls_loss_factor\n",
    "        # KD loss\n",
    "        div_loss = kd_loss(s_logits, t_logits) * min(1.0, epoch/args.warm_up)\n",
    "        # ND loss\n",
    "        norm_dir_loss = nd_loss(s_emb=s_emb, t_emb=t_emb, T_EMB=T_EMB, labels=labels)\n",
    "\n",
    "        loss = cls_loss + div_loss + norm_dir_loss\n",
    "        # measure accuracy and record loss\n",
    "        batch_size = images.size(0)\n",
    "        _, pred = s_logits.data.cpu().topk(1, dim=1)\n",
    "        train_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "        train_loss.update(loss.item(), batch_size)\n",
    "\n",
    "        Cls_loss.update(cls_loss.item(), batch_size)\n",
    "        Div_loss.update(div_loss.item(), batch_size)\n",
    "        Norm_Dir_loss.update(norm_dir_loss.item(), batch_size)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "        s1 = '\\r{} [{}/{}]'.format(t, step+1, step_per_epoch)\n",
    "        s2 = ' - {:.2f}ms/step - nd_loss: {:.3f} - kd_loss: {:.3f} - cls_loss: {:.3f} - train_loss: {:.3f} - train_acc: {:.3f}'.format(\n",
    "             1000 * (time.time() - start), norm_dir_loss.item(), div_loss.item(), cls_loss.item(), train_loss.val, 1-train_error.val)\n",
    "\n",
    "        print(s1+s2, end='', flush=True)\n",
    "\n",
    "    print()\n",
    "    return Norm_Dir_loss.avg, Div_loss.avg, Cls_loss.avg, train_loss.avg, train_error.avg\n",
    "\n",
    "\n",
    "def test(model, test_dataloader, criterion):\n",
    "    test_loss = AverageMeter()\n",
    "    test_error = AverageMeter()\n",
    "\n",
    "    # Model on eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # compute logits\n",
    "            logits = model(images, embed=False)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            batch_size = images.size(0)\n",
    "            _, pred = logits.data.cpu().topk(1, dim=1)\n",
    "            test_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "            test_loss.update(loss.item(), batch_size)\n",
    "\n",
    "    return test_loss.avg, test_error.avg\n",
    "\n",
    "\n",
    "def epoch_loop(model, teacher, train_set, test_set, args):\n",
    "    # data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=args.workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=args.workers)\n",
    "\n",
    "    # model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # model = nn.DataParallel(model, device_ids=args.gpus)\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    # teacher = nn.DataParallel(teacher, device_ids=args.gpus)\n",
    "    teacher = nn.DataParallel(teacher)\n",
    "    teacher.to(device)\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    kd_loss = KDLoss(kl_loss_factor=args.kd_loss_factor, T=args.t).to(device)\n",
    "    nd_loss = DirectNormLoss(num_class=100, nd_loss_factor=args.nd_loss_factor).to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)\n",
    "\n",
    "    # weights\n",
    "    save_dir = Path(args.save_dir)\n",
    "    weights = save_dir / 'weights'\n",
    "    weights.mkdir(parents=True, exist_ok=True)\n",
    "    last = weights / 'last'\n",
    "    best = weights / 'best'\n",
    "\n",
    "    # acc,loss\n",
    "    acc_loss = save_dir / 'acc_loss'\n",
    "    acc_loss.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_acc_savepath = acc_loss / 'train_acc.npy'\n",
    "    train_loss_savepath = acc_loss / 'train_loss.npy'\n",
    "    val_acc_savepath = acc_loss / 'val_acc.npy'\n",
    "    val_loss_savepath = acc_loss / 'val_loss.npy'\n",
    "\n",
    "    # tensorboard\n",
    "    logdir = save_dir / 'logs'\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "    summary_writer = SummaryWriter(logdir, flush_secs=120)\n",
    "\n",
    "    # resume\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_error = checkpoint['best_error']\n",
    "        train_acc = checkpoint['train_acc']\n",
    "        train_loss = checkpoint['train_loss']\n",
    "        test_acc = checkpoint['test_acc']\n",
    "        test_loss = checkpoint['test_loss']\n",
    "        logger.info(colorstr('green', 'Resuming training from {} epoch'.format(start_epoch)))\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_error = 0\n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        test_acc = []\n",
    "        test_loss = []\n",
    "\n",
    "    # Train model\n",
    "    best_error = 1\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        if epoch in [150, 180, 210]:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, args.epochs))\n",
    "        norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error = train(model=model,\n",
    "                                                                                 teacher=teacher,\n",
    "                                                                                 T_EMB=T_EMB,\n",
    "                                                                                 train_dataloader=train_loader,\n",
    "                                                                                 optimizer=optimizer,\n",
    "                                                                                 criterion=criterion,\n",
    "                                                                                 kd_loss=kd_loss,\n",
    "                                                                                 nd_loss=nd_loss,\n",
    "                                                                                 args=args,\n",
    "                                                                                 epoch=epoch)\n",
    "        test_epoch_loss, test_error = test(model=model,\n",
    "                                           test_dataloader=test_loader,\n",
    "                                           criterion=criterion)\n",
    "\n",
    "        s = \"Train Loss: {:.3f}, Train Acc: {:.3f}, Test Loss: {:.3f}, Test Acc: {:.3f}, lr: {:.5f}\".format(\n",
    "            train_epoch_loss, 1-train_error, test_epoch_loss, 1-test_error, optimizer.param_groups[0]['lr'])\n",
    "        logger.info(colorstr('green', s))\n",
    "\n",
    "        # save acc,loss\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        train_acc.append(1-train_error)\n",
    "        test_loss.append(test_epoch_loss)\n",
    "        test_acc.append(1-test_error)\n",
    "\n",
    "        # save model\n",
    "        is_best = test_error < best_error\n",
    "        best_error = min(best_error, test_error)\n",
    "        state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_error': best_error,\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'test_acc': test_acc,\n",
    "                'test_loss': test_loss,\n",
    "            }\n",
    "\n",
    "        last_path = last / 'epoch_{}_loss_{:.3f}_acc_{:.3f}'.format(\n",
    "            epoch + 1, test_epoch_loss, 1-test_error)\n",
    "        best_path = best / 'epoch_{}_acc_{:.3f}'.format(\n",
    "                epoch + 1, 1-best_error)\n",
    "\n",
    "        Save_Checkpoint(state, last, last_path, best, best_path, is_best)\n",
    "\n",
    "        # tensorboard\n",
    "        if epoch == 1:\n",
    "            images, labels = next(iter(train_loader))\n",
    "            img_grid = torchvision.utils.make_grid(images)\n",
    "            summary_writer.add_image('Cifar Image', img_grid)\n",
    "        summary_writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "        summary_writer.add_scalar('train_loss', train_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('train_error', train_error, epoch)\n",
    "        summary_writer.add_scalar('val_loss', test_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('val_error', test_error, epoch)\n",
    "\n",
    "        summary_writer.add_scalar('nd_loss', norm_dir_loss, epoch)\n",
    "        summary_writer.add_scalar('kd_loss', div_loss, epoch)\n",
    "        summary_writer.add_scalar('cls_loss', cls_loss, epoch)\n",
    "\n",
    "    summary_writer.close()\n",
    "    if not os.path.exists(train_acc_savepath) or not os.path.exists(train_loss_savepath):\n",
    "        np.save(train_acc_savepath, train_acc)\n",
    "        np.save(train_loss_savepath, train_loss)\n",
    "        np.save(val_acc_savepath, test_acc)\n",
    "        np.save(val_loss_savepath, test_loss)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_names = sorted(name for name in Models.__dict__\n",
    "                         if name.islower() and not name.startswith(\"__\")\n",
    "                         and callable(Models.__dict__[name]))\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Cifar Training')\n",
    "    parser.add_argument('-f') # added to make this run in collab\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"resnet20_cifar\", choices=model_names, help=\"model architecture\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default='cifar100')\n",
    "    parser.add_argument(\"--epochs\", type=int, default=240)\n",
    "    # parser.add_argument(\"--epochs\", type=int, default=4)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size per gpu\")\n",
    "    parser.add_argument('--workers', default=32, type=int, help='number of data loading workers')\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.1)\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum')\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=5e-4)\n",
    "\n",
    "    parser.add_argument(\"--teacher\", type=str, default=\"resnet56_cifar\", help=\"teacher architecture\")\n",
    "    parser.add_argument(\"--teacher_weights\", type=str, default=\"./ckpt/cifar_teachers/resnet56_vanilla/ckpt_epoch_240.pth\", help=\"teacher weights path\")\n",
    "    parser.add_argument(\"--cls_loss_factor\", type=float, default=1.0, help=\"cls loss weight factor\")\n",
    "    parser.add_argument(\"--kd_loss_factor\", type=float, default=1.0, help=\"KD loss weight factor\")\n",
    "    parser.add_argument(\"--t\", type=float, default=4.0, help=\"temperature\")\n",
    "    parser.add_argument(\"--nd_loss_factor\", type=float, default=1.0, help=\"ND loss weight factor\")\n",
    "    parser.add_argument(\"--warm_up\", type=float, default=20.0, help='loss weight warm up epochs')\n",
    "\n",
    "    # parser.add_argument(\"--gpus\", type=list, default=[0, 1])\n",
    "    parser.add_argument('--seed', default=None, type=int, help='seed for initializing training.')\n",
    "    parser.add_argument(\"--resume\", type=str, help=\"best ckpt's path to resume most recent training\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"./run/KD++\", help=\"save path, eg, acc_loss, weights, tensorboard, and so on\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        cudnn.benchmark = False\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s [line:%(lineno)d] %(message)s',\n",
    "                        datefmt='%d %b %Y %H:%M:%S')\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # args.batch_size = args.batch_size * len(args.gpus)\n",
    "    args.batch_size = args.batch_size * 1\n",
    "\n",
    "    # logger.info(colorstr('green', \"Distribute train, gpus:{}, total batch size:{}, epoch:{}\".format(args.gpus, args.batch_size, args.epochs)))\n",
    "    logger.info(colorstr('green', \"Distribute train, total batch size:{}, epoch:{}\".format(args.batch_size, args.epochs)))\n",
    "\n",
    "\n",
    "    train_set, test_set, num_class = CIFAR(name=args.dataset)\n",
    "    model = Models.__dict__[args.model_name](num_class=num_class)\n",
    "\n",
    "    if args.model_name in ['wrn40_1_cifar', 'mobilenetv2', 'shufflev1_cifar', 'shufflev2_cifar']:\n",
    "        model = EmbTrans(student=model, model_name=args.model_name)\n",
    "\n",
    "    teacher = Models.__dict__[args.teacher](num_class=num_class)\n",
    "\n",
    "    if args.teacher_weights:\n",
    "        print('Load Teacher Weights')\n",
    "        teacher_ckpt = torch.load(args.teacher_weights)['model']\n",
    "        teacher.load_state_dict(teacher_ckpt)\n",
    "\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # res56    ./ckpt/teacher/resnet56/center_emb_train.json\n",
    "    # res32x4  ./ckpt/teacher/resnet32x4/center_emb_train.json\n",
    "    # wrn40_2  ./ckpt/teacher/wrn_40_2/center_emb_train.json\n",
    "    # res50    ./ckpt/teacher/resnet50/center_emb_train.json\n",
    "    # class-mean\n",
    "    with open(\"./ckpt/teacher/resnet56/center_emb_train.json\", 'r') as f:\n",
    "        T_EMB = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    logger.info(colorstr('green', 'Use ' + args.teacher + ' Training ' + args.model_name + ' ...'))\n",
    "    # Train the model\n",
    "    epoch_loop(model=model, teacher=teacher, train_set=train_set, test_set=test_set, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7vrpER7Ix0Cr"
   },
   "source": [
    "#3. DKD++/DIST++\n",
    "## DKD++\n",
    "\n",
    "\n",
    "```\n",
    "python3 train_cifar_dkd.py \\\n",
    "        --model_name shufflev1_cifar \\\n",
    "        --teacher resnet32x4_cifar \\\n",
    "        --teacher_weights 'Teacher WEIGHT PATH' \\\n",
    "        --dataset 'cifar100' \\\n",
    "        --epoch 240 \\\n",
    "        --batch_size 64 \\\n",
    "        --lr 0.02 \\\n",
    "        --cls_loss_factor 1.0 \\\n",
    "        --dkd_alpha 0.5 \\\n",
    "        --dkd_beta 8.0 \\\n",
    "        --nd_loss_factor 4.0 \\\n",
    "        --save_dir \"./run/CIFAR100/DKD++/res32x4-sv1\"\n",
    "\n",
    "```\n",
    "\n",
    "other models, please refer to train_cifar.sh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "ogML41zTz1po"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29 Oct 2023 22:46:45 [line:297] \u001b[32mDistribute train, total batch size:128, epoch:240\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29 Oct 2023 22:46:46 [line:323] \u001b[32mUse resnet32x4_cifar Training resnet20_cifar ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Teacher Weights\n",
      "Epoch 1/240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 325\u001b[0m\n\u001b[1;32m    323\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(colorstr(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m args\u001b[38;5;241m.\u001b[39mteacher \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Training \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m args\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ...\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    324\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 325\u001b[0m \u001b[43mepoch_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[57], line 178\u001b[0m, in \u001b[0;36mepoch_loop\u001b[0;34m(model, teacher, train_set, test_set, args)\u001b[0m\n\u001b[1;32m    176\u001b[0m         param_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs))\n\u001b[0;32m--> 178\u001b[0m norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mteacher\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mteacher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mT_EMB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT_EMB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mdkd_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdkd_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mnd_loss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnd_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                                                                         \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m test_epoch_loss, test_error \u001b[38;5;241m=\u001b[39m test(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    189\u001b[0m                                    test_dataloader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[1;32m    190\u001b[0m                                    criterion\u001b[38;5;241m=\u001b[39mcriterion)\n\u001b[1;32m    192\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m, Test Loss: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m, Test Acc: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m, lr: \u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    193\u001b[0m     train_epoch_loss, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mtrain_error, test_epoch_loss, \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mtest_error, optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[57], line 55\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, teacher, T_EMB, train_dataloader, optimizer, criterion, dkd_loss, nd_loss, args, epoch)\u001b[0m\n\u001b[1;32m     53\u001b[0m div_loss \u001b[38;5;241m=\u001b[39m dkd_loss(s_logits, t_logits, labels) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1.0\u001b[39m, epoch\u001b[38;5;241m/\u001b[39margs\u001b[38;5;241m.\u001b[39mwarm_up)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# ND loss\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m norm_dir_loss \u001b[38;5;241m=\u001b[39m \u001b[43mnd_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_EMB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT_EMB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m loss \u001b[38;5;241m=\u001b[39m cls_loss \u001b[38;5;241m+\u001b[39m div_loss \u001b[38;5;241m+\u001b[39m norm_dir_loss\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# measure accuracy and record loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/W210-Capstone/competition_models/MB_Knowledge-Distillation-via-ND/CIFAR/utils.py:64\u001b[0m, in \u001b[0;36mDirectNormLoss.forward\u001b[0;34m(self, s_emb, t_emb, T_EMB, labels)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, s_emb, t_emb, T_EMB, labels): \n\u001b[0;32m---> 64\u001b[0m     nd_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject_center\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_EMB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mT_EMB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnd_loss_factor\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nd_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n",
      "File \u001b[0;32m~/W210-Capstone/competition_models/MB_Knowledge-Distillation-via-ND/CIFAR/utils.py:52\u001b[0m, in \u001b[0;36mDirectNormLoss.project_center\u001b[0;34m(self, s_emb, t_emb, T_EMB, labels)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproject_center\u001b[39m(\u001b[38;5;28mself\u001b[39m, s_emb, t_emb, T_EMB, labels):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m s_emb\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m t_emb\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m s_emb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[1;32m     54\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torch import nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torch.nn.functional as F\n",
    "# import torch.backends.cudnn as cudnn\n",
    "\n",
    "# import Models\n",
    "# from Models.embtrans_cifar import EmbTrans\n",
    "# from Dataset import CIFAR\n",
    "# from utils import colorstr, Save_Checkpoint, AverageMeter, DirectNormLoss, DKDLoss\n",
    "# from utils import colorstr, Save_Checkpoint, AverageMeter, DirectNormLoss, KDLoss\n",
    "\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "# import time\n",
    "# import json\n",
    "# import random\n",
    "# import logging\n",
    "# import argparse\n",
    "# import warnings\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# def train(model, teacher, T_EMB, train_dataloader, optimizer, criterion, dkd_loss, nd_loss, args, epoch):\n",
    "#     train_loss = AverageMeter()\n",
    "#     train_error = AverageMeter()\n",
    "\n",
    "#     Cls_loss = AverageMeter()\n",
    "#     Div_loss = AverageMeter()\n",
    "#     Norm_Dir_loss = AverageMeter()\n",
    "\n",
    "#     # Model on train mode\n",
    "#     model.train()\n",
    "#     teacher.eval()\n",
    "#     step_per_epoch = len(train_dataloader)\n",
    "\n",
    "#     for step, (images, labels) in enumerate(train_dataloader):\n",
    "#         start = time.time()\n",
    "#         images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "#         # compute output\n",
    "#         s_emb, s_logits = model(images, embed=True)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             t_emb, t_logits = teacher(images, embed=True)\n",
    "\n",
    "#         # cls loss\n",
    "#         cls_loss = criterion(s_logits, labels) * args.cls_loss_factor\n",
    "#         # KD loss\n",
    "#         div_loss = dkd_loss(s_logits, t_logits, labels) * min(1.0, epoch/args.warm_up)\n",
    "#         # ND loss\n",
    "#         norm_dir_loss = nd_loss(s_emb=s_emb, t_emb=t_emb, T_EMB=T_EMB, labels=labels)\n",
    "\n",
    "#         loss = cls_loss + div_loss + norm_dir_loss\n",
    "#         # measure accuracy and record loss\n",
    "#         batch_size = images.size(0)\n",
    "#         _, pred = s_logits.data.cpu().topk(1, dim=1)\n",
    "#         train_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "#         train_loss.update(loss.item(), batch_size)\n",
    "\n",
    "#         Cls_loss.update(cls_loss.item(), batch_size)\n",
    "#         Div_loss.update(div_loss.item(), batch_size)\n",
    "#         Norm_Dir_loss.update(norm_dir_loss.item(), batch_size)\n",
    "\n",
    "#         # compute gradient and do SGD step\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "#         s1 = '\\r{} [{}/{}]'.format(t, step+1, step_per_epoch)\n",
    "#         s2 = ' - {:.2f}ms/step - nd_loss: {:.3f} - dkd_loss: {:.3f} - cls_loss: {:.3f} - train_loss: {:.3f} - train_acc: {:.3f}'.format(\n",
    "#              1000 * (time.time() - start), norm_dir_loss.item(), div_loss.item(), cls_loss.item(), train_loss.val, 1-train_error.val)\n",
    "\n",
    "#         print(s1+s2, end='', flush=True)\n",
    "\n",
    "#     print()\n",
    "#     return Norm_Dir_loss.avg, Div_loss.avg, Cls_loss.avg, train_loss.avg, train_error.avg\n",
    "\n",
    "\n",
    "# def test(model, test_dataloader, criterion):\n",
    "#     test_loss = AverageMeter()\n",
    "#     test_error = AverageMeter()\n",
    "\n",
    "#     # Model on eval mode\n",
    "#     model.eval()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_dataloader:\n",
    "#             images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "#             # compute logits\n",
    "#             logits = model(images, embed=False)\n",
    "\n",
    "#             loss = criterion(logits, labels)\n",
    "\n",
    "#             # measure accuracy and record loss\n",
    "#             batch_size = images.size(0)\n",
    "#             _, pred = logits.data.cpu().topk(1, dim=1)\n",
    "#             test_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "#             test_loss.update(loss.item(), batch_size)\n",
    "\n",
    "#     return test_loss.avg, test_error.avg\n",
    "\n",
    "\n",
    "# def epoch_loop(model, teacher, train_set, test_set, args):\n",
    "#     # data loaders\n",
    "#     train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=args.workers)\n",
    "#     test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=args.workers)\n",
    "\n",
    "#     # model\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     # model = nn.DataParallel(model, device_ids=args.gpus)\n",
    "#     model = nn.DataParallel(model)\n",
    "#     model.to(device)\n",
    "#     # teacher = nn.DataParallel(teacher, device_ids=args.gpus)\n",
    "#     teacher = nn.DataParallel(teacher)\n",
    "#     teacher.to(device)\n",
    "\n",
    "#     # loss\n",
    "#     criterion = nn.CrossEntropyLoss().to(device)\n",
    "#     dkd_loss = DKDLoss(alpha=args.dkd_alpha, beta=args.dkd_beta, T=args.t).to(device)\n",
    "#     nd_loss = DirectNormLoss(num_class=100, nd_loss_factor=args.nd_loss_factor).to(device)\n",
    "#     # optimizer\n",
    "#     optimizer = torch.optim.SGD(params=model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)\n",
    "\n",
    "#     # weights\n",
    "#     save_dir = Path(args.save_dir)\n",
    "#     weights = save_dir / 'weights'\n",
    "#     weights.mkdir(parents=True, exist_ok=True)\n",
    "#     last = weights / 'last'\n",
    "#     best = weights / 'best'\n",
    "\n",
    "#     # acc,loss\n",
    "#     acc_loss = save_dir / 'acc_loss'\n",
    "#     acc_loss.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     train_acc_savepath = acc_loss / 'train_acc.npy'\n",
    "#     train_loss_savepath = acc_loss / 'train_loss.npy'\n",
    "#     val_acc_savepath = acc_loss / 'val_acc.npy'\n",
    "#     val_loss_savepath = acc_loss / 'val_loss.npy'\n",
    "\n",
    "#     # tensorboard\n",
    "#     logdir = save_dir / 'logs'\n",
    "#     logdir.mkdir(parents=True, exist_ok=True)\n",
    "#     summary_writer = SummaryWriter(logdir, flush_secs=120)\n",
    "\n",
    "#     # resume\n",
    "#     if args.resume:\n",
    "#         checkpoint = torch.load(args.resume)\n",
    "#         start_epoch = checkpoint['epoch']\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         best_error = checkpoint['best_error']\n",
    "#         train_acc = checkpoint['train_acc']\n",
    "#         train_loss = checkpoint['train_loss']\n",
    "#         test_acc = checkpoint['test_acc']\n",
    "#         test_loss = checkpoint['test_loss']\n",
    "#         logger.info(colorstr('green', 'Resuming training from {} epoch'.format(start_epoch)))\n",
    "#     else:\n",
    "#         start_epoch = 0\n",
    "#         best_error = 0\n",
    "#         train_acc = []\n",
    "#         train_loss = []\n",
    "#         test_acc = []\n",
    "#         test_loss = []\n",
    "\n",
    "#     # Train model\n",
    "#     best_error = 1\n",
    "#     for epoch in range(start_epoch, args.epochs):\n",
    "#         if epoch in [150, 180, 210]:\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] *= 0.1\n",
    "#         print(\"Epoch {}/{}\".format(epoch + 1, args.epochs))\n",
    "#         norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error = train(model=model,\n",
    "#                                                                                  teacher=teacher,\n",
    "#                                                                                  T_EMB=T_EMB,\n",
    "#                                                                                  train_dataloader=train_loader,\n",
    "#                                                                                  optimizer=optimizer,\n",
    "#                                                                                  criterion=criterion,\n",
    "#                                                                                  dkd_loss=dkd_loss,\n",
    "#                                                                                  nd_loss=nd_loss,\n",
    "#                                                                                  args=args,\n",
    "#                                                                                  epoch=epoch)\n",
    "#         test_epoch_loss, test_error = test(model=model,\n",
    "#                                            test_dataloader=test_loader,\n",
    "#                                            criterion=criterion)\n",
    "\n",
    "#         s = \"Train Loss: {:.3f}, Train Acc: {:.3f}, Test Loss: {:.3f}, Test Acc: {:.3f}, lr: {:.5f}\".format(\n",
    "#             train_epoch_loss, 1-train_error, test_epoch_loss, 1-test_error, optimizer.param_groups[0]['lr'])\n",
    "#         logger.info(colorstr('green', s))\n",
    "\n",
    "#         # save acc,loss\n",
    "#         train_loss.append(train_epoch_loss)\n",
    "#         train_acc.append(1-train_error)\n",
    "#         test_loss.append(test_epoch_loss)\n",
    "#         test_acc.append(1-test_error)\n",
    "\n",
    "#         # save model\n",
    "#         is_best = test_error < best_error\n",
    "#         best_error = min(best_error, test_error)\n",
    "#         state = {\n",
    "#                 'epoch': epoch + 1,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'best_error': best_error,\n",
    "#                 'train_acc': train_acc,\n",
    "#                 'train_loss': train_loss,\n",
    "#                 'test_acc': test_acc,\n",
    "#                 'test_loss': test_loss,\n",
    "#             }\n",
    "\n",
    "#         last_path = last / 'epoch_{}_loss_{:.3f}_acc_{:.3f}'.format(\n",
    "#             epoch + 1, test_epoch_loss, 1-test_error)\n",
    "#         best_path = best / 'epoch_{}_acc_{:.3f}'.format(\n",
    "#                 epoch + 1, 1-best_error)\n",
    "\n",
    "#         Save_Checkpoint(state, last, last_path, best, best_path, is_best)\n",
    "\n",
    "#         # tensorboard\n",
    "#         if epoch == 1:\n",
    "#             images, labels = next(iter(train_loader))\n",
    "#             img_grid = torchvision.utils.make_grid(images)\n",
    "#             summary_writer.add_image('Cifar Image', img_grid)\n",
    "#         summary_writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "#         summary_writer.add_scalar('train_loss', train_epoch_loss, epoch)\n",
    "#         summary_writer.add_scalar('train_error', train_error, epoch)\n",
    "#         summary_writer.add_scalar('val_loss', test_epoch_loss, epoch)\n",
    "#         summary_writer.add_scalar('val_error', test_error, epoch)\n",
    "\n",
    "#         summary_writer.add_scalar('nd_loss', norm_dir_loss, epoch)\n",
    "#         summary_writer.add_scalar('kd_loss', div_loss, epoch)\n",
    "#         summary_writer.add_scalar('cls_loss', cls_loss, epoch)\n",
    "\n",
    "#     summary_writer.close()\n",
    "#     if not os.path.exists(train_acc_savepath) or not os.path.exists(train_loss_savepath):\n",
    "#         np.save(train_acc_savepath, train_acc)\n",
    "#         np.save(train_loss_savepath, train_loss)\n",
    "#         np.save(val_acc_savepath, test_acc)\n",
    "#         np.save(val_loss_savepath, test_loss)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     model_names = sorted(name for name in Models.__dict__\n",
    "#                          if name.islower() and not name.startswith(\"__\")\n",
    "#                          and callable(Models.__dict__[name]))\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='PyTorch Cifar Training')\n",
    "#     parser.add_argument('-f') # added to make this run in collab\n",
    "#     parser.add_argument(\"--model_name\", type=str, default=\"resnet20_cifar\", choices=model_names, help=\"model architecture\")\n",
    "#     parser.add_argument(\"--dataset\", type=str, default='cifar100')\n",
    "#     parser.add_argument(\"--epochs\", type=int, default=240)\n",
    "#     # parser.add_argument(\"--epochs\", type=int, default=4)\n",
    "#     parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size per gpu\")\n",
    "#     parser.add_argument('--workers', default=32, type=int, help='number of data loading workers')\n",
    "#     parser.add_argument(\"--lr\", type=float, default=0.1)\n",
    "#     parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum')\n",
    "#     parser.add_argument(\"--weight_decay\", type=float, default=5e-4)\n",
    "\n",
    "#     parser.add_argument(\"--teacher\", type=str, default=\"resnet32x4_cifar\", help=\"teacher architecture\")\n",
    "#     parser.add_argument(\"--teacher_weights\", type=str, default=\"./ckpt/cifar_teachers/resnet32x4_vanilla/ckpt_epoch_240.pth\", help=\"teacher weights path\")\n",
    "#     parser.add_argument(\"--cls_loss_factor\", type=float, default=1.0, help=\"cls loss weight factor\")\n",
    "#     parser.add_argument(\"--dkd_alpha\", type=float, default=1.0, help=\"dkd loss weight factor, alpha\")\n",
    "#     parser.add_argument(\"--dkd_beta\", type=float, default=1.0, help=\"dkd loss weight factor, beta\")\n",
    "#     parser.add_argument(\"--t\", type=float, default=4.0, help=\"temperature\")\n",
    "#     parser.add_argument(\"--nd_loss_factor\", type=float, default=1.0, help=\"ND loss weight factor\")\n",
    "#     parser.add_argument(\"--warm_up\", type=float, default=20.0, help='loss weight warm up epochs')\n",
    "\n",
    "#     # parser.add_argument(\"--gpus\", type=list, default=[0, 1])\n",
    "#     parser.add_argument('--seed', default=None, type=int, help='seed for initializing training.')\n",
    "#     parser.add_argument(\"--resume\", type=str, help=\"best ckpt's path to resume most recent training\")\n",
    "#     parser.add_argument(\"--save_dir\", type=str, default=\"./run/DKD++\", help=\"save path, eg, acc_loss, weights, tensorboard, and so on\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     if args.seed is not None:\n",
    "#         random.seed(args.seed)\n",
    "#         torch.manual_seed(args.seed)\n",
    "#         cudnn.deterministic = True\n",
    "#         cudnn.benchmark = False\n",
    "#         warnings.warn('You have chosen to seed training. '\n",
    "#                       'This will turn on the CUDNN deterministic setting, '\n",
    "#                       'which can slow down your training considerably! '\n",
    "#                       'You may see unexpected behavior when restarting '\n",
    "#                       'from checkpoints.')\n",
    "\n",
    "#     logging.basicConfig(level=logging.INFO, format='%(asctime)s [line:%(lineno)d] %(message)s',\n",
    "#                         datefmt='%d %b %Y %H:%M:%S')\n",
    "#     logger = logging.getLogger(__name__)\n",
    "\n",
    "#     # args.batch_size = args.batch_size * len(args.gpus)\n",
    "#     args.batch_size = args.batch_size * 1\n",
    "\n",
    "#     # logger.info(colorstr('green', \"Distribute train, gpus:{}, total batch size:{}, epoch:{}\".format(args.gpus, args.batch_size, args.epochs)))\n",
    "#     logger.info(colorstr('green', \"Distribute train, total batch size:{}, epoch:{}\".format(args.batch_size, args.epochs)))\n",
    "\n",
    "\n",
    "#     train_set, test_set, num_class = CIFAR(name=args.dataset)\n",
    "#     model = Models.__dict__[args.model_name](num_class=num_class)\n",
    "#     if args.model_name in ['wrn40_1_cifar', 'mobilenetv2', 'shufflev1_cifar', 'shufflev2_cifar']:\n",
    "#         model = EmbTrans(student=model, model_name=args.model_name)\n",
    "#     teacher = Models.__dict__[args.teacher](num_class=num_class)\n",
    "\n",
    "#     if args.teacher_weights:\n",
    "#         print('Load Teacher Weights')\n",
    "#         teacher_ckpt = torch.load(args.teacher_weights)['model']\n",
    "#         teacher.load_state_dict(teacher_ckpt)\n",
    "\n",
    "#         for param in teacher.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#     # res56    ./ckpt/teacher/resnet56/center_emb_train.json\n",
    "#     # res32x4  ./ckpt/teacher/resnet32x4/center_emb_train.json\n",
    "#     # wrn40_2  ./ckpt/teacher/wrn_40_2/center_emb_train.json\n",
    "#     # res50    ./ckpt/teacher/resnet50/center_emb_train.json\n",
    "#     # class-mean\n",
    "#     with open(\"./ckpt/teacher/resnet56/center_emb_train.json\", 'r') as f:\n",
    "#         T_EMB = json.load(f)\n",
    "#     f.close()\n",
    "\n",
    "#     logger.info(colorstr('green', 'Use ' + args.teacher + ' Training ' + args.model_name + ' ...'))\n",
    "#     # Train the model\n",
    "#     epoch_loop(model=model, teacher=teacher, train_set=train_set, test_set=test_set, args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLmP_umm2qpf"
   },
   "source": [
    "##DIST++\n",
    "\n",
    "cd ./DIST++\n",
    "\n",
    "\n",
    "```\n",
    "python3 train_dist.py \\\n",
    "    --model_name shufflenetv2 \\\n",
    "    --teacher resnet32x4 \\\n",
    "    --teacher_weights 'Teacher WEIGHT PATH' \\\n",
    "    --dataset 'cifar100' \\\n",
    "    --epoch 240 \\\n",
    "    --batch_size 64 \\\n",
    "    --lr 0.02 \\\n",
    "    --cls_loss_factor 1.0 \\\n",
    "    --kd_loss_factor 3.0 \\\n",
    "    --nd_loss_factor 4.0 \\\n",
    "    --save_dir \"./run/CIFAR100/DIST++/res32X4-SV2\"\n",
    "```\n",
    "\n",
    "\n",
    "other models, please refer to train_dist.sh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1698428927228,
     "user": {
      "displayName": "Michelle Bolner",
      "userId": "00246950705303252689"
     },
     "user_tz": 240
    },
    "id": "UR2vdhzk2iiH",
    "outputId": "bbb94018-8711-4ec9-c1c7-f44f8fb84f3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/210_Competition_Models/#2_Knowledge-Distillation-via-ND-main/CIFAR/DIST++\n"
     ]
    }
   ],
   "source": [
    "%cd ./DIST++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ruiGkZw89EN"
   },
   "source": [
    "Because their utils.py folder was not loading in properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 185,
     "status": "ok",
     "timestamp": 1698428931632,
     "user": {
      "displayName": "Michelle Bolner",
      "userId": "00246950705303252689"
     },
     "user_tz": 240
    },
    "id": "vrW46PsX8HGN"
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pdb\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def colorstr(*input):\n",
    "    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')\n",
    "    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string\n",
    "    colors = {'black': '\\033[30m',  # basic colors\n",
    "              'red': '\\033[31m',\n",
    "              'green': '\\033[32m',\n",
    "              'yellow': '\\033[33m',\n",
    "              'blue': '\\033[34m',\n",
    "              'magenta': '\\033[35m',\n",
    "              'cyan': '\\033[36m',\n",
    "              'white': '\\033[37m',\n",
    "              'bright_black': '\\033[90m',  # bright colors\n",
    "              'bright_red': '\\033[91m',\n",
    "              'bright_green': '\\033[92m',\n",
    "              'bright_yellow': '\\033[93m',\n",
    "              'bright_blue': '\\033[94m',\n",
    "              'bright_magenta': '\\033[95m',\n",
    "              'bright_cyan': '\\033[96m',\n",
    "              'bright_white': '\\033[97m',\n",
    "              'end': '\\033[0m',  # misc\n",
    "              'bold': '\\033[1m',\n",
    "              'underline': '\\033[4m'}\n",
    "    return ''.join(colors[x] for x in args) + f'{string}' + colors['end']\n",
    "\n",
    "\n",
    "def Save_Checkpoint(state, last, last_path, best, best_path, is_best):\n",
    "    if os.path.exists(last):\n",
    "        shutil.rmtree(last)\n",
    "    last_path.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(state, os.path.join(last_path, 'ckpt.pth'))\n",
    "\n",
    "    if is_best:\n",
    "        if os.path.exists(best):\n",
    "            shutil.rmtree(best)\n",
    "        best_path.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(state, os.path.join(best_path, 'ckpt.pth'))\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b, eps=1e-8):\n",
    "    return (a * b).sum(1) / (a.norm(dim=1) * b.norm(dim=1) + eps)\n",
    "\n",
    "\n",
    "def pearson_correlation(a, b, eps=1e-8):\n",
    "    return cosine_similarity(a - a.mean(1).unsqueeze(1),\n",
    "                             b - b.mean(1).unsqueeze(1), eps)\n",
    "\n",
    "\n",
    "def inter_class_relation(y_s, y_t):\n",
    "    return 1 - pearson_correlation(y_s, y_t).mean()\n",
    "\n",
    "\n",
    "def intra_class_relation(y_s, y_t):\n",
    "    return inter_class_relation(y_s.transpose(0, 1), y_t.transpose(0, 1))\n",
    "\n",
    "\n",
    "class DIST(nn.Module):\n",
    "    def __init__(self, beta=1.0, gamma=1.0, tau=1.0):\n",
    "        super(DIST, self).__init__()\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, z_s, z_t):\n",
    "        # y_s = (z_s / self.tau).softmax(dim=1)\n",
    "        # y_t = (z_t / self.tau).softmax(dim=1)\n",
    "        y_s = F.softmax(z_s / self.tau, dim=1)\n",
    "        y_t = F.softmax(z_t / self.tau, dim=1)\n",
    "        inter_loss = self.tau**2 * inter_class_relation(y_s, y_t)\n",
    "        intra_loss = self.tau**2 * intra_class_relation(y_s, y_t)\n",
    "        kd_loss = self.beta * inter_loss + self.gamma * intra_loss\n",
    "        return kd_loss\n",
    "\n",
    "\n",
    "class DirectNormLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class=100, nd_loss_factor=1.0):\n",
    "        super(DirectNormLoss, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.nd_loss_factor = nd_loss_factor\n",
    "\n",
    "    def project_center(self, s_emb, t_emb, T_EMB, labels):\n",
    "        assert s_emb.size() == t_emb.size()\n",
    "        assert s_emb.shape[0] == len(labels)\n",
    "        loss = 0.0\n",
    "        for s, t, i in zip(s_emb, t_emb, labels):\n",
    "            i = i.item()\n",
    "            center = torch.tensor(T_EMB[str(i)]).cuda()\n",
    "            e_c = center / center.norm(p=2)\n",
    "            max_norm = max(s.norm(p=2), t.norm(p=2))\n",
    "            loss += 1 - torch.dot(s, e_c) / max_norm\n",
    "        return loss\n",
    "\n",
    "    def forward(self, s_emb, t_emb, T_EMB, labels):\n",
    "        nd_loss = self.project_center(s_emb=s_emb, t_emb=t_emb, T_EMB=T_EMB, labels=labels) * self.nd_loss_factor\n",
    "\n",
    "        return nd_loss / len(labels)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Copied from: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 8018,
     "status": "error",
     "timestamp": 1698428941003,
     "user": {
      "displayName": "Michelle Bolner",
      "userId": "00246950705303252689"
     },
     "user_tz": 240
    },
    "id": "yTxiYYLE23ky",
    "outputId": "21a943f0-0cb8-4d9d-a642-cd94b5025aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Load Teacher Weights\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f6160c8735fc>\u001b[0m in \u001b[0;36m<cell line: 246>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolorstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Use '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteacher\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' Training '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mepoch_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-f6160c8735fc>\u001b[0m in \u001b[0;36mepoch_loop\u001b[0;34m(model, teacher, train_set, test_set, args)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error = train(model=model,\n\u001b[0m\u001b[1;32m    179\u001b[0m                                                                                  \u001b[0mteacher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                                                                                  \u001b[0mT_EMB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_EMB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f6160c8735fc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, teacher, T_EMB, train_dataloader, optimizer, criterion, kd_loss, nd_loss, args, epoch)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mdiv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_logits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkd_loss_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarm_up\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# ND loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mnorm_dir_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_EMB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_EMB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiv_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnorm_dir_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6b5cfe2d7a60>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s_emb, t_emb, T_EMB, labels)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_EMB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mnd_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproject_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_EMB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_EMB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd_loss_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnd_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6b5cfe2d7a60>\u001b[0m in \u001b[0;36mproject_center\u001b[0;34m(self, s_emb, t_emb, T_EMB, labels)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mproject_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_EMB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0ms_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0ms_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '3,2'\n",
    "# import torch\n",
    "# import torchvision\n",
    "# from torch import nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torch.nn.functional as F\n",
    "# import torch.backends.cudnn as cudnn\n",
    "\n",
    "# import Models\n",
    "# from Models.embtrans_cifar import EmbTrans\n",
    "# from Dataset import CIFAR\n",
    "# # from utils import colorstr, Save_Checkpoint, AverageMeter, DirectNormLoss, DIST\n",
    "# # from utils import colorstr, Save_Checkpoint, AverageMeter, DirectNormLoss, KDLoss\n",
    "\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "# import time\n",
    "# import json\n",
    "# import random\n",
    "# import logging\n",
    "# import argparse\n",
    "# import warnings\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# import pdb\n",
    "\n",
    "# def train(model, teacher, T_EMB, train_dataloader, optimizer, criterion, kd_loss, nd_loss, args, epoch):\n",
    "#     train_loss = AverageMeter()\n",
    "#     train_error = AverageMeter()\n",
    "\n",
    "#     Cls_loss = AverageMeter()\n",
    "#     Div_loss = AverageMeter()\n",
    "#     Norm_Dir_loss = AverageMeter()\n",
    "\n",
    "#     # Model on train mode\n",
    "#     model.train()\n",
    "#     teacher.eval()\n",
    "#     step_per_epoch = len(train_dataloader)\n",
    "\n",
    "#     for step, (images, labels) in enumerate(train_dataloader):\n",
    "#         start = time.time()\n",
    "#         images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "#         # compute output\n",
    "#         s_emb, s_logits = model(images, embed=True)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             t_emb, t_logits = teacher(images, embed=True)\n",
    "\n",
    "#         # cls loss\n",
    "#         cls_loss = criterion(s_logits, labels) * args.cls_loss_factor\n",
    "#         # Kd loss\n",
    "#         div_loss = kd_loss(s_logits, t_logits) * args.kd_loss_factor * min(1.0, epoch/args.warm_up)\n",
    "#         # ND loss\n",
    "#         norm_dir_loss = nd_loss(s_emb=s_emb, t_emb=t_emb, T_EMB=T_EMB, labels=labels)\n",
    "\n",
    "#         loss = cls_loss + div_loss + norm_dir_loss\n",
    "#         # measure accuracy and record loss\n",
    "#         batch_size = images.size(0)\n",
    "#         _, pred = s_logits.data.cpu().topk(1, dim=1)\n",
    "#         train_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "#         train_loss.update(loss.item(), batch_size)\n",
    "\n",
    "#         Cls_loss.update(cls_loss.item(), batch_size)\n",
    "#         Div_loss.update(div_loss.item(), batch_size)\n",
    "#         Norm_Dir_loss.update(norm_dir_loss.item(), batch_size)\n",
    "\n",
    "#         # compute gradient and do SGD step\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "#         s1 = '\\r{} [{}/{}]'.format(t, step+1, step_per_epoch)\n",
    "#         s2 = ' - {:.2f}ms/step - nd_loss: {:.3f} - div_loss: {:.3f} - cls_loss: {:.3f} - train_loss: {:.3f} - train_acc: {:.3f}'.format(\n",
    "#              1000 * (time.time() - start), norm_dir_loss.item(), div_loss.item(), cls_loss.item(), train_loss.val, 1-train_error.val)\n",
    "\n",
    "#         print(s1+s2, end='', flush=True)\n",
    "\n",
    "#     print()\n",
    "#     return Norm_Dir_loss.avg, Div_loss.avg, Cls_loss.avg, train_loss.avg, train_error.avg\n",
    "\n",
    "\n",
    "# def test(model, test_dataloader, criterion):\n",
    "#     test_loss = AverageMeter()\n",
    "#     test_error = AverageMeter()\n",
    "\n",
    "#     # Model on eval mode\n",
    "#     model.eval()\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, labels in test_dataloader:\n",
    "#             images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "#             # compute logits\n",
    "#             logits = model(images, embed=False)\n",
    "\n",
    "#             loss = criterion(logits, labels)\n",
    "\n",
    "#             # measure accuracy and record loss\n",
    "#             batch_size = images.size(0)\n",
    "#             _, pred = logits.data.cpu().topk(1, dim=1)\n",
    "#             test_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "#             test_loss.update(loss.item(), batch_size)\n",
    "\n",
    "#     return test_loss.avg, test_error.avg\n",
    "\n",
    "\n",
    "# def epoch_loop(model, teacher, train_set, test_set, args):\n",
    "#     # data loaders\n",
    "#     train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=args.workers)\n",
    "#     test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=args.workers)\n",
    "\n",
    "#     # model\n",
    "#     device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#     # model = nn.DataParallel(model, device_ids=args.gpus)\n",
    "#     model = nn.DataParallel(model)\n",
    "#     model.to(device)\n",
    "#     # teacher = nn.DataParallel(teacher, device_ids=args.gpus)\n",
    "#     teacher = nn.DataParallel(teacher)\n",
    "#     teacher.to(device)\n",
    "\n",
    "#     # loss\n",
    "#     criterion = nn.CrossEntropyLoss().to(device)\n",
    "#     kd_loss = DIST(beta=1, gamma=1, tau=args.t).to(device)\n",
    "#     nd_loss = DirectNormLoss(num_class=100, nd_loss_factor=args.nd_loss_factor).to(device)\n",
    "#     # optimizer\n",
    "#     optimizer = torch.optim.SGD(params=model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)\n",
    "\n",
    "#     # weights\n",
    "#     save_dir = Path(args.save_dir)\n",
    "#     weights = save_dir / 'weights'\n",
    "#     weights.mkdir(parents=True, exist_ok=True)\n",
    "#     last = weights / 'last'\n",
    "#     best = weights / 'best'\n",
    "\n",
    "#     # acc,loss\n",
    "#     acc_loss = save_dir / 'acc_loss'\n",
    "#     acc_loss.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#     train_acc_savepath = acc_loss / 'train_acc.npy'\n",
    "#     train_loss_savepath = acc_loss / 'train_loss.npy'\n",
    "#     val_acc_savepath = acc_loss / 'val_acc.npy'\n",
    "#     val_loss_savepath = acc_loss / 'val_loss.npy'\n",
    "\n",
    "#     # tensorboard\n",
    "#     logdir = save_dir / 'logs'\n",
    "#     logdir.mkdir(parents=True, exist_ok=True)\n",
    "#     summary_writer = SummaryWriter(logdir, flush_secs=120)\n",
    "\n",
    "#     # resume\n",
    "#     if args.resume:\n",
    "#         checkpoint = torch.load(args.resume)\n",
    "#         start_epoch = checkpoint['epoch']\n",
    "#         model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         best_error = checkpoint['best_error']\n",
    "#         train_acc = checkpoint['train_acc']\n",
    "#         train_loss = checkpoint['train_loss']\n",
    "#         test_acc = checkpoint['test_acc']\n",
    "#         test_loss = checkpoint['test_loss']\n",
    "#         logger.info(colorstr('green', 'Resuming training from {} epoch'.format(start_epoch)))\n",
    "#     else:\n",
    "#         start_epoch = 0\n",
    "#         best_error = 0\n",
    "#         train_acc = []\n",
    "#         train_loss = []\n",
    "#         test_acc = []\n",
    "#         test_loss = []\n",
    "\n",
    "#     # Train model\n",
    "#     best_error = 1\n",
    "#     for epoch in range(start_epoch, args.epochs):\n",
    "#         if epoch in [150, 180, 210]:\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] *= 0.1\n",
    "#         print(\"Epoch {}/{}\".format(epoch + 1, args.epochs))\n",
    "#         norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error = train(model=model,\n",
    "#                                                                                  teacher=teacher,\n",
    "#                                                                                  T_EMB=T_EMB,\n",
    "#                                                                                  train_dataloader=train_loader,\n",
    "#                                                                                  optimizer=optimizer,\n",
    "#                                                                                  criterion=criterion,\n",
    "#                                                                                  kd_loss=kd_loss,\n",
    "#                                                                                  nd_loss=nd_loss,\n",
    "#                                                                                  args=args,\n",
    "#                                                                                  epoch=epoch)\n",
    "#         test_epoch_loss, test_error = test(model=model,\n",
    "#                                            test_dataloader=test_loader,\n",
    "#                                            criterion=criterion)\n",
    "\n",
    "#         s = \"Train Loss: {:.3f}, Train Acc: {:.3f}, Test Loss: {:.3f}, Test Acc: {:.3f}, lr: {:.5f}\".format(\n",
    "#             train_epoch_loss, 1-train_error, test_epoch_loss, 1-test_error, optimizer.param_groups[0]['lr'])\n",
    "#         logger.info(colorstr('green', s))\n",
    "\n",
    "#         # save acc,loss\n",
    "#         train_loss.append(train_epoch_loss)\n",
    "#         train_acc.append(1-train_error)\n",
    "#         test_loss.append(test_epoch_loss)\n",
    "#         test_acc.append(1-test_error)\n",
    "\n",
    "#         # save model\n",
    "#         is_best = test_error < best_error\n",
    "#         best_error = min(best_error, test_error)\n",
    "#         state = {\n",
    "#                 'epoch': epoch + 1,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'best_error': best_error,\n",
    "#                 'train_acc': train_acc,\n",
    "#                 'train_loss': train_loss,\n",
    "#                 'test_acc': test_acc,\n",
    "#                 'test_loss': test_loss,\n",
    "#             }\n",
    "\n",
    "#         last_path = last / 'epoch_{}_loss_{:.3f}_acc_{:.3f}'.format(\n",
    "#             epoch + 1, test_epoch_loss, 1-test_error)\n",
    "#         best_path = best / 'epoch_{}_acc_{:.3f}'.format(\n",
    "#                 epoch + 1, 1-best_error)\n",
    "\n",
    "#         Save_Checkpoint(state, last, last_path, best, best_path, is_best)\n",
    "\n",
    "#         # tensorboard\n",
    "#         if epoch == 1:\n",
    "#             images, labels = next(iter(train_loader))\n",
    "#             img_grid = torchvision.utils.make_grid(images)\n",
    "#             summary_writer.add_image('Cifar Image', img_grid)\n",
    "#         summary_writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "#         summary_writer.add_scalar('train_loss', train_epoch_loss, epoch)\n",
    "#         summary_writer.add_scalar('train_error', train_error, epoch)\n",
    "#         summary_writer.add_scalar('val_loss', test_epoch_loss, epoch)\n",
    "#         summary_writer.add_scalar('val_error', test_error, epoch)\n",
    "\n",
    "#         summary_writer.add_scalar('nd_loss', norm_dir_loss, epoch)\n",
    "#         summary_writer.add_scalar('kd_loss', div_loss, epoch)\n",
    "#         summary_writer.add_scalar('cls_loss', cls_loss, epoch)\n",
    "\n",
    "#     summary_writer.close()\n",
    "#     if not os.path.exists(train_acc_savepath) or not os.path.exists(train_loss_savepath):\n",
    "#         np.save(train_acc_savepath, train_acc)\n",
    "#         np.save(train_loss_savepath, train_loss)\n",
    "#         np.save(val_acc_savepath, test_acc)\n",
    "#         np.save(val_loss_savepath, test_loss)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     model_names = sorted(name for name in Models.__dict__\n",
    "#                          if name.islower() and not name.startswith(\"__\")\n",
    "#                          and callable(Models.__dict__[name]))\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='PyTorch Cifar Training')\n",
    "#     parser.add_argument('-f') # added to make this run in collab\n",
    "#     parser.add_argument(\"--model_name\", type=str, default=\"resnet20_cifar\", choices=model_names, help=\"model architecture\")\n",
    "#     parser.add_argument(\"--dataset\", type=str, default='cifar100')\n",
    "#     # parser.add_argument(\"--epochs\", type=int, default=240)\n",
    "#     parser.add_argument(\"--epochs\", type=int, default=4)\n",
    "#     parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size per gpu\")\n",
    "#     parser.add_argument('--workers', default=8, type=int, help='number of data loading workers')\n",
    "#     parser.add_argument(\"--lr\", type=float, default=0.1)\n",
    "#     parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum')\n",
    "#     parser.add_argument(\"--weight_decay\", type=float, default=5e-4)\n",
    "\n",
    "#     parser.add_argument(\"--teacher\", type=str, default=\"resnet32x4_cifar\", help=\"teacher architecture\")\n",
    "#     parser.add_argument(\"--teacher_weights\", type=str, default=\"../ckpt/cifar_teachers/resnet32x4_vanilla/ckpt_epoch_240.pth\", help=\"teacher weights path\")\n",
    "#     parser.add_argument(\"--cls_loss_factor\", type=float, default=1.0, help=\"cls loss weight factor\")\n",
    "#     parser.add_argument(\"--kd_loss_factor\", type=float, default=1.0, help=\"KL loss weight factor\")\n",
    "#     parser.add_argument(\"--t\", type=float, default=4.0, help=\"temperature\")\n",
    "#     parser.add_argument(\"--nd_loss_factor\", type=float, default=1.0, help=\"ND loss weight factor\")\n",
    "#     parser.add_argument(\"--warm_up\", type=float, default=20.0, help='loss weight warm up epochs')\n",
    "\n",
    "#     # parser.add_argument(\"--gpus\", type=list, default=[0, 1])\n",
    "#     parser.add_argument('--seed', default=None, type=int, help='seed for initializing training.')\n",
    "#     parser.add_argument(\"--resume\", type=str, help=\"best ckpt's path to resume most recent training\")\n",
    "#     parser.add_argument(\"--save_dir\", type=str, default=\"./run\", help=\"save path, eg, acc_loss, weights, tensorboard, and so on\")\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     if args.seed is not None:\n",
    "#         random.seed(args.seed)\n",
    "#         torch.manual_seed(args.seed)\n",
    "#         cudnn.deterministic = True\n",
    "#         cudnn.benchmark = False\n",
    "#         warnings.warn('You have chosen to seed training. '\n",
    "#                       'This will turn on the CUDNN deterministic setting, '\n",
    "#                       'which can slow down your training considerably! '\n",
    "#                       'You may see unexpected behavior when restarting '\n",
    "#                       'from checkpoints.')\n",
    "\n",
    "#     logging.basicConfig(level=logging.INFO, format='%(asctime)s [line:%(lineno)d] %(message)s',\n",
    "#                         datefmt='%d %b %Y %H:%M:%S')\n",
    "#     logger = logging.getLogger(__name__)\n",
    "\n",
    "#     # args.batch_size = args.batch_size * len(args.gpus)\n",
    "#     args.batch_size = args.batch_size * 1\n",
    "\n",
    "\n",
    "#     # logger.info(colorstr('green', \"Distribute train, gpus:{}, total batch size:{}, epoch:{}\".format(args.gpus, args.batch_size, args.epochs)))\n",
    "#     logger.info(colorstr('green', \"Distribute train, total batch size:{}, epoch:{}\".format(args.batch_size, args.epochs)))\n",
    "\n",
    "#     train_set, test_set, num_class = CIFAR(name=args.dataset)\n",
    "#     model = Models.__dict__[args.model_name](num_class=num_class)\n",
    "#     if args.model_name in ['wrn40_1_cifar', 'mobilenetv2', 'shufflev1_cifar', 'shufflev2_cifar']:\n",
    "#         model = EmbTrans(student=model, model_name=args.model_name)\n",
    "#     teacher = Models.__dict__[args.teacher](num_class=num_class)\n",
    "\n",
    "#     # resnet56/resnet32x4, no need emb size\n",
    "#     if args.teacher_weights:\n",
    "#         print('Load Teacher Weights')\n",
    "#         teacher_ckpt = torch.load(args.teacher_weights)['model']\n",
    "#         teacher.load_state_dict(teacher_ckpt)\n",
    "\n",
    "#         for param in teacher.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#     # res56    ./ckpt/teacher/resnet56/center_emb_train.json\n",
    "#     # res32x4  ./ckpt/teacher/resnet32x4/center_emb_train.json\n",
    "#     # wrn40_2  ./ckpt/teacher/wrn_40_2/center_emb_train.json\n",
    "#     # res50    ./ckpt/teacher/resnet50/center_emb_train.json\n",
    "#     # class-mean\n",
    "#     with open(\"./ckpt/teacher/resnet32x4/center_emb_train.json\", 'r') as f:\n",
    "#         T_EMB = json.load(f)\n",
    "#     f.close()\n",
    "\n",
    "#     logger.info(colorstr('green', 'Use ' + args.teacher + ' Training ' + args.model_name + ' ...'))\n",
    "#     # Train the model\n",
    "#     epoch_loop(model=model, teacher=teacher, train_set=train_set, test_set=test_set, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1698424053690,
     "user": {
      "displayName": "Michelle Bolner",
      "userId": "00246950705303252689"
     },
     "user_tz": 240
    },
    "id": "qleJ_VWU4Byx",
    "outputId": "9d3efd22-ee52-444f-90aa-6e5729f0b25e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACp2rHU7QcI4"
   },
   "source": [
    "#4 Review KD++\n",
    "## Review KD++\n",
    "\n",
    "Top model on leaderboard -> T:resnet-32x4, S:shufflenet-v2\n",
    "\n",
    "Next best ->\n",
    "T:resnet-32x4, S:shufflenet-v1\n",
    "\n",
    "```\n",
    "python3 train_reviewkd.py \\\n",
    "    --model_name wrn40_1_cifar \\\n",
    "    --teacher wrn40_2_cifar \\\n",
    "    --teacher_weights 'Teacher WEIGHT PATH' \\\n",
    "    --dataset 'cifar100' \\\n",
    "    --epoch 240 \\\n",
    "    --batch_size 64 \\\n",
    "    --lr 0.1 \\\n",
    "    --cls_loss_factor 1.0 \\\n",
    "    --kd_loss_factor 5.0 \\\n",
    "    --nd_loss_factor 4.0 \\\n",
    "    --save_dir \"./run/CIFAR100/ReviewKD++/res32X4-SV2\"\n",
    "\n",
    "# other models, please refer to train_reviewkd.sh.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIST++\n",
      "Dataset\n",
      "Improving_KD_via_Regularizing_Feature_Norm_and_Direction_MB.ipynb\n",
      "Models\n",
      "README.md\n",
      "ReviewKD++\n",
      "__pycache__\n",
      "ckpt\n",
      "emb_fea_distribution.py\n",
      "emb_visual.sh\n",
      "fiftyone_KD_ND.ipynb\n",
      "run\n",
      "train_cifar.sh\n",
      "train_cifar_baseline.py\n",
      "train_cifar_dkd.py\n",
      "train_cifar_kd.py\n",
      "utils.py\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "vzppCOuI8hYZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/W210-Capstone/competition_models/MB_Knowledge-Distillation-via-ND/CIFAR/ReviewKD++\n"
     ]
    }
   ],
   "source": [
    "%cd ./ReviewKD++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "sdQfroNMQsXo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:50:17 [line:298] \u001b[32mDistribute train, total batch size:128, epoch:240\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to Dataset/data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 169001437/169001437 [00:05<00:00, 31520293.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Dataset/data/cifar-100-python.tar.gz to Dataset/data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:50:26 [line:321] \u001b[32mUse wrn40_2_cifar Training wrn40_1_cifar ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Teacher Weights\n",
      "Epoch 1/240\n",
      "2023-10-30 00:51:01 [391/391] - 154.56ms/step - nd_loss: 0.296 - div_loss: 0.000 - cls_loss: 3.425 - train_loss: 3.720 - train_acc: 0.188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:51:02 [line:195] \u001b[32mTrain Loss: 4.391, Train Acc: 0.074, Test Loss: 3.770, Test Acc: 0.115, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/240\n",
      "2023-10-30 00:51:37 [391/391] - 55.05ms/step - nd_loss: 0.280 - div_loss: 0.165 - cls_loss: 2.963 - train_loss: 3.407 - train_acc: 0.200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:51:38 [line:195] \u001b[32mTrain Loss: 3.820, Train Acc: 0.175, Test Loss: 3.439, Test Acc: 0.171, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/240\n",
      "2023-10-30 00:52:14 [391/391] - 55.58ms/step - nd_loss: 0.274 - div_loss: 0.260 - cls_loss: 2.794 - train_loss: 3.328 - train_acc: 0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:52:15 [line:195] \u001b[32mTrain Loss: 3.382, Train Acc: 0.274, Test Loss: 3.168, Test Acc: 0.230, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/240\n",
      "2023-10-30 00:52:50 [391/391] - 56.16ms/step - nd_loss: 0.272 - div_loss: 0.378 - cls_loss: 2.582 - train_loss: 3.232 - train_acc: 0.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:52:51 [line:195] \u001b[32mTrain Loss: 3.157, Train Acc: 0.336, Test Loss: 2.731, Test Acc: 0.308, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/240\n",
      "2023-10-30 00:53:26 [391/391] - 56.13ms/step - nd_loss: 0.263 - div_loss: 0.438 - cls_loss: 2.325 - train_loss: 3.027 - train_acc: 0.425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:53:27 [line:195] \u001b[32mTrain Loss: 3.027, Train Acc: 0.386, Test Loss: 2.502, Test Acc: 0.340, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/240\n",
      "2023-10-30 00:54:02 [391/391] - 55.58ms/step - nd_loss: 0.257 - div_loss: 0.541 - cls_loss: 1.978 - train_loss: 2.776 - train_acc: 0.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:54:03 [line:195] \u001b[32mTrain Loss: 2.952, Train Acc: 0.423, Test Loss: 2.343, Test Acc: 0.384, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/240\n",
      "2023-10-30 00:54:38 [391/391] - 55.54ms/step - nd_loss: 0.258 - div_loss: 0.641 - cls_loss: 1.993 - train_loss: 2.892 - train_acc: 0.325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:54:39 [line:195] \u001b[32mTrain Loss: 2.906, Train Acc: 0.451, Test Loss: 2.363, Test Acc: 0.399, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/240\n",
      "2023-10-30 00:55:14 [391/391] - 56.66ms/step - nd_loss: 0.253 - div_loss: 0.710 - cls_loss: 1.555 - train_loss: 2.518 - train_acc: 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:55:15 [line:195] \u001b[32mTrain Loss: 2.894, Train Acc: 0.474, Test Loss: 2.157, Test Acc: 0.425, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/240\n",
      "2023-10-30 00:55:50 [391/391] - 55.49ms/step - nd_loss: 0.252 - div_loss: 0.882 - cls_loss: 2.070 - train_loss: 3.203 - train_acc: 0.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:55:51 [line:195] \u001b[32mTrain Loss: 2.895, Train Acc: 0.494, Test Loss: 1.991, Test Acc: 0.454, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/240\n",
      "2023-10-30 00:56:26 [391/391] - 57.09ms/step - nd_loss: 0.246 - div_loss: 0.944 - cls_loss: 1.770 - train_loss: 2.959 - train_acc: 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:56:27 [line:195] \u001b[32mTrain Loss: 2.920, Train Acc: 0.511, Test Loss: 2.281, Test Acc: 0.413, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/240\n",
      "2023-10-30 00:57:02 [391/391] - 55.58ms/step - nd_loss: 0.246 - div_loss: 0.979 - cls_loss: 1.689 - train_loss: 2.914 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:57:03 [line:195] \u001b[32mTrain Loss: 2.944, Train Acc: 0.524, Test Loss: 1.977, Test Acc: 0.475, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/240\n",
      "2023-10-30 00:57:38 [391/391] - 55.68ms/step - nd_loss: 0.243 - div_loss: 1.066 - cls_loss: 1.690 - train_loss: 2.999 - train_acc: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:57:39 [line:195] \u001b[32mTrain Loss: 2.990, Train Acc: 0.534, Test Loss: 2.113, Test Acc: 0.455, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/240\n",
      "2023-10-30 00:58:14 [391/391] - 55.52ms/step - nd_loss: 0.243 - div_loss: 1.142 - cls_loss: 1.370 - train_loss: 2.755 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:58:15 [line:195] \u001b[32mTrain Loss: 3.031, Train Acc: 0.546, Test Loss: 1.969, Test Acc: 0.477, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/240\n",
      "2023-10-30 00:58:50 [391/391] - 55.47ms/step - nd_loss: 0.246 - div_loss: 1.263 - cls_loss: 1.606 - train_loss: 3.116 - train_acc: 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:58:51 [line:195] \u001b[32mTrain Loss: 3.076, Train Acc: 0.554, Test Loss: 2.258, Test Acc: 0.433, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/240\n",
      "2023-10-30 00:59:26 [391/391] - 55.40ms/step - nd_loss: 0.248 - div_loss: 1.336 - cls_loss: 1.834 - train_loss: 3.418 - train_acc: 0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 00:59:27 [line:195] \u001b[32mTrain Loss: 3.116, Train Acc: 0.565, Test Loss: 2.004, Test Acc: 0.476, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/240\n",
      "2023-10-30 01:00:02 [391/391] - 55.75ms/step - nd_loss: 0.243 - div_loss: 1.403 - cls_loss: 1.460 - train_loss: 3.106 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:00:04 [line:195] \u001b[32mTrain Loss: 3.172, Train Acc: 0.568, Test Loss: 2.042, Test Acc: 0.479, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/240\n",
      "2023-10-30 01:00:39 [391/391] - 56.06ms/step - nd_loss: 0.234 - div_loss: 1.361 - cls_loss: 1.323 - train_loss: 2.918 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:00:40 [line:195] \u001b[32mTrain Loss: 3.216, Train Acc: 0.579, Test Loss: 2.174, Test Acc: 0.439, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/240\n",
      "2023-10-30 01:01:15 [391/391] - 56.36ms/step - nd_loss: 0.233 - div_loss: 1.513 - cls_loss: 1.366 - train_loss: 3.113 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:01:16 [line:195] \u001b[32mTrain Loss: 3.275, Train Acc: 0.581, Test Loss: 1.949, Test Acc: 0.488, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/240\n",
      "2023-10-30 01:01:51 [391/391] - 56.41ms/step - nd_loss: 0.244 - div_loss: 1.626 - cls_loss: 1.712 - train_loss: 3.582 - train_acc: 0.6138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:01:52 [line:195] \u001b[32mTrain Loss: 3.326, Train Acc: 0.589, Test Loss: 1.745, Test Acc: 0.524, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/240\n",
      "2023-10-30 01:02:27 [391/391] - 55.51ms/step - nd_loss: 0.236 - div_loss: 1.828 - cls_loss: 1.609 - train_loss: 3.672 - train_acc: 0.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:02:28 [line:195] \u001b[32mTrain Loss: 3.376, Train Acc: 0.592, Test Loss: 2.052, Test Acc: 0.478, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/240\n",
      "2023-10-30 01:03:03 [391/391] - 55.91ms/step - nd_loss: 0.242 - div_loss: 1.896 - cls_loss: 1.433 - train_loss: 3.571 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:03:04 [line:195] \u001b[32mTrain Loss: 3.434, Train Acc: 0.598, Test Loss: 2.054, Test Acc: 0.478, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/240\n",
      "2023-10-30 01:03:39 [391/391] - 55.70ms/step - nd_loss: 0.239 - div_loss: 1.735 - cls_loss: 1.451 - train_loss: 3.424 - train_acc: 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:03:40 [line:195] \u001b[32mTrain Loss: 3.398, Train Acc: 0.601, Test Loss: 1.714, Test Acc: 0.531, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/240\n",
      "2023-10-30 01:04:15 [391/391] - 57.82ms/step - nd_loss: 0.248 - div_loss: 1.934 - cls_loss: 1.456 - train_loss: 3.637 - train_acc: 0.5507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:04:16 [line:195] \u001b[32mTrain Loss: 3.378, Train Acc: 0.606, Test Loss: 1.782, Test Acc: 0.523, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/240\n",
      "2023-10-30 01:04:51 [391/391] - 55.78ms/step - nd_loss: 0.240 - div_loss: 1.914 - cls_loss: 1.487 - train_loss: 3.640 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:04:52 [line:195] \u001b[32mTrain Loss: 3.359, Train Acc: 0.606, Test Loss: 2.121, Test Acc: 0.486, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/240\n",
      "2023-10-30 01:05:27 [391/391] - 55.60ms/step - nd_loss: 0.236 - div_loss: 1.858 - cls_loss: 1.456 - train_loss: 3.550 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:05:29 [line:195] \u001b[32mTrain Loss: 3.332, Train Acc: 0.614, Test Loss: 1.941, Test Acc: 0.493, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/240\n",
      "2023-10-30 01:06:03 [391/391] - 55.26ms/step - nd_loss: 0.234 - div_loss: 1.737 - cls_loss: 1.264 - train_loss: 3.235 - train_acc: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:06:04 [line:195] \u001b[32mTrain Loss: 3.313, Train Acc: 0.616, Test Loss: 1.905, Test Acc: 0.505, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/240\n",
      "2023-10-30 01:06:40 [391/391] - 55.54ms/step - nd_loss: 0.238 - div_loss: 1.733 - cls_loss: 1.493 - train_loss: 3.464 - train_acc: 0.5623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:06:41 [line:195] \u001b[32mTrain Loss: 3.290, Train Acc: 0.619, Test Loss: 1.969, Test Acc: 0.499, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/240\n",
      "2023-10-30 01:07:16 [391/391] - 56.40ms/step - nd_loss: 0.234 - div_loss: 1.750 - cls_loss: 1.300 - train_loss: 3.284 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:07:17 [line:195] \u001b[32mTrain Loss: 3.285, Train Acc: 0.618, Test Loss: 1.854, Test Acc: 0.525, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/240\n",
      "2023-10-30 01:07:52 [391/391] - 55.81ms/step - nd_loss: 0.224 - div_loss: 1.672 - cls_loss: 1.168 - train_loss: 3.063 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:07:53 [line:195] \u001b[32mTrain Loss: 3.273, Train Acc: 0.620, Test Loss: 2.197, Test Acc: 0.459, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/240\n",
      "2023-10-30 01:08:28 [391/391] - 56.55ms/step - nd_loss: 0.245 - div_loss: 1.724 - cls_loss: 1.274 - train_loss: 3.242 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:08:29 [line:195] \u001b[32mTrain Loss: 3.258, Train Acc: 0.623, Test Loss: 2.080, Test Acc: 0.500, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/240\n",
      "2023-10-30 01:09:04 [391/391] - 55.36ms/step - nd_loss: 0.237 - div_loss: 1.802 - cls_loss: 1.439 - train_loss: 3.478 - train_acc: 0.6132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:09:05 [line:195] \u001b[32mTrain Loss: 3.252, Train Acc: 0.625, Test Loss: 1.777, Test Acc: 0.530, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/240\n",
      "2023-10-30 01:09:40 [391/391] - 55.78ms/step - nd_loss: 0.235 - div_loss: 1.746 - cls_loss: 1.257 - train_loss: 3.238 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:09:41 [line:195] \u001b[32mTrain Loss: 3.241, Train Acc: 0.628, Test Loss: 2.220, Test Acc: 0.456, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/240\n",
      "2023-10-30 01:10:16 [391/391] - 55.57ms/step - nd_loss: 0.231 - div_loss: 1.697 - cls_loss: 1.340 - train_loss: 3.268 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:10:17 [line:195] \u001b[32mTrain Loss: 3.225, Train Acc: 0.630, Test Loss: 1.973, Test Acc: 0.505, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/240\n",
      "2023-10-30 01:10:52 [391/391] - 55.95ms/step - nd_loss: 0.233 - div_loss: 1.849 - cls_loss: 1.472 - train_loss: 3.554 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:10:53 [line:195] \u001b[32mTrain Loss: 3.220, Train Acc: 0.628, Test Loss: 1.778, Test Acc: 0.535, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/240\n",
      "2023-10-30 01:11:28 [391/391] - 55.59ms/step - nd_loss: 0.240 - div_loss: 1.807 - cls_loss: 1.542 - train_loss: 3.590 - train_acc: 0.5626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:11:29 [line:195] \u001b[32mTrain Loss: 3.211, Train Acc: 0.630, Test Loss: 1.897, Test Acc: 0.525, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/240\n",
      "2023-10-30 01:12:04 [391/391] - 55.58ms/step - nd_loss: 0.227 - div_loss: 1.565 - cls_loss: 1.382 - train_loss: 3.174 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:12:05 [line:195] \u001b[32mTrain Loss: 3.212, Train Acc: 0.628, Test Loss: 1.637, Test Acc: 0.563, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/240\n",
      "2023-10-30 01:12:40 [391/391] - 55.41ms/step - nd_loss: 0.230 - div_loss: 1.748 - cls_loss: 1.096 - train_loss: 3.074 - train_acc: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:12:41 [line:195] \u001b[32mTrain Loss: 3.191, Train Acc: 0.634, Test Loss: 1.776, Test Acc: 0.533, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/240\n",
      "2023-10-30 01:13:16 [391/391] - 56.06ms/step - nd_loss: 0.235 - div_loss: 1.546 - cls_loss: 1.249 - train_loss: 3.030 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:13:18 [line:195] \u001b[32mTrain Loss: 3.186, Train Acc: 0.636, Test Loss: 1.736, Test Acc: 0.541, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/240\n",
      "2023-10-30 01:13:53 [391/391] - 56.12ms/step - nd_loss: 0.230 - div_loss: 1.662 - cls_loss: 1.326 - train_loss: 3.218 - train_acc: 0.6375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:13:54 [line:195] \u001b[32mTrain Loss: 3.186, Train Acc: 0.634, Test Loss: 2.120, Test Acc: 0.475, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/240\n",
      "2023-10-30 01:14:29 [391/391] - 55.13ms/step - nd_loss: 0.252 - div_loss: 1.791 - cls_loss: 1.539 - train_loss: 3.582 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:14:30 [line:195] \u001b[32mTrain Loss: 3.165, Train Acc: 0.639, Test Loss: 1.896, Test Acc: 0.509, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/240\n",
      "2023-10-30 01:15:05 [391/391] - 56.10ms/step - nd_loss: 0.236 - div_loss: 1.666 - cls_loss: 1.448 - train_loss: 3.351 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:15:06 [line:195] \u001b[32mTrain Loss: 3.169, Train Acc: 0.637, Test Loss: 1.803, Test Acc: 0.539, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/240\n",
      "2023-10-30 01:15:41 [391/391] - 55.80ms/step - nd_loss: 0.224 - div_loss: 1.800 - cls_loss: 1.288 - train_loss: 3.311 - train_acc: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:15:42 [line:195] \u001b[32mTrain Loss: 3.174, Train Acc: 0.636, Test Loss: 1.910, Test Acc: 0.505, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/240\n",
      "2023-10-30 01:16:17 [391/391] - 55.54ms/step - nd_loss: 0.231 - div_loss: 1.689 - cls_loss: 1.421 - train_loss: 3.342 - train_acc: 0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:16:18 [line:195] \u001b[32mTrain Loss: 3.167, Train Acc: 0.639, Test Loss: 1.732, Test Acc: 0.547, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/240\n",
      "2023-10-30 01:16:53 [391/391] - 55.19ms/step - nd_loss: 0.242 - div_loss: 1.783 - cls_loss: 1.334 - train_loss: 3.358 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:16:54 [line:195] \u001b[32mTrain Loss: 3.146, Train Acc: 0.639, Test Loss: 1.907, Test Acc: 0.521, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/240\n",
      "2023-10-30 01:17:29 [391/391] - 55.75ms/step - nd_loss: 0.245 - div_loss: 1.687 - cls_loss: 1.415 - train_loss: 3.346 - train_acc: 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:17:30 [line:195] \u001b[32mTrain Loss: 3.165, Train Acc: 0.637, Test Loss: 1.700, Test Acc: 0.541, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/240\n",
      "2023-10-30 01:18:05 [391/391] - 55.46ms/step - nd_loss: 0.228 - div_loss: 1.607 - cls_loss: 1.494 - train_loss: 3.329 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:18:06 [line:195] \u001b[32mTrain Loss: 3.146, Train Acc: 0.640, Test Loss: 1.818, Test Acc: 0.529, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/240\n",
      "2023-10-30 01:18:42 [391/391] - 54.96ms/step - nd_loss: 0.231 - div_loss: 1.771 - cls_loss: 1.271 - train_loss: 3.274 - train_acc: 0.6502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:18:43 [line:195] \u001b[32mTrain Loss: 3.141, Train Acc: 0.642, Test Loss: 1.608, Test Acc: 0.565, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/240\n",
      "2023-10-30 01:19:18 [391/391] - 55.42ms/step - nd_loss: 0.236 - div_loss: 1.624 - cls_loss: 1.246 - train_loss: 3.106 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:19:19 [line:195] \u001b[32mTrain Loss: 3.139, Train Acc: 0.644, Test Loss: 1.836, Test Acc: 0.532, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/240\n",
      "2023-10-30 01:19:54 [391/391] - 55.32ms/step - nd_loss: 0.235 - div_loss: 1.766 - cls_loss: 1.361 - train_loss: 3.362 - train_acc: 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:19:55 [line:195] \u001b[32mTrain Loss: 3.128, Train Acc: 0.644, Test Loss: 1.670, Test Acc: 0.551, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/240\n",
      "2023-10-30 01:20:29 [391/391] - 55.43ms/step - nd_loss: 0.219 - div_loss: 1.568 - cls_loss: 0.991 - train_loss: 2.778 - train_acc: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:20:31 [line:195] \u001b[32mTrain Loss: 3.125, Train Acc: 0.648, Test Loss: 1.748, Test Acc: 0.546, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/240\n",
      "2023-10-30 01:21:06 [391/391] - 55.61ms/step - nd_loss: 0.230 - div_loss: 1.757 - cls_loss: 1.330 - train_loss: 3.317 - train_acc: 0.6373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:21:07 [line:195] \u001b[32mTrain Loss: 3.135, Train Acc: 0.642, Test Loss: 1.676, Test Acc: 0.557, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/240\n",
      "2023-10-30 01:21:42 [391/391] - 55.47ms/step - nd_loss: 0.227 - div_loss: 1.543 - cls_loss: 0.984 - train_loss: 2.754 - train_acc: 0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:21:43 [line:195] \u001b[32mTrain Loss: 3.112, Train Acc: 0.647, Test Loss: 1.734, Test Acc: 0.542, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/240\n",
      "2023-10-30 01:22:18 [391/391] - 56.06ms/step - nd_loss: 0.227 - div_loss: 1.585 - cls_loss: 1.415 - train_loss: 3.226 - train_acc: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:22:19 [line:195] \u001b[32mTrain Loss: 3.123, Train Acc: 0.644, Test Loss: 1.844, Test Acc: 0.527, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/240\n",
      "2023-10-30 01:22:54 [391/391] - 55.41ms/step - nd_loss: 0.224 - div_loss: 1.697 - cls_loss: 1.173 - train_loss: 3.094 - train_acc: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:22:55 [line:195] \u001b[32mTrain Loss: 3.112, Train Acc: 0.647, Test Loss: 1.677, Test Acc: 0.560, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/240\n",
      "2023-10-30 01:23:30 [391/391] - 55.66ms/step - nd_loss: 0.230 - div_loss: 1.661 - cls_loss: 1.276 - train_loss: 3.167 - train_acc: 0.7006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:23:31 [line:195] \u001b[32mTrain Loss: 3.119, Train Acc: 0.646, Test Loss: 1.907, Test Acc: 0.516, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/240\n",
      "2023-10-30 01:24:06 [391/391] - 55.92ms/step - nd_loss: 0.231 - div_loss: 1.679 - cls_loss: 1.471 - train_loss: 3.382 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:24:07 [line:195] \u001b[32mTrain Loss: 3.122, Train Acc: 0.643, Test Loss: 1.756, Test Acc: 0.532, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/240\n",
      "2023-10-30 01:24:42 [391/391] - 55.51ms/step - nd_loss: 0.232 - div_loss: 1.693 - cls_loss: 1.134 - train_loss: 3.059 - train_acc: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:24:43 [line:195] \u001b[32mTrain Loss: 3.107, Train Acc: 0.647, Test Loss: 2.159, Test Acc: 0.480, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/240\n",
      "2023-10-30 01:25:18 [391/391] - 55.33ms/step - nd_loss: 0.237 - div_loss: 1.644 - cls_loss: 1.259 - train_loss: 3.140 - train_acc: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:25:19 [line:195] \u001b[32mTrain Loss: 3.108, Train Acc: 0.648, Test Loss: 2.097, Test Acc: 0.501, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/240\n",
      "2023-10-30 01:25:54 [391/391] - 55.57ms/step - nd_loss: 0.228 - div_loss: 1.651 - cls_loss: 1.255 - train_loss: 3.133 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:25:55 [line:195] \u001b[32mTrain Loss: 3.111, Train Acc: 0.645, Test Loss: 1.713, Test Acc: 0.547, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/240\n",
      "2023-10-30 01:26:31 [391/391] - 55.22ms/step - nd_loss: 0.238 - div_loss: 1.847 - cls_loss: 1.562 - train_loss: 3.647 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:26:32 [line:195] \u001b[32mTrain Loss: 3.093, Train Acc: 0.650, Test Loss: 1.718, Test Acc: 0.555, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/240\n",
      "2023-10-30 01:27:07 [391/391] - 56.26ms/step - nd_loss: 0.230 - div_loss: 1.680 - cls_loss: 1.656 - train_loss: 3.566 - train_acc: 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:27:08 [line:195] \u001b[32mTrain Loss: 3.101, Train Acc: 0.650, Test Loss: 1.769, Test Acc: 0.541, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/240\n",
      "2023-10-30 01:27:43 [391/391] - 56.90ms/step - nd_loss: 0.239 - div_loss: 1.693 - cls_loss: 1.424 - train_loss: 3.355 - train_acc: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:27:44 [line:195] \u001b[32mTrain Loss: 3.090, Train Acc: 0.653, Test Loss: 2.087, Test Acc: 0.491, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/240\n",
      "2023-10-30 01:28:18 [391/391] - 55.57ms/step - nd_loss: 0.230 - div_loss: 1.566 - cls_loss: 1.310 - train_loss: 3.107 - train_acc: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:28:19 [line:195] \u001b[32mTrain Loss: 3.093, Train Acc: 0.650, Test Loss: 1.803, Test Acc: 0.539, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/240\n",
      "2023-10-30 01:28:55 [391/391] - 56.01ms/step - nd_loss: 0.247 - div_loss: 1.760 - cls_loss: 1.392 - train_loss: 3.400 - train_acc: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:28:56 [line:195] \u001b[32mTrain Loss: 3.089, Train Acc: 0.647, Test Loss: 2.366, Test Acc: 0.458, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/240\n",
      "2023-10-30 01:29:31 [391/391] - 55.72ms/step - nd_loss: 0.239 - div_loss: 1.758 - cls_loss: 1.714 - train_loss: 3.712 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:29:32 [line:195] \u001b[32mTrain Loss: 3.092, Train Acc: 0.649, Test Loss: 1.886, Test Acc: 0.518, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/240\n",
      "2023-10-30 01:30:07 [391/391] - 55.57ms/step - nd_loss: 0.229 - div_loss: 1.740 - cls_loss: 1.841 - train_loss: 3.809 - train_acc: 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:30:08 [line:195] \u001b[32mTrain Loss: 3.078, Train Acc: 0.653, Test Loss: 1.684, Test Acc: 0.555, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/240\n",
      "2023-10-30 01:30:43 [391/391] - 55.59ms/step - nd_loss: 0.219 - div_loss: 1.663 - cls_loss: 1.400 - train_loss: 3.283 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:30:44 [line:195] \u001b[32mTrain Loss: 3.089, Train Acc: 0.646, Test Loss: 1.980, Test Acc: 0.506, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/240\n",
      "2023-10-30 01:31:19 [391/391] - 56.35ms/step - nd_loss: 0.218 - div_loss: 1.592 - cls_loss: 1.051 - train_loss: 2.861 - train_acc: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:31:20 [line:195] \u001b[32mTrain Loss: 3.085, Train Acc: 0.652, Test Loss: 1.750, Test Acc: 0.542, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/240\n",
      "2023-10-30 01:31:55 [391/391] - 56.69ms/step - nd_loss: 0.230 - div_loss: 1.717 - cls_loss: 1.427 - train_loss: 3.374 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:31:56 [line:195] \u001b[32mTrain Loss: 3.085, Train Acc: 0.649, Test Loss: 1.614, Test Acc: 0.577, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/240\n",
      "2023-10-30 01:32:32 [391/391] - 56.74ms/step - nd_loss: 0.225 - div_loss: 1.637 - cls_loss: 1.322 - train_loss: 3.184 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:32:33 [line:195] \u001b[32mTrain Loss: 3.078, Train Acc: 0.651, Test Loss: 1.627, Test Acc: 0.578, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/240\n",
      "2023-10-30 01:33:08 [391/391] - 56.22ms/step - nd_loss: 0.228 - div_loss: 1.634 - cls_loss: 1.257 - train_loss: 3.119 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:33:09 [line:195] \u001b[32mTrain Loss: 3.077, Train Acc: 0.651, Test Loss: 1.965, Test Acc: 0.504, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/240\n",
      "2023-10-30 01:33:44 [391/391] - 55.58ms/step - nd_loss: 0.232 - div_loss: 1.681 - cls_loss: 1.266 - train_loss: 3.180 - train_acc: 0.6757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:33:45 [line:195] \u001b[32mTrain Loss: 3.073, Train Acc: 0.649, Test Loss: 1.936, Test Acc: 0.509, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/240\n",
      "2023-10-30 01:34:20 [391/391] - 56.58ms/step - nd_loss: 0.227 - div_loss: 1.741 - cls_loss: 1.101 - train_loss: 3.070 - train_acc: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:34:21 [line:195] \u001b[32mTrain Loss: 3.079, Train Acc: 0.650, Test Loss: 1.731, Test Acc: 0.555, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/240\n",
      "2023-10-30 01:34:56 [391/391] - 55.82ms/step - nd_loss: 0.232 - div_loss: 1.607 - cls_loss: 1.317 - train_loss: 3.155 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:34:57 [line:195] \u001b[32mTrain Loss: 3.070, Train Acc: 0.652, Test Loss: 1.850, Test Acc: 0.530, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/240\n",
      "2023-10-30 01:35:32 [391/391] - 55.44ms/step - nd_loss: 0.237 - div_loss: 1.668 - cls_loss: 1.385 - train_loss: 3.290 - train_acc: 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:35:33 [line:195] \u001b[32mTrain Loss: 3.066, Train Acc: 0.654, Test Loss: 2.037, Test Acc: 0.494, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/240\n",
      "2023-10-30 01:36:08 [391/391] - 55.34ms/step - nd_loss: 0.228 - div_loss: 1.650 - cls_loss: 1.314 - train_loss: 3.191 - train_acc: 0.5508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:36:09 [line:195] \u001b[32mTrain Loss: 3.073, Train Acc: 0.655, Test Loss: 1.745, Test Acc: 0.560, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/240\n",
      "2023-10-30 01:36:44 [391/391] - 56.19ms/step - nd_loss: 0.235 - div_loss: 1.742 - cls_loss: 1.416 - train_loss: 3.393 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:36:45 [line:195] \u001b[32mTrain Loss: 3.077, Train Acc: 0.650, Test Loss: 1.626, Test Acc: 0.564, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/240\n",
      "2023-10-30 01:37:20 [391/391] - 56.49ms/step - nd_loss: 0.221 - div_loss: 1.596 - cls_loss: 1.309 - train_loss: 3.125 - train_acc: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:37:21 [line:195] \u001b[32mTrain Loss: 3.056, Train Acc: 0.654, Test Loss: 1.635, Test Acc: 0.578, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/240\n",
      "2023-10-30 01:37:56 [391/391] - 55.72ms/step - nd_loss: 0.233 - div_loss: 1.699 - cls_loss: 1.537 - train_loss: 3.469 - train_acc: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:37:58 [line:195] \u001b[32mTrain Loss: 3.049, Train Acc: 0.657, Test Loss: 1.717, Test Acc: 0.560, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/240\n",
      "2023-10-30 01:38:33 [391/391] - 55.05ms/step - nd_loss: 0.233 - div_loss: 1.638 - cls_loss: 1.458 - train_loss: 3.330 - train_acc: 0.5888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:38:34 [line:195] \u001b[32mTrain Loss: 3.056, Train Acc: 0.655, Test Loss: 1.656, Test Acc: 0.565, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/240\n",
      "2023-10-30 01:39:09 [391/391] - 55.47ms/step - nd_loss: 0.238 - div_loss: 1.684 - cls_loss: 1.595 - train_loss: 3.516 - train_acc: 0.537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:39:10 [line:195] \u001b[32mTrain Loss: 3.061, Train Acc: 0.653, Test Loss: 1.713, Test Acc: 0.554, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/240\n",
      "2023-10-30 01:39:45 [391/391] - 56.88ms/step - nd_loss: 0.224 - div_loss: 1.593 - cls_loss: 1.284 - train_loss: 3.101 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:39:46 [line:195] \u001b[32mTrain Loss: 3.055, Train Acc: 0.655, Test Loss: 1.494, Test Acc: 0.588, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/240\n",
      "2023-10-30 01:40:21 [391/391] - 55.66ms/step - nd_loss: 0.228 - div_loss: 1.747 - cls_loss: 1.259 - train_loss: 3.234 - train_acc: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:40:22 [line:195] \u001b[32mTrain Loss: 3.047, Train Acc: 0.654, Test Loss: 1.693, Test Acc: 0.553, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/240\n",
      "2023-10-30 01:40:57 [391/391] - 55.24ms/step - nd_loss: 0.250 - div_loss: 1.893 - cls_loss: 1.539 - train_loss: 3.682 - train_acc: 0.6003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:40:58 [line:195] \u001b[32mTrain Loss: 3.050, Train Acc: 0.655, Test Loss: 2.068, Test Acc: 0.498, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/240\n",
      "2023-10-30 01:41:33 [391/391] - 55.51ms/step - nd_loss: 0.226 - div_loss: 1.589 - cls_loss: 1.215 - train_loss: 3.031 - train_acc: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:41:34 [line:195] \u001b[32mTrain Loss: 3.063, Train Acc: 0.654, Test Loss: 1.867, Test Acc: 0.522, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/240\n",
      "2023-10-30 01:42:09 [391/391] - 55.93ms/step - nd_loss: 0.221 - div_loss: 1.655 - cls_loss: 1.180 - train_loss: 3.056 - train_acc: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:42:10 [line:195] \u001b[32mTrain Loss: 3.052, Train Acc: 0.655, Test Loss: 1.704, Test Acc: 0.546, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/240\n",
      "2023-10-30 01:42:45 [391/391] - 55.43ms/step - nd_loss: 0.231 - div_loss: 1.728 - cls_loss: 1.481 - train_loss: 3.441 - train_acc: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:42:46 [line:195] \u001b[32mTrain Loss: 3.045, Train Acc: 0.657, Test Loss: 1.920, Test Acc: 0.525, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/240\n",
      "2023-10-30 01:43:22 [391/391] - 55.74ms/step - nd_loss: 0.220 - div_loss: 1.602 - cls_loss: 0.938 - train_loss: 2.760 - train_acc: 0.7752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:43:23 [line:195] \u001b[32mTrain Loss: 3.062, Train Acc: 0.654, Test Loss: 1.572, Test Acc: 0.584, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/240\n",
      "2023-10-30 01:43:58 [391/391] - 56.02ms/step - nd_loss: 0.234 - div_loss: 1.619 - cls_loss: 1.182 - train_loss: 3.035 - train_acc: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:43:59 [line:195] \u001b[32mTrain Loss: 3.035, Train Acc: 0.658, Test Loss: 1.658, Test Acc: 0.562, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/240\n",
      "2023-10-30 01:44:34 [391/391] - 55.65ms/step - nd_loss: 0.229 - div_loss: 1.633 - cls_loss: 1.288 - train_loss: 3.150 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:44:35 [line:195] \u001b[32mTrain Loss: 3.044, Train Acc: 0.658, Test Loss: 1.890, Test Acc: 0.518, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/240\n",
      "2023-10-30 01:45:10 [391/391] - 55.60ms/step - nd_loss: 0.223 - div_loss: 1.682 - cls_loss: 1.108 - train_loss: 3.013 - train_acc: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:45:11 [line:195] \u001b[32mTrain Loss: 3.041, Train Acc: 0.657, Test Loss: 1.783, Test Acc: 0.547, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/240\n",
      "2023-10-30 01:45:46 [391/391] - 56.76ms/step - nd_loss: 0.232 - div_loss: 1.745 - cls_loss: 1.592 - train_loss: 3.569 - train_acc: 0.5128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:45:47 [line:195] \u001b[32mTrain Loss: 3.046, Train Acc: 0.654, Test Loss: 1.811, Test Acc: 0.544, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/240\n",
      "2023-10-30 01:46:22 [391/391] - 55.43ms/step - nd_loss: 0.247 - div_loss: 1.778 - cls_loss: 1.542 - train_loss: 3.568 - train_acc: 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:46:23 [line:195] \u001b[32mTrain Loss: 3.048, Train Acc: 0.657, Test Loss: 1.758, Test Acc: 0.547, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/240\n",
      "2023-10-30 01:46:58 [391/391] - 55.52ms/step - nd_loss: 0.222 - div_loss: 1.632 - cls_loss: 1.188 - train_loss: 3.042 - train_acc: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:46:59 [line:195] \u001b[32mTrain Loss: 3.048, Train Acc: 0.653, Test Loss: 1.726, Test Acc: 0.540, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/240\n",
      "2023-10-30 01:47:34 [391/391] - 55.39ms/step - nd_loss: 0.229 - div_loss: 1.729 - cls_loss: 1.375 - train_loss: 3.333 - train_acc: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:47:35 [line:195] \u001b[32mTrain Loss: 3.036, Train Acc: 0.659, Test Loss: 1.935, Test Acc: 0.521, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/240\n",
      "2023-10-30 01:48:10 [391/391] - 55.40ms/step - nd_loss: 0.222 - div_loss: 1.537 - cls_loss: 0.977 - train_loss: 2.736 - train_acc: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:48:11 [line:195] \u001b[32mTrain Loss: 3.043, Train Acc: 0.654, Test Loss: 1.734, Test Acc: 0.554, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/240\n",
      "2023-10-30 01:48:46 [391/391] - 55.56ms/step - nd_loss: 0.224 - div_loss: 1.604 - cls_loss: 1.243 - train_loss: 3.071 - train_acc: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:48:47 [line:195] \u001b[32mTrain Loss: 3.032, Train Acc: 0.655, Test Loss: 1.820, Test Acc: 0.527, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/240\n",
      "2023-10-30 01:49:22 [391/391] - 55.95ms/step - nd_loss: 0.251 - div_loss: 1.615 - cls_loss: 1.440 - train_loss: 3.305 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:49:23 [line:195] \u001b[32mTrain Loss: 3.042, Train Acc: 0.654, Test Loss: 2.029, Test Acc: 0.502, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/240\n",
      "2023-10-30 01:49:58 [391/391] - 55.65ms/step - nd_loss: 0.230 - div_loss: 1.647 - cls_loss: 1.126 - train_loss: 3.003 - train_acc: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:49:59 [line:195] \u001b[32mTrain Loss: 3.037, Train Acc: 0.659, Test Loss: 1.738, Test Acc: 0.547, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/240\n",
      "2023-10-30 01:50:35 [391/391] - 55.69ms/step - nd_loss: 0.229 - div_loss: 1.671 - cls_loss: 1.389 - train_loss: 3.289 - train_acc: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:50:36 [line:195] \u001b[32mTrain Loss: 3.040, Train Acc: 0.660, Test Loss: 1.611, Test Acc: 0.576, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/240\n",
      "2023-10-30 01:51:11 [391/391] - 55.64ms/step - nd_loss: 0.228 - div_loss: 1.658 - cls_loss: 1.173 - train_loss: 3.059 - train_acc: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:51:12 [line:195] \u001b[32mTrain Loss: 3.039, Train Acc: 0.658, Test Loss: 1.828, Test Acc: 0.531, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/240\n",
      "2023-10-30 01:51:47 [391/391] - 55.52ms/step - nd_loss: 0.245 - div_loss: 1.692 - cls_loss: 1.453 - train_loss: 3.390 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:51:48 [line:195] \u001b[32mTrain Loss: 3.038, Train Acc: 0.658, Test Loss: 1.705, Test Acc: 0.547, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/240\n",
      "2023-10-30 01:52:23 [391/391] - 55.30ms/step - nd_loss: 0.228 - div_loss: 1.658 - cls_loss: 1.176 - train_loss: 3.063 - train_acc: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:52:24 [line:195] \u001b[32mTrain Loss: 3.038, Train Acc: 0.658, Test Loss: 1.698, Test Acc: 0.555, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/240\n",
      "2023-10-30 01:52:59 [391/391] - 55.35ms/step - nd_loss: 0.228 - div_loss: 1.620 - cls_loss: 0.916 - train_loss: 2.764 - train_acc: 0.750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:53:00 [line:195] \u001b[32mTrain Loss: 3.033, Train Acc: 0.659, Test Loss: 1.680, Test Acc: 0.559, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/240\n",
      "2023-10-30 01:53:35 [391/391] - 55.44ms/step - nd_loss: 0.228 - div_loss: 1.587 - cls_loss: 1.213 - train_loss: 3.028 - train_acc: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:53:36 [line:195] \u001b[32mTrain Loss: 3.019, Train Acc: 0.662, Test Loss: 1.751, Test Acc: 0.546, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/240\n",
      "2023-10-30 01:54:12 [391/391] - 55.54ms/step - nd_loss: 0.223 - div_loss: 1.660 - cls_loss: 1.061 - train_loss: 2.943 - train_acc: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:54:13 [line:195] \u001b[32mTrain Loss: 3.030, Train Acc: 0.661, Test Loss: 1.758, Test Acc: 0.550, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/240\n",
      "2023-10-30 01:54:48 [391/391] - 55.28ms/step - nd_loss: 0.232 - div_loss: 1.584 - cls_loss: 1.178 - train_loss: 2.995 - train_acc: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:54:49 [line:195] \u001b[32mTrain Loss: 3.032, Train Acc: 0.658, Test Loss: 1.955, Test Acc: 0.520, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/240\n",
      "2023-10-30 01:55:24 [391/391] - 55.89ms/step - nd_loss: 0.223 - div_loss: 1.714 - cls_loss: 1.331 - train_loss: 3.268 - train_acc: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:55:25 [line:195] \u001b[32mTrain Loss: 3.030, Train Acc: 0.658, Test Loss: 1.876, Test Acc: 0.527, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/240\n",
      "2023-10-30 01:56:00 [391/391] - 55.61ms/step - nd_loss: 0.233 - div_loss: 1.682 - cls_loss: 1.567 - train_loss: 3.482 - train_acc: 0.5628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:56:01 [line:195] \u001b[32mTrain Loss: 3.021, Train Acc: 0.661, Test Loss: 1.659, Test Acc: 0.571, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/240\n",
      "2023-10-30 01:56:36 [391/391] - 55.79ms/step - nd_loss: 0.231 - div_loss: 1.581 - cls_loss: 1.239 - train_loss: 3.050 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:56:37 [line:195] \u001b[32mTrain Loss: 3.033, Train Acc: 0.658, Test Loss: 1.661, Test Acc: 0.565, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/240\n",
      "2023-10-30 01:57:12 [391/391] - 55.67ms/step - nd_loss: 0.228 - div_loss: 1.578 - cls_loss: 1.054 - train_loss: 2.860 - train_acc: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:57:13 [line:195] \u001b[32mTrain Loss: 3.025, Train Acc: 0.657, Test Loss: 1.905, Test Acc: 0.527, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/240\n",
      "2023-10-30 01:57:48 [391/391] - 55.29ms/step - nd_loss: 0.244 - div_loss: 1.800 - cls_loss: 1.290 - train_loss: 3.335 - train_acc: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:57:49 [line:195] \u001b[32mTrain Loss: 3.025, Train Acc: 0.659, Test Loss: 1.909, Test Acc: 0.510, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/240\n",
      "2023-10-30 01:58:24 [391/391] - 55.26ms/step - nd_loss: 0.245 - div_loss: 1.699 - cls_loss: 1.420 - train_loss: 3.364 - train_acc: 0.6132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:58:25 [line:195] \u001b[32mTrain Loss: 3.031, Train Acc: 0.658, Test Loss: 1.982, Test Acc: 0.520, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/240\n",
      "2023-10-30 01:59:00 [391/391] - 55.48ms/step - nd_loss: 0.240 - div_loss: 1.709 - cls_loss: 1.423 - train_loss: 3.372 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:59:01 [line:195] \u001b[32mTrain Loss: 3.021, Train Acc: 0.659, Test Loss: 2.309, Test Acc: 0.464, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/240\n",
      "2023-10-30 01:59:36 [391/391] - 55.43ms/step - nd_loss: 0.235 - div_loss: 1.639 - cls_loss: 1.263 - train_loss: 3.137 - train_acc: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 01:59:37 [line:195] \u001b[32mTrain Loss: 3.028, Train Acc: 0.661, Test Loss: 1.996, Test Acc: 0.510, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116/240\n",
      "2023-10-30 02:00:12 [391/391] - 55.59ms/step - nd_loss: 0.230 - div_loss: 1.703 - cls_loss: 1.108 - train_loss: 3.042 - train_acc: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:00:13 [line:195] \u001b[32mTrain Loss: 3.019, Train Acc: 0.660, Test Loss: 1.765, Test Acc: 0.545, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/240\n",
      "2023-10-30 02:00:48 [391/391] - 55.66ms/step - nd_loss: 0.225 - div_loss: 1.629 - cls_loss: 1.119 - train_loss: 2.972 - train_acc: 0.6888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:00:49 [line:195] \u001b[32mTrain Loss: 3.022, Train Acc: 0.660, Test Loss: 1.744, Test Acc: 0.559, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/240\n",
      "2023-10-30 02:01:24 [391/391] - 55.25ms/step - nd_loss: 0.230 - div_loss: 1.613 - cls_loss: 1.222 - train_loss: 3.066 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:01:25 [line:195] \u001b[32mTrain Loss: 3.005, Train Acc: 0.661, Test Loss: 1.865, Test Acc: 0.528, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/240\n",
      "2023-10-30 02:02:01 [391/391] - 55.48ms/step - nd_loss: 0.231 - div_loss: 1.673 - cls_loss: 1.314 - train_loss: 3.218 - train_acc: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:02:02 [line:195] \u001b[32mTrain Loss: 3.012, Train Acc: 0.660, Test Loss: 1.657, Test Acc: 0.558, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/240\n",
      "2023-10-30 02:02:37 [391/391] - 55.83ms/step - nd_loss: 0.228 - div_loss: 1.651 - cls_loss: 1.198 - train_loss: 3.078 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:02:38 [line:195] \u001b[32mTrain Loss: 3.022, Train Acc: 0.660, Test Loss: 1.992, Test Acc: 0.509, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/240\n",
      "2023-10-30 02:03:13 [391/391] - 55.38ms/step - nd_loss: 0.237 - div_loss: 1.480 - cls_loss: 0.901 - train_loss: 2.618 - train_acc: 0.7622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:03:14 [line:195] \u001b[32mTrain Loss: 3.024, Train Acc: 0.661, Test Loss: 1.666, Test Acc: 0.566, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/240\n",
      "2023-10-30 02:03:49 [391/391] - 56.04ms/step - nd_loss: 0.227 - div_loss: 1.662 - cls_loss: 1.474 - train_loss: 3.363 - train_acc: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:03:50 [line:195] \u001b[32mTrain Loss: 3.013, Train Acc: 0.664, Test Loss: 2.013, Test Acc: 0.494, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/240\n",
      "2023-10-30 02:04:25 [391/391] - 55.84ms/step - nd_loss: 0.220 - div_loss: 1.624 - cls_loss: 1.378 - train_loss: 3.223 - train_acc: 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:04:26 [line:195] \u001b[32mTrain Loss: 3.023, Train Acc: 0.657, Test Loss: 1.795, Test Acc: 0.540, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/240\n",
      "2023-10-30 02:05:01 [391/391] - 55.45ms/step - nd_loss: 0.232 - div_loss: 1.575 - cls_loss: 1.330 - train_loss: 3.137 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:05:02 [line:195] \u001b[32mTrain Loss: 3.023, Train Acc: 0.661, Test Loss: 1.809, Test Acc: 0.536, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/240\n",
      "2023-10-30 02:05:37 [391/391] - 55.57ms/step - nd_loss: 0.234 - div_loss: 1.629 - cls_loss: 1.190 - train_loss: 3.054 - train_acc: 0.6621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:05:38 [line:195] \u001b[32mTrain Loss: 3.024, Train Acc: 0.661, Test Loss: 1.720, Test Acc: 0.561, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/240\n",
      "2023-10-30 02:06:14 [391/391] - 55.43ms/step - nd_loss: 0.230 - div_loss: 1.705 - cls_loss: 1.262 - train_loss: 3.197 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:06:15 [line:195] \u001b[32mTrain Loss: 3.008, Train Acc: 0.664, Test Loss: 1.625, Test Acc: 0.564, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/240\n",
      "2023-10-30 02:06:50 [391/391] - 55.90ms/step - nd_loss: 0.229 - div_loss: 1.564 - cls_loss: 1.130 - train_loss: 2.923 - train_acc: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:06:51 [line:195] \u001b[32mTrain Loss: 3.006, Train Acc: 0.664, Test Loss: 1.818, Test Acc: 0.532, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128/240\n",
      "2023-10-30 02:07:26 [391/391] - 56.62ms/step - nd_loss: 0.237 - div_loss: 1.763 - cls_loss: 1.671 - train_loss: 3.671 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:07:27 [line:195] \u001b[32mTrain Loss: 3.027, Train Acc: 0.661, Test Loss: 1.903, Test Acc: 0.524, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 129/240\n",
      "2023-10-30 02:08:02 [391/391] - 55.22ms/step - nd_loss: 0.241 - div_loss: 1.817 - cls_loss: 1.416 - train_loss: 3.473 - train_acc: 0.7006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:08:03 [line:195] \u001b[32mTrain Loss: 3.015, Train Acc: 0.663, Test Loss: 1.612, Test Acc: 0.567, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/240\n",
      "2023-10-30 02:08:38 [391/391] - 55.90ms/step - nd_loss: 0.223 - div_loss: 1.663 - cls_loss: 1.247 - train_loss: 3.133 - train_acc: 0.650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:08:39 [line:195] \u001b[32mTrain Loss: 3.013, Train Acc: 0.662, Test Loss: 1.866, Test Acc: 0.540, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/240\n",
      "2023-10-30 02:09:14 [391/391] - 55.42ms/step - nd_loss: 0.229 - div_loss: 1.490 - cls_loss: 1.060 - train_loss: 2.779 - train_acc: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:09:15 [line:195] \u001b[32mTrain Loss: 3.023, Train Acc: 0.658, Test Loss: 1.683, Test Acc: 0.558, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/240\n",
      "2023-10-30 02:09:50 [391/391] - 55.73ms/step - nd_loss: 0.231 - div_loss: 1.746 - cls_loss: 1.611 - train_loss: 3.589 - train_acc: 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:09:51 [line:195] \u001b[32mTrain Loss: 3.013, Train Acc: 0.660, Test Loss: 1.891, Test Acc: 0.536, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/240\n",
      "2023-10-30 02:10:27 [391/391] - 56.09ms/step - nd_loss: 0.233 - div_loss: 1.594 - cls_loss: 1.172 - train_loss: 2.999 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:10:28 [line:195] \u001b[32mTrain Loss: 3.012, Train Acc: 0.662, Test Loss: 2.010, Test Acc: 0.512, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/240\n",
      "2023-10-30 02:11:03 [391/391] - 55.57ms/step - nd_loss: 0.223 - div_loss: 1.624 - cls_loss: 1.310 - train_loss: 3.158 - train_acc: 0.625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:11:04 [line:195] \u001b[32mTrain Loss: 3.009, Train Acc: 0.661, Test Loss: 2.067, Test Acc: 0.487, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/240\n",
      "2023-10-30 02:11:39 [391/391] - 55.49ms/step - nd_loss: 0.229 - div_loss: 1.670 - cls_loss: 1.285 - train_loss: 3.185 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:11:40 [line:195] \u001b[32mTrain Loss: 3.013, Train Acc: 0.661, Test Loss: 1.759, Test Acc: 0.552, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 136/240\n",
      "2023-10-30 02:12:15 [391/391] - 56.78ms/step - nd_loss: 0.236 - div_loss: 1.645 - cls_loss: 1.153 - train_loss: 3.034 - train_acc: 0.713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:12:16 [line:195] \u001b[32mTrain Loss: 3.016, Train Acc: 0.659, Test Loss: 1.686, Test Acc: 0.566, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/240\n",
      "2023-10-30 02:12:51 [391/391] - 56.18ms/step - nd_loss: 0.222 - div_loss: 1.571 - cls_loss: 1.154 - train_loss: 2.946 - train_acc: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:12:52 [line:195] \u001b[32mTrain Loss: 3.006, Train Acc: 0.664, Test Loss: 1.811, Test Acc: 0.544, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/240\n",
      "2023-10-30 02:13:27 [391/391] - 55.28ms/step - nd_loss: 0.225 - div_loss: 1.674 - cls_loss: 0.969 - train_loss: 2.868 - train_acc: 0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:13:28 [line:195] \u001b[32mTrain Loss: 3.001, Train Acc: 0.664, Test Loss: 1.828, Test Acc: 0.540, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/240\n",
      "2023-10-30 02:14:03 [391/391] - 55.28ms/step - nd_loss: 0.240 - div_loss: 1.684 - cls_loss: 1.237 - train_loss: 3.161 - train_acc: 0.662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:14:04 [line:195] \u001b[32mTrain Loss: 3.016, Train Acc: 0.662, Test Loss: 1.869, Test Acc: 0.524, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/240\n",
      "2023-10-30 02:14:39 [391/391] - 55.86ms/step - nd_loss: 0.229 - div_loss: 1.585 - cls_loss: 1.018 - train_loss: 2.832 - train_acc: 0.688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:14:40 [line:195] \u001b[32mTrain Loss: 3.004, Train Acc: 0.665, Test Loss: 1.901, Test Acc: 0.517, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/240\n",
      "2023-10-30 02:15:15 [391/391] - 55.35ms/step - nd_loss: 0.231 - div_loss: 1.648 - cls_loss: 1.259 - train_loss: 3.137 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:15:16 [line:195] \u001b[32mTrain Loss: 2.997, Train Acc: 0.665, Test Loss: 1.959, Test Acc: 0.506, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/240\n",
      "2023-10-30 02:15:51 [391/391] - 57.28ms/step - nd_loss: 0.227 - div_loss: 1.615 - cls_loss: 1.238 - train_loss: 3.081 - train_acc: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:15:52 [line:195] \u001b[32mTrain Loss: 3.004, Train Acc: 0.664, Test Loss: 1.922, Test Acc: 0.512, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/240\n",
      "2023-10-30 02:16:27 [391/391] - 55.73ms/step - nd_loss: 0.223 - div_loss: 1.637 - cls_loss: 1.061 - train_loss: 2.921 - train_acc: 0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:16:28 [line:195] \u001b[32mTrain Loss: 3.008, Train Acc: 0.663, Test Loss: 1.694, Test Acc: 0.554, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/240\n",
      "2023-10-30 02:17:03 [391/391] - 55.53ms/step - nd_loss: 0.226 - div_loss: 1.613 - cls_loss: 1.165 - train_loss: 3.004 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:17:04 [line:195] \u001b[32mTrain Loss: 2.998, Train Acc: 0.665, Test Loss: 2.417, Test Acc: 0.459, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/240\n",
      "2023-10-30 02:17:39 [391/391] - 56.09ms/step - nd_loss: 0.250 - div_loss: 1.681 - cls_loss: 1.637 - train_loss: 3.568 - train_acc: 0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:17:40 [line:195] \u001b[32mTrain Loss: 3.015, Train Acc: 0.663, Test Loss: 1.601, Test Acc: 0.579, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/240\n",
      "2023-10-30 02:18:15 [391/391] - 55.65ms/step - nd_loss: 0.228 - div_loss: 1.486 - cls_loss: 1.158 - train_loss: 2.872 - train_acc: 0.6751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:18:16 [line:195] \u001b[32mTrain Loss: 3.002, Train Acc: 0.664, Test Loss: 1.788, Test Acc: 0.540, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147/240\n",
      "2023-10-30 02:18:51 [391/391] - 55.54ms/step - nd_loss: 0.247 - div_loss: 1.748 - cls_loss: 1.686 - train_loss: 3.680 - train_acc: 0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:18:52 [line:195] \u001b[32mTrain Loss: 3.005, Train Acc: 0.664, Test Loss: 1.600, Test Acc: 0.569, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/240\n",
      "2023-10-30 02:19:27 [391/391] - 55.72ms/step - nd_loss: 0.226 - div_loss: 1.633 - cls_loss: 1.225 - train_loss: 3.084 - train_acc: 0.675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:19:29 [line:195] \u001b[32mTrain Loss: 2.996, Train Acc: 0.664, Test Loss: 1.770, Test Acc: 0.556, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/240\n",
      "2023-10-30 02:20:03 [391/391] - 56.02ms/step - nd_loss: 0.234 - div_loss: 1.772 - cls_loss: 1.545 - train_loss: 3.551 - train_acc: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:20:04 [line:195] \u001b[32mTrain Loss: 2.995, Train Acc: 0.662, Test Loss: 1.668, Test Acc: 0.552, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/240\n",
      "2023-10-30 02:20:40 [391/391] - 56.10ms/step - nd_loss: 0.239 - div_loss: 1.621 - cls_loss: 1.419 - train_loss: 3.279 - train_acc: 0.6753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:20:41 [line:195] \u001b[32mTrain Loss: 3.007, Train Acc: 0.664, Test Loss: 1.593, Test Acc: 0.579, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/240\n",
      "2023-10-30 02:21:16 [391/391] - 55.58ms/step - nd_loss: 0.209 - div_loss: 1.287 - cls_loss: 0.582 - train_loss: 2.078 - train_acc: 0.850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:21:17 [line:195] \u001b[32mTrain Loss: 2.426, Train Acc: 0.763, Test Loss: 1.026, Test Acc: 0.710, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/240\n",
      "2023-10-30 02:21:52 [391/391] - 55.58ms/step - nd_loss: 0.203 - div_loss: 1.303 - cls_loss: 0.659 - train_loss: 2.165 - train_acc: 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:21:53 [line:195] \u001b[32mTrain Loss: 2.244, Train Acc: 0.790, Test Loss: 1.021, Test Acc: 0.713, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/240\n",
      "2023-10-30 02:22:28 [391/391] - 55.82ms/step - nd_loss: 0.213 - div_loss: 1.406 - cls_loss: 0.681 - train_loss: 2.299 - train_acc: 0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:22:29 [line:195] \u001b[32mTrain Loss: 2.172, Train Acc: 0.803, Test Loss: 1.014, Test Acc: 0.716, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 154/240\n",
      "2023-10-30 02:23:04 [391/391] - 55.57ms/step - nd_loss: 0.225 - div_loss: 1.238 - cls_loss: 0.646 - train_loss: 2.108 - train_acc: 0.8129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:23:05 [line:195] \u001b[32mTrain Loss: 2.127, Train Acc: 0.811, Test Loss: 1.004, Test Acc: 0.722, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/240\n",
      "2023-10-30 02:23:40 [391/391] - 55.89ms/step - nd_loss: 0.213 - div_loss: 1.281 - cls_loss: 0.694 - train_loss: 2.188 - train_acc: 0.775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:23:41 [line:195] \u001b[32mTrain Loss: 2.093, Train Acc: 0.815, Test Loss: 1.003, Test Acc: 0.718, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/240\n",
      "2023-10-30 02:24:16 [391/391] - 55.44ms/step - nd_loss: 0.212 - div_loss: 1.227 - cls_loss: 0.673 - train_loss: 2.111 - train_acc: 0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:24:17 [line:195] \u001b[32mTrain Loss: 2.067, Train Acc: 0.823, Test Loss: 1.002, Test Acc: 0.724, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/240\n",
      "2023-10-30 02:24:52 [391/391] - 55.58ms/step - nd_loss: 0.207 - div_loss: 1.289 - cls_loss: 0.534 - train_loss: 2.029 - train_acc: 0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:24:53 [line:195] \u001b[32mTrain Loss: 2.040, Train Acc: 0.827, Test Loss: 1.025, Test Acc: 0.719, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/240\n",
      "2023-10-30 02:25:28 [391/391] - 55.76ms/step - nd_loss: 0.202 - div_loss: 1.193 - cls_loss: 0.322 - train_loss: 1.718 - train_acc: 0.9128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:25:29 [line:195] \u001b[32mTrain Loss: 2.023, Train Acc: 0.832, Test Loss: 1.023, Test Acc: 0.722, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 159/240\n",
      "2023-10-30 02:26:04 [391/391] - 55.47ms/step - nd_loss: 0.211 - div_loss: 1.345 - cls_loss: 0.805 - train_loss: 2.362 - train_acc: 0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:26:05 [line:195] \u001b[32mTrain Loss: 2.003, Train Acc: 0.834, Test Loss: 1.044, Test Acc: 0.716, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 160/240\n",
      "2023-10-30 02:26:40 [391/391] - 55.79ms/step - nd_loss: 0.221 - div_loss: 1.334 - cls_loss: 0.698 - train_loss: 2.253 - train_acc: 0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:26:41 [line:195] \u001b[32mTrain Loss: 1.994, Train Acc: 0.838, Test Loss: 1.051, Test Acc: 0.714, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/240\n",
      "2023-10-30 02:27:16 [391/391] - 55.30ms/step - nd_loss: 0.206 - div_loss: 1.269 - cls_loss: 0.598 - train_loss: 2.073 - train_acc: 0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:27:18 [line:195] \u001b[32mTrain Loss: 1.982, Train Acc: 0.839, Test Loss: 1.036, Test Acc: 0.719, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162/240\n",
      "2023-10-30 02:27:53 [391/391] - 55.58ms/step - nd_loss: 0.201 - div_loss: 1.208 - cls_loss: 0.513 - train_loss: 1.922 - train_acc: 0.8380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:27:54 [line:195] \u001b[32mTrain Loss: 1.970, Train Acc: 0.843, Test Loss: 1.089, Test Acc: 0.708, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 163/240\n",
      "2023-10-30 02:28:29 [391/391] - 55.85ms/step - nd_loss: 0.208 - div_loss: 1.274 - cls_loss: 0.393 - train_loss: 1.875 - train_acc: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:28:30 [line:195] \u001b[32mTrain Loss: 1.961, Train Acc: 0.846, Test Loss: 1.085, Test Acc: 0.713, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 164/240\n",
      "2023-10-30 02:29:05 [391/391] - 55.72ms/step - nd_loss: 0.213 - div_loss: 1.329 - cls_loss: 0.511 - train_loss: 2.053 - train_acc: 0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:29:06 [line:195] \u001b[32mTrain Loss: 1.947, Train Acc: 0.848, Test Loss: 1.078, Test Acc: 0.711, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/240\n",
      "2023-10-30 02:29:41 [391/391] - 55.40ms/step - nd_loss: 0.212 - div_loss: 1.230 - cls_loss: 0.502 - train_loss: 1.944 - train_acc: 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:29:42 [line:195] \u001b[32mTrain Loss: 1.942, Train Acc: 0.849, Test Loss: 1.078, Test Acc: 0.715, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 166/240\n",
      "2023-10-30 02:30:17 [391/391] - 55.70ms/step - nd_loss: 0.201 - div_loss: 1.165 - cls_loss: 0.491 - train_loss: 1.856 - train_acc: 0.8256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:30:18 [line:195] \u001b[32mTrain Loss: 1.946, Train Acc: 0.849, Test Loss: 1.090, Test Acc: 0.712, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 167/240\n",
      "2023-10-30 02:30:53 [391/391] - 56.42ms/step - nd_loss: 0.218 - div_loss: 1.260 - cls_loss: 0.526 - train_loss: 2.004 - train_acc: 0.800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:30:54 [line:195] \u001b[32mTrain Loss: 1.937, Train Acc: 0.850, Test Loss: 1.103, Test Acc: 0.712, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 168/240\n",
      "2023-10-30 02:31:30 [391/391] - 55.25ms/step - nd_loss: 0.211 - div_loss: 1.290 - cls_loss: 0.420 - train_loss: 1.922 - train_acc: 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:31:31 [line:195] \u001b[32mTrain Loss: 1.923, Train Acc: 0.854, Test Loss: 1.132, Test Acc: 0.705, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/240\n",
      "2023-10-30 02:32:06 [391/391] - 55.36ms/step - nd_loss: 0.206 - div_loss: 1.326 - cls_loss: 0.631 - train_loss: 2.163 - train_acc: 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:32:07 [line:195] \u001b[32mTrain Loss: 1.936, Train Acc: 0.849, Test Loss: 1.132, Test Acc: 0.702, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170/240\n",
      "2023-10-30 02:32:42 [391/391] - 55.96ms/step - nd_loss: 0.201 - div_loss: 1.239 - cls_loss: 0.369 - train_loss: 1.809 - train_acc: 0.8632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:32:43 [line:195] \u001b[32mTrain Loss: 1.941, Train Acc: 0.851, Test Loss: 1.165, Test Acc: 0.692, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/240\n",
      "2023-10-30 02:33:18 [391/391] - 55.78ms/step - nd_loss: 0.212 - div_loss: 1.247 - cls_loss: 0.670 - train_loss: 2.128 - train_acc: 0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:33:19 [line:195] \u001b[32mTrain Loss: 1.922, Train Acc: 0.855, Test Loss: 1.142, Test Acc: 0.705, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/240\n",
      "2023-10-30 02:33:54 [391/391] - 55.75ms/step - nd_loss: 0.208 - div_loss: 1.317 - cls_loss: 0.719 - train_loss: 2.244 - train_acc: 0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:33:55 [line:195] \u001b[32mTrain Loss: 1.935, Train Acc: 0.850, Test Loss: 1.114, Test Acc: 0.712, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/240\n",
      "2023-10-30 02:34:30 [391/391] - 55.46ms/step - nd_loss: 0.208 - div_loss: 1.253 - cls_loss: 0.455 - train_loss: 1.916 - train_acc: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:34:31 [line:195] \u001b[32mTrain Loss: 1.924, Train Acc: 0.855, Test Loss: 1.150, Test Acc: 0.706, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174/240\n",
      "2023-10-30 02:35:06 [391/391] - 55.48ms/step - nd_loss: 0.208 - div_loss: 1.222 - cls_loss: 0.423 - train_loss: 1.853 - train_acc: 0.8382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:35:07 [line:195] \u001b[32mTrain Loss: 1.934, Train Acc: 0.852, Test Loss: 1.168, Test Acc: 0.704, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/240\n",
      "2023-10-30 02:35:42 [391/391] - 55.53ms/step - nd_loss: 0.214 - div_loss: 1.360 - cls_loss: 0.531 - train_loss: 2.106 - train_acc: 0.850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:35:43 [line:195] \u001b[32mTrain Loss: 1.927, Train Acc: 0.853, Test Loss: 1.146, Test Acc: 0.707, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 176/240\n",
      "2023-10-30 02:36:18 [391/391] - 55.08ms/step - nd_loss: 0.207 - div_loss: 1.288 - cls_loss: 0.523 - train_loss: 2.019 - train_acc: 0.825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:36:19 [line:195] \u001b[32mTrain Loss: 1.925, Train Acc: 0.854, Test Loss: 1.152, Test Acc: 0.698, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/240\n",
      "2023-10-30 02:36:54 [391/391] - 56.09ms/step - nd_loss: 0.203 - div_loss: 1.285 - cls_loss: 0.449 - train_loss: 1.936 - train_acc: 0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:36:55 [line:195] \u001b[32mTrain Loss: 1.926, Train Acc: 0.854, Test Loss: 1.154, Test Acc: 0.705, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/240\n",
      "2023-10-30 02:37:31 [391/391] - 55.42ms/step - nd_loss: 0.211 - div_loss: 1.229 - cls_loss: 0.623 - train_loss: 2.064 - train_acc: 0.8256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:37:32 [line:195] \u001b[32mTrain Loss: 1.931, Train Acc: 0.853, Test Loss: 1.208, Test Acc: 0.695, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/240\n",
      "2023-10-30 02:38:07 [391/391] - 55.53ms/step - nd_loss: 0.221 - div_loss: 1.267 - cls_loss: 0.459 - train_loss: 1.947 - train_acc: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:38:08 [line:195] \u001b[32mTrain Loss: 1.927, Train Acc: 0.855, Test Loss: 1.244, Test Acc: 0.693, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/240\n",
      "2023-10-30 02:38:43 [391/391] - 56.06ms/step - nd_loss: 0.204 - div_loss: 1.285 - cls_loss: 0.655 - train_loss: 2.144 - train_acc: 0.812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:38:44 [line:195] \u001b[32mTrain Loss: 1.939, Train Acc: 0.852, Test Loss: 1.151, Test Acc: 0.707, lr: 0.01000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/240\n",
      "2023-10-30 02:39:19 [391/391] - 55.88ms/step - nd_loss: 0.199 - div_loss: 1.203 - cls_loss: 0.413 - train_loss: 1.814 - train_acc: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:39:20 [line:195] \u001b[32mTrain Loss: 1.751, Train Acc: 0.893, Test Loss: 1.015, Test Acc: 0.734, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 182/240\n",
      "2023-10-30 02:39:55 [391/391] - 56.34ms/step - nd_loss: 0.198 - div_loss: 1.228 - cls_loss: 0.392 - train_loss: 1.818 - train_acc: 0.8754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:39:56 [line:195] \u001b[32mTrain Loss: 1.692, Train Acc: 0.905, Test Loss: 1.020, Test Acc: 0.737, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183/240\n",
      "2023-10-30 02:40:31 [391/391] - 55.58ms/step - nd_loss: 0.206 - div_loss: 1.144 - cls_loss: 0.399 - train_loss: 1.749 - train_acc: 0.850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:40:32 [line:195] \u001b[32mTrain Loss: 1.672, Train Acc: 0.909, Test Loss: 1.016, Test Acc: 0.739, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 184/240\n",
      "2023-10-30 02:41:07 [391/391] - 55.50ms/step - nd_loss: 0.200 - div_loss: 1.195 - cls_loss: 0.372 - train_loss: 1.767 - train_acc: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:41:08 [line:195] \u001b[32mTrain Loss: 1.657, Train Acc: 0.912, Test Loss: 1.016, Test Acc: 0.739, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185/240\n",
      "2023-10-30 02:41:43 [391/391] - 55.30ms/step - nd_loss: 0.200 - div_loss: 1.172 - cls_loss: 0.248 - train_loss: 1.620 - train_acc: 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:41:44 [line:195] \u001b[32mTrain Loss: 1.644, Train Acc: 0.915, Test Loss: 1.021, Test Acc: 0.741, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/240\n",
      "2023-10-30 02:42:19 [391/391] - 56.77ms/step - nd_loss: 0.197 - div_loss: 1.160 - cls_loss: 0.327 - train_loss: 1.684 - train_acc: 0.9128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:42:20 [line:195] \u001b[32mTrain Loss: 1.639, Train Acc: 0.915, Test Loss: 1.024, Test Acc: 0.739, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 187/240\n",
      "2023-10-30 02:42:55 [391/391] - 55.76ms/step - nd_loss: 0.200 - div_loss: 1.181 - cls_loss: 0.310 - train_loss: 1.692 - train_acc: 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:42:56 [line:195] \u001b[32mTrain Loss: 1.635, Train Acc: 0.916, Test Loss: 1.023, Test Acc: 0.740, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/240\n",
      "2023-10-30 02:43:32 [391/391] - 55.03ms/step - nd_loss: 0.220 - div_loss: 1.164 - cls_loss: 0.348 - train_loss: 1.731 - train_acc: 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:43:33 [line:195] \u001b[32mTrain Loss: 1.628, Train Acc: 0.919, Test Loss: 1.025, Test Acc: 0.741, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 189/240\n",
      "2023-10-30 02:44:08 [391/391] - 55.90ms/step - nd_loss: 0.189 - div_loss: 1.040 - cls_loss: 0.250 - train_loss: 1.479 - train_acc: 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:44:09 [line:195] \u001b[32mTrain Loss: 1.617, Train Acc: 0.921, Test Loss: 1.028, Test Acc: 0.738, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190/240\n",
      "2023-10-30 02:44:44 [391/391] - 56.01ms/step - nd_loss: 0.208 - div_loss: 1.246 - cls_loss: 0.442 - train_loss: 1.895 - train_acc: 0.8879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:44:45 [line:195] \u001b[32mTrain Loss: 1.617, Train Acc: 0.921, Test Loss: 1.026, Test Acc: 0.740, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 191/240\n",
      "2023-10-30 02:45:20 [391/391] - 56.11ms/step - nd_loss: 0.197 - div_loss: 1.167 - cls_loss: 0.298 - train_loss: 1.661 - train_acc: 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:45:21 [line:195] \u001b[32mTrain Loss: 1.612, Train Acc: 0.922, Test Loss: 1.033, Test Acc: 0.738, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/240\n",
      "2023-10-30 02:45:56 [391/391] - 56.11ms/step - nd_loss: 0.201 - div_loss: 1.240 - cls_loss: 0.415 - train_loss: 1.856 - train_acc: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:45:57 [line:195] \u001b[32mTrain Loss: 1.608, Train Acc: 0.921, Test Loss: 1.033, Test Acc: 0.740, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/240\n",
      "2023-10-30 02:46:32 [391/391] - 56.44ms/step - nd_loss: 0.194 - div_loss: 1.105 - cls_loss: 0.283 - train_loss: 1.582 - train_acc: 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:46:33 [line:195] \u001b[32mTrain Loss: 1.603, Train Acc: 0.924, Test Loss: 1.042, Test Acc: 0.740, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 194/240\n",
      "2023-10-30 02:47:08 [391/391] - 55.55ms/step - nd_loss: 0.201 - div_loss: 1.137 - cls_loss: 0.403 - train_loss: 1.741 - train_acc: 0.8630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:47:09 [line:195] \u001b[32mTrain Loss: 1.598, Train Acc: 0.924, Test Loss: 1.041, Test Acc: 0.740, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 195/240\n",
      "2023-10-30 02:47:44 [391/391] - 55.20ms/step - nd_loss: 0.195 - div_loss: 1.174 - cls_loss: 0.239 - train_loss: 1.608 - train_acc: 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:47:45 [line:195] \u001b[32mTrain Loss: 1.594, Train Acc: 0.926, Test Loss: 1.043, Test Acc: 0.739, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196/240\n",
      "2023-10-30 02:48:20 [391/391] - 55.29ms/step - nd_loss: 0.205 - div_loss: 1.159 - cls_loss: 0.242 - train_loss: 1.606 - train_acc: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:48:21 [line:195] \u001b[32mTrain Loss: 1.595, Train Acc: 0.927, Test Loss: 1.039, Test Acc: 0.740, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/240\n",
      "2023-10-30 02:48:56 [391/391] - 55.75ms/step - nd_loss: 0.194 - div_loss: 1.242 - cls_loss: 0.298 - train_loss: 1.733 - train_acc: 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:48:57 [line:195] \u001b[32mTrain Loss: 1.586, Train Acc: 0.929, Test Loss: 1.039, Test Acc: 0.740, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/240\n",
      "2023-10-30 02:49:32 [391/391] - 55.65ms/step - nd_loss: 0.194 - div_loss: 1.166 - cls_loss: 0.192 - train_loss: 1.552 - train_acc: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:49:33 [line:195] \u001b[32mTrain Loss: 1.587, Train Acc: 0.926, Test Loss: 1.047, Test Acc: 0.738, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/240\n",
      "2023-10-30 02:50:08 [391/391] - 55.66ms/step - nd_loss: 0.211 - div_loss: 1.135 - cls_loss: 0.332 - train_loss: 1.678 - train_acc: 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:50:09 [line:195] \u001b[32mTrain Loss: 1.583, Train Acc: 0.928, Test Loss: 1.048, Test Acc: 0.738, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200/240\n",
      "2023-10-30 02:50:44 [391/391] - 55.20ms/step - nd_loss: 0.195 - div_loss: 1.096 - cls_loss: 0.332 - train_loss: 1.623 - train_acc: 0.875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:50:45 [line:195] \u001b[32mTrain Loss: 1.584, Train Acc: 0.928, Test Loss: 1.044, Test Acc: 0.737, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201/240\n",
      "2023-10-30 02:51:20 [391/391] - 56.08ms/step - nd_loss: 0.197 - div_loss: 1.125 - cls_loss: 0.239 - train_loss: 1.561 - train_acc: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:51:21 [line:195] \u001b[32mTrain Loss: 1.578, Train Acc: 0.929, Test Loss: 1.051, Test Acc: 0.736, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/240\n",
      "2023-10-30 02:51:56 [391/391] - 56.27ms/step - nd_loss: 0.191 - div_loss: 1.215 - cls_loss: 0.134 - train_loss: 1.539 - train_acc: 0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:51:58 [line:195] \u001b[32mTrain Loss: 1.571, Train Acc: 0.931, Test Loss: 1.055, Test Acc: 0.739, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/240\n",
      "2023-10-30 02:52:33 [391/391] - 55.61ms/step - nd_loss: 0.194 - div_loss: 1.089 - cls_loss: 0.338 - train_loss: 1.621 - train_acc: 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:52:34 [line:195] \u001b[32mTrain Loss: 1.573, Train Acc: 0.931, Test Loss: 1.057, Test Acc: 0.738, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204/240\n",
      "2023-10-30 02:53:09 [391/391] - 55.48ms/step - nd_loss: 0.203 - div_loss: 1.028 - cls_loss: 0.291 - train_loss: 1.521 - train_acc: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:53:10 [line:195] \u001b[32mTrain Loss: 1.572, Train Acc: 0.931, Test Loss: 1.059, Test Acc: 0.739, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/240\n",
      "2023-10-30 02:53:45 [391/391] - 55.81ms/step - nd_loss: 0.197 - div_loss: 1.147 - cls_loss: 0.292 - train_loss: 1.636 - train_acc: 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:53:46 [line:195] \u001b[32mTrain Loss: 1.568, Train Acc: 0.932, Test Loss: 1.061, Test Acc: 0.737, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206/240\n",
      "2023-10-30 02:54:21 [391/391] - 55.79ms/step - nd_loss: 0.192 - div_loss: 1.161 - cls_loss: 0.313 - train_loss: 1.666 - train_acc: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:54:22 [line:195] \u001b[32mTrain Loss: 1.568, Train Acc: 0.932, Test Loss: 1.062, Test Acc: 0.739, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207/240\n",
      "2023-10-30 02:54:57 [391/391] - 55.24ms/step - nd_loss: 0.199 - div_loss: 1.203 - cls_loss: 0.273 - train_loss: 1.675 - train_acc: 0.9258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:54:58 [line:195] \u001b[32mTrain Loss: 1.562, Train Acc: 0.932, Test Loss: 1.068, Test Acc: 0.738, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 208/240\n",
      "2023-10-30 02:55:33 [391/391] - 55.42ms/step - nd_loss: 0.205 - div_loss: 1.154 - cls_loss: 0.255 - train_loss: 1.615 - train_acc: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:55:34 [line:195] \u001b[32mTrain Loss: 1.562, Train Acc: 0.932, Test Loss: 1.073, Test Acc: 0.737, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/240\n",
      "2023-10-30 02:56:09 [391/391] - 55.38ms/step - nd_loss: 0.197 - div_loss: 1.160 - cls_loss: 0.269 - train_loss: 1.626 - train_acc: 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:56:10 [line:195] \u001b[32mTrain Loss: 1.560, Train Acc: 0.934, Test Loss: 1.073, Test Acc: 0.737, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 210/240\n",
      "2023-10-30 02:56:45 [391/391] - 55.36ms/step - nd_loss: 0.208 - div_loss: 1.145 - cls_loss: 0.251 - train_loss: 1.604 - train_acc: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:56:46 [line:195] \u001b[32mTrain Loss: 1.558, Train Acc: 0.934, Test Loss: 1.074, Test Acc: 0.740, lr: 0.00100\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211/240\n",
      "2023-10-30 02:57:21 [391/391] - 55.55ms/step - nd_loss: 0.202 - div_loss: 1.229 - cls_loss: 0.208 - train_loss: 1.639 - train_acc: 0.9122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:57:22 [line:195] \u001b[32mTrain Loss: 1.542, Train Acc: 0.939, Test Loss: 1.062, Test Acc: 0.740, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 212/240\n",
      "2023-10-30 02:57:57 [391/391] - 56.07ms/step - nd_loss: 0.201 - div_loss: 1.195 - cls_loss: 0.301 - train_loss: 1.697 - train_acc: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:57:58 [line:195] \u001b[32mTrain Loss: 1.539, Train Acc: 0.940, Test Loss: 1.064, Test Acc: 0.740, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/240\n",
      "2023-10-30 02:58:34 [391/391] - 55.27ms/step - nd_loss: 0.196 - div_loss: 1.148 - cls_loss: 0.232 - train_loss: 1.576 - train_acc: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:58:35 [line:195] \u001b[32mTrain Loss: 1.535, Train Acc: 0.939, Test Loss: 1.065, Test Acc: 0.740, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 214/240\n",
      "2023-10-30 02:59:10 [391/391] - 56.70ms/step - nd_loss: 0.202 - div_loss: 1.128 - cls_loss: 0.305 - train_loss: 1.634 - train_acc: 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:59:11 [line:195] \u001b[32mTrain Loss: 1.536, Train Acc: 0.940, Test Loss: 1.064, Test Acc: 0.739, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 215/240\n",
      "2023-10-30 02:59:46 [391/391] - 55.12ms/step - nd_loss: 0.208 - div_loss: 1.166 - cls_loss: 0.221 - train_loss: 1.596 - train_acc: 0.9505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 02:59:47 [line:195] \u001b[32mTrain Loss: 1.536, Train Acc: 0.939, Test Loss: 1.063, Test Acc: 0.739, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 216/240\n",
      "2023-10-30 03:00:22 [391/391] - 55.29ms/step - nd_loss: 0.189 - div_loss: 1.197 - cls_loss: 0.139 - train_loss: 1.525 - train_acc: 0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:00:23 [line:195] \u001b[32mTrain Loss: 1.532, Train Acc: 0.940, Test Loss: 1.070, Test Acc: 0.740, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/240\n",
      "2023-10-30 03:00:57 [391/391] - 55.67ms/step - nd_loss: 0.193 - div_loss: 1.126 - cls_loss: 0.171 - train_loss: 1.490 - train_acc: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:00:59 [line:195] \u001b[32mTrain Loss: 1.534, Train Acc: 0.939, Test Loss: 1.067, Test Acc: 0.738, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 218/240\n",
      "2023-10-30 03:01:34 [391/391] - 55.42ms/step - nd_loss: 0.194 - div_loss: 1.079 - cls_loss: 0.270 - train_loss: 1.544 - train_acc: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:01:35 [line:195] \u001b[32mTrain Loss: 1.529, Train Acc: 0.940, Test Loss: 1.066, Test Acc: 0.740, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219/240\n",
      "2023-10-30 03:02:10 [391/391] - 55.22ms/step - nd_loss: 0.206 - div_loss: 1.182 - cls_loss: 0.207 - train_loss: 1.594 - train_acc: 0.9638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:02:11 [line:195] \u001b[32mTrain Loss: 1.530, Train Acc: 0.941, Test Loss: 1.066, Test Acc: 0.738, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 220/240\n",
      "2023-10-30 03:02:46 [391/391] - 56.04ms/step - nd_loss: 0.183 - div_loss: 1.149 - cls_loss: 0.258 - train_loss: 1.590 - train_acc: 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:02:47 [line:195] \u001b[32mTrain Loss: 1.532, Train Acc: 0.941, Test Loss: 1.072, Test Acc: 0.738, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 221/240\n",
      "2023-10-30 03:03:21 [391/391] - 55.44ms/step - nd_loss: 0.184 - div_loss: 1.119 - cls_loss: 0.239 - train_loss: 1.542 - train_acc: 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:03:23 [line:195] \u001b[32mTrain Loss: 1.528, Train Acc: 0.941, Test Loss: 1.066, Test Acc: 0.739, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 222/240\n",
      "2023-10-30 03:03:58 [391/391] - 55.44ms/step - nd_loss: 0.204 - div_loss: 1.179 - cls_loss: 0.399 - train_loss: 1.782 - train_acc: 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:03:59 [line:195] \u001b[32mTrain Loss: 1.531, Train Acc: 0.942, Test Loss: 1.071, Test Acc: 0.738, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/240\n",
      "2023-10-30 03:04:34 [391/391] - 55.39ms/step - nd_loss: 0.200 - div_loss: 1.189 - cls_loss: 0.280 - train_loss: 1.670 - train_acc: 0.9121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:04:35 [line:195] \u001b[32mTrain Loss: 1.528, Train Acc: 0.941, Test Loss: 1.067, Test Acc: 0.741, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/240\n",
      "2023-10-30 03:05:10 [391/391] - 55.34ms/step - nd_loss: 0.198 - div_loss: 1.164 - cls_loss: 0.156 - train_loss: 1.518 - train_acc: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:05:11 [line:195] \u001b[32mTrain Loss: 1.526, Train Acc: 0.942, Test Loss: 1.070, Test Acc: 0.736, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 225/240\n",
      "2023-10-30 03:05:46 [391/391] - 55.49ms/step - nd_loss: 0.201 - div_loss: 1.138 - cls_loss: 0.170 - train_loss: 1.510 - train_acc: 0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:05:47 [line:195] \u001b[32mTrain Loss: 1.530, Train Acc: 0.942, Test Loss: 1.067, Test Acc: 0.739, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 226/240\n",
      "2023-10-30 03:06:22 [391/391] - 55.54ms/step - nd_loss: 0.190 - div_loss: 1.146 - cls_loss: 0.179 - train_loss: 1.515 - train_acc: 0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:06:23 [line:195] \u001b[32mTrain Loss: 1.528, Train Acc: 0.941, Test Loss: 1.069, Test Acc: 0.739, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 227/240\n",
      "2023-10-30 03:06:58 [391/391] - 55.54ms/step - nd_loss: 0.199 - div_loss: 1.121 - cls_loss: 0.181 - train_loss: 1.501 - train_acc: 0.9508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:06:59 [line:195] \u001b[32mTrain Loss: 1.526, Train Acc: 0.941, Test Loss: 1.069, Test Acc: 0.739, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/240\n",
      "2023-10-30 03:07:34 [391/391] - 55.41ms/step - nd_loss: 0.197 - div_loss: 1.190 - cls_loss: 0.274 - train_loss: 1.662 - train_acc: 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:07:35 [line:195] \u001b[32mTrain Loss: 1.530, Train Acc: 0.941, Test Loss: 1.065, Test Acc: 0.740, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/240\n",
      "2023-10-30 03:08:10 [391/391] - 56.16ms/step - nd_loss: 0.196 - div_loss: 1.115 - cls_loss: 0.192 - train_loss: 1.504 - train_acc: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:08:11 [line:195] \u001b[32mTrain Loss: 1.528, Train Acc: 0.941, Test Loss: 1.066, Test Acc: 0.739, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/240\n",
      "2023-10-30 03:08:46 [391/391] - 55.70ms/step - nd_loss: 0.191 - div_loss: 1.189 - cls_loss: 0.223 - train_loss: 1.603 - train_acc: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:08:47 [line:195] \u001b[32mTrain Loss: 1.527, Train Acc: 0.942, Test Loss: 1.073, Test Acc: 0.739, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 231/240\n",
      "2023-10-30 03:09:22 [391/391] - 55.36ms/step - nd_loss: 0.196 - div_loss: 1.126 - cls_loss: 0.183 - train_loss: 1.505 - train_acc: 0.9508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:09:23 [line:195] \u001b[32mTrain Loss: 1.524, Train Acc: 0.942, Test Loss: 1.066, Test Acc: 0.740, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 232/240\n",
      "2023-10-30 03:09:58 [391/391] - 55.12ms/step - nd_loss: 0.203 - div_loss: 1.155 - cls_loss: 0.271 - train_loss: 1.630 - train_acc: 0.912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:09:59 [line:195] \u001b[32mTrain Loss: 1.524, Train Acc: 0.942, Test Loss: 1.071, Test Acc: 0.740, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/240\n",
      "2023-10-30 03:10:34 [391/391] - 55.54ms/step - nd_loss: 0.199 - div_loss: 1.132 - cls_loss: 0.220 - train_loss: 1.551 - train_acc: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:10:35 [line:195] \u001b[32mTrain Loss: 1.523, Train Acc: 0.942, Test Loss: 1.071, Test Acc: 0.738, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 234/240\n",
      "2023-10-30 03:11:10 [391/391] - 55.40ms/step - nd_loss: 0.200 - div_loss: 1.101 - cls_loss: 0.188 - train_loss: 1.489 - train_acc: 0.950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:11:11 [line:195] \u001b[32mTrain Loss: 1.530, Train Acc: 0.941, Test Loss: 1.072, Test Acc: 0.738, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 235/240\n",
      "2023-10-30 03:11:46 [391/391] - 55.42ms/step - nd_loss: 0.194 - div_loss: 1.092 - cls_loss: 0.204 - train_loss: 1.491 - train_acc: 0.9258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:11:47 [line:195] \u001b[32mTrain Loss: 1.522, Train Acc: 0.943, Test Loss: 1.073, Test Acc: 0.738, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 236/240\n",
      "2023-10-30 03:12:22 [391/391] - 55.47ms/step - nd_loss: 0.189 - div_loss: 1.166 - cls_loss: 0.199 - train_loss: 1.554 - train_acc: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:12:23 [line:195] \u001b[32mTrain Loss: 1.520, Train Acc: 0.943, Test Loss: 1.073, Test Acc: 0.739, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/240\n",
      "2023-10-30 03:12:58 [391/391] - 55.71ms/step - nd_loss: 0.195 - div_loss: 1.125 - cls_loss: 0.181 - train_loss: 1.501 - train_acc: 0.963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:12:59 [line:195] \u001b[32mTrain Loss: 1.526, Train Acc: 0.941, Test Loss: 1.074, Test Acc: 0.739, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 238/240\n",
      "2023-10-30 03:13:35 [391/391] - 55.45ms/step - nd_loss: 0.189 - div_loss: 1.186 - cls_loss: 0.232 - train_loss: 1.606 - train_acc: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:13:36 [line:195] \u001b[32mTrain Loss: 1.525, Train Acc: 0.942, Test Loss: 1.071, Test Acc: 0.741, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/240\n",
      "2023-10-30 03:14:11 [391/391] - 55.73ms/step - nd_loss: 0.201 - div_loss: 1.111 - cls_loss: 0.130 - train_loss: 1.441 - train_acc: 0.9752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:14:12 [line:195] \u001b[32mTrain Loss: 1.524, Train Acc: 0.942, Test Loss: 1.068, Test Acc: 0.740, lr: 0.00010\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 240/240\n",
      "2023-10-30 03:14:47 [391/391] - 55.50ms/step - nd_loss: 0.190 - div_loss: 1.094 - cls_loss: 0.306 - train_loss: 1.589 - train_acc: 0.887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30 Oct 2023 03:14:48 [line:195] \u001b[32mTrain Loss: 1.524, Train Acc: 0.943, Test Loss: 1.072, Test Acc: 0.740, lr: 0.00010\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,0'\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import models\n",
    "from models.reviewkd import build_review_kd, hcl\n",
    "from Dataset import CIFAR\n",
    "from utils import colorstr, Save_Checkpoint, AverageMeter, DirectNormLoss\n",
    "from torchsummaryX import summary\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pdb\n",
    "\n",
    "def train(model, teacher, T_EMB, train_dataloader, optimizer, criterion, nd_loss, args, epoch):\n",
    "    train_loss = AverageMeter()\n",
    "    train_error = AverageMeter()\n",
    "\n",
    "    Cls_loss = AverageMeter()\n",
    "    Div_loss = AverageMeter()\n",
    "    Norm_Dir_loss = AverageMeter()\n",
    "\n",
    "    # Model on train mode\n",
    "    model.train()\n",
    "    teacher.eval()\n",
    "    step_per_epoch = len(train_dataloader)\n",
    "\n",
    "    # pdb.set_trace()\n",
    "    for step, (images, labels) in enumerate(train_dataloader):\n",
    "        start = time.time()\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # compute output\n",
    "        s_features, s_emb, s_logits = model(images)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_features, t_emb, t_logits = teacher(images, is_feat=True, preact=True)\n",
    "            t_features = t_features[1:]\n",
    "\n",
    "        # cls loss\n",
    "        cls_loss = criterion(s_logits, labels) * args.cls_loss_factor\n",
    "        # Kd loss\n",
    "        kd_loss = hcl(s_features, t_features) * min(1, epoch/args.warm_up) * args.kd_loss_factor\n",
    "        # ND loss\n",
    "        norm_dir_loss = nd_loss(s_emb=s_emb, t_emb=t_emb, T_EMB=T_EMB, labels=labels)\n",
    "\n",
    "        loss = cls_loss + kd_loss + norm_dir_loss\n",
    "        # measure accuracy and record loss\n",
    "        batch_size = images.size(0)\n",
    "        _, pred = s_logits.data.cpu().topk(1, dim=1)\n",
    "        train_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "        train_loss.update(loss.item(), batch_size)\n",
    "\n",
    "        Cls_loss.update(cls_loss.item(), batch_size)\n",
    "        Div_loss.update(kd_loss.item(), batch_size)\n",
    "        Norm_Dir_loss.update(norm_dir_loss.item(), batch_size)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "        s1 = '\\r{} [{}/{}]'.format(t, step+1, step_per_epoch)\n",
    "        s2 = ' - {:.2f}ms/step - nd_loss: {:.3f} - div_loss: {:.3f} - cls_loss: {:.3f} - train_loss: {:.3f} - train_acc: {:.3f}'.format(\n",
    "             1000 * (time.time() - start), norm_dir_loss.item(), kd_loss.item(), cls_loss.item(), train_loss.val, 1-train_error.val)\n",
    "\n",
    "        print(s1+s2, end='', flush=True)\n",
    "\n",
    "    print()\n",
    "    return Norm_Dir_loss.avg, Div_loss.avg, Cls_loss.avg, train_loss.avg, train_error.avg\n",
    "\n",
    "\n",
    "def test(model, test_dataloader, criterion):\n",
    "    test_loss = AverageMeter()\n",
    "    test_error = AverageMeter()\n",
    "\n",
    "    # Model on eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # compute logits\n",
    "            _, _, logits = model(images)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            batch_size = images.size(0)\n",
    "            _, pred = logits.data.cpu().topk(1, dim=1)\n",
    "            test_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "            test_loss.update(loss.item(), batch_size)\n",
    "\n",
    "    return test_loss.avg, test_error.avg\n",
    "\n",
    "\n",
    "def epoch_loop(model, teacher, train_set, test_set, args):\n",
    "    # data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=args.workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=args.workers)\n",
    "\n",
    "    # model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # model = nn.DataParallel(model, device_ids=args.gpus)\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    # teacher = nn.DataParallel(teacher, device_ids=args.gpus)\n",
    "    teacher = nn.DataParallel(teacher)\n",
    "    teacher.to(device)\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    nd_loss = DirectNormLoss(num_class=100, nd_loss_factor=args.nd_loss_factor).to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)\n",
    "\n",
    "    # 权重\n",
    "    save_dir = Path(args.save_dir)\n",
    "    weights = save_dir / 'weights'\n",
    "    weights.mkdir(parents=True, exist_ok=True)\n",
    "    last = weights / 'last'\n",
    "    best = weights / 'best'\n",
    "\n",
    "    # acc,loss\n",
    "    acc_loss = save_dir / 'acc_loss'\n",
    "    acc_loss.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_acc_savepath = acc_loss / 'train_acc.npy'\n",
    "    train_loss_savepath = acc_loss / 'train_loss.npy'\n",
    "    val_acc_savepath = acc_loss / 'val_acc.npy'\n",
    "    val_loss_savepath = acc_loss / 'val_loss.npy'\n",
    "\n",
    "    # tensorboard\n",
    "    logdir = save_dir / 'logs'\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "    summary_writer = SummaryWriter(logdir, flush_secs=120)\n",
    "\n",
    "    # resume\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_error = checkpoint['best_error']\n",
    "        train_acc = checkpoint['train_acc']\n",
    "        train_loss = checkpoint['train_loss']\n",
    "        test_acc = checkpoint['test_acc']\n",
    "        test_loss = checkpoint['test_loss']\n",
    "        logger.info(colorstr('green', 'Resuming training from {} epoch'.format(start_epoch)))\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_error = 0\n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        test_acc = []\n",
    "        test_loss = []\n",
    "\n",
    "    # Train model\n",
    "    best_error = 1\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        if epoch in [150, 180, 210]:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, args.epochs))\n",
    "        norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error = train(model=model,\n",
    "                                                                                 teacher=teacher,\n",
    "                                                                                 T_EMB=T_EMB,\n",
    "                                                                                 train_dataloader=train_loader,\n",
    "                                                                                 optimizer=optimizer,\n",
    "                                                                                 criterion=criterion,\n",
    "                                                                                 nd_loss=nd_loss,\n",
    "                                                                                 args=args,\n",
    "                                                                                 epoch=epoch)\n",
    "        test_epoch_loss, test_error = test(model=model,\n",
    "                                           test_dataloader=test_loader,\n",
    "                                           criterion=criterion)\n",
    "\n",
    "        s = \"Train Loss: {:.3f}, Train Acc: {:.3f}, Test Loss: {:.3f}, Test Acc: {:.3f}, lr: {:.5f}\".format(\n",
    "            train_epoch_loss, 1-train_error, test_epoch_loss, 1-test_error, optimizer.param_groups[0]['lr'])\n",
    "        logger.info(colorstr('green', s))\n",
    "\n",
    "        # save acc,loss\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        train_acc.append(1-train_error)\n",
    "        test_loss.append(test_epoch_loss)\n",
    "        test_acc.append(1-test_error)\n",
    "\n",
    "        # save model\n",
    "        is_best = test_error < best_error\n",
    "        best_error = min(best_error, test_error)\n",
    "        state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_error': best_error,\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'test_acc': test_acc,\n",
    "                'test_loss': test_loss,\n",
    "            }\n",
    "\n",
    "        last_path = last / 'epoch_{}_loss_{:.3f}_acc_{:.3f}'.format(\n",
    "            epoch + 1, test_epoch_loss, 1-test_error)\n",
    "        best_path = best / 'epoch_{}_acc_{:.3f}'.format(\n",
    "                epoch + 1, 1-best_error)\n",
    "\n",
    "        Save_Checkpoint(state, last, last_path, best, best_path, is_best)\n",
    "\n",
    "        # tensorboard\n",
    "        # pdb.set_trace()\n",
    "        if epoch == 1:\n",
    "            images, labels = next(iter(train_loader))\n",
    "            img_grid = torchvision.utils.make_grid(images)\n",
    "            summary_writer.add_image('Cifar Image', img_grid)\n",
    "        summary_writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "        summary_writer.add_scalar('train_loss', train_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('train_error', train_error, epoch)\n",
    "        summary_writer.add_scalar('val_loss', test_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('val_error', test_error, epoch)\n",
    "\n",
    "        summary_writer.add_scalar('nd_loss', norm_dir_loss, epoch)\n",
    "        summary_writer.add_scalar('kd_loss', div_loss, epoch)\n",
    "        summary_writer.add_scalar('cls_loss', cls_loss, epoch)\n",
    "\n",
    "    summary_writer.close()\n",
    "    if not os.path.exists(train_acc_savepath) or not os.path.exists(train_loss_savepath):\n",
    "        np.save(train_acc_savepath, train_acc)\n",
    "        np.save(train_loss_savepath, train_loss)\n",
    "        np.save(val_acc_savepath, test_acc)\n",
    "        np.save(val_loss_savepath, test_loss)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_names = sorted(name for name in models.__dict__\n",
    "                         if name.islower() and not name.startswith(\"__\")\n",
    "                         and callable(models.__dict__[name]))\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Cifar Training')\n",
    "    parser.add_argument('-f') # added to make this run in collab\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"wrn40_1_cifar\", choices=model_names, help=\"model architecture\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default='cifar100')\n",
    "    parser.add_argument(\"--epochs\", type=int, default=240)\n",
    "    # parser.add_argument(\"--epochs\", type=int, default=4)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size per gpu\")\n",
    "    parser.add_argument('--workers', default=8, type=int, help='number of data loading workers')\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.1)\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum')\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=5e-4)\n",
    "\n",
    "    parser.add_argument(\"--teacher\", type=str, default=\"wrn40_2_cifar\", help=\"teacher architecture\")\n",
    "    parser.add_argument(\"--teacher_weights\", type=str, default=\"../ckpt/cifar_teachers/wrn_40_2_vanilla/ckpt_epoch_240.pth\", help=\"teacher weights path\")\n",
    "    parser.add_argument(\"--cls_loss_factor\", type=float, default=1.0, help=\"cls loss weight factor\")\n",
    "    parser.add_argument(\"--kd_loss_factor\", type=float, default=1.0, help=\"KL loss weight factor\")\n",
    "    parser.add_argument(\"--nd_loss_factor\", type=float, default=1.0, help=\"ND loss weight factor\")\n",
    "    parser.add_argument(\"--warm_up\", type=float, default=20.0, help='loss weight warm up epochs')\n",
    "\n",
    "    # parser.add_argument(\"--gpus\", type=list, default=[0, 1])\n",
    "    parser.add_argument('--seed', default=None, type=int, help='seed for initializing training.')\n",
    "    parser.add_argument(\"--resume\", type=str, help=\"best ckpt's path to resume most recent training\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"./run\", help=\"save path, eg, acc_loss, weights, tensorboard, and so on\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        cudnn.benchmark = False\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s [line:%(lineno)d] %(message)s',\n",
    "                        datefmt='%d %b %Y %H:%M:%S')\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # args.batch_size = args.batch_size * len(args.gpus)\n",
    "    args.batch_size = args.batch_size * 1\n",
    "\n",
    "\n",
    "    # logger.info(colorstr('green', \"Distribute train, gpus:{}, total batch size:{}, epoch:{}\".format(args.gpus, args.batch_size, args.epochs)))\n",
    "    logger.info(colorstr('green', \"Distribute train, total batch size:{}, epoch:{}\".format(args.batch_size, args.epochs)))\n",
    "\n",
    "    train_set, test_set, num_class = CIFAR(name=args.dataset)\n",
    "    model = build_review_kd(student_name=args.model_name, num_class=num_class, teacher_name=args.teacher)\n",
    "    teacher = models.__dict__[args.teacher](num_class=num_class)\n",
    "\n",
    "    if args.teacher_weights:\n",
    "        print('Load Teacher Weights')\n",
    "        teacher_ckpt = torch.load(args.teacher_weights)['model']\n",
    "        teacher.load_state_dict(teacher_ckpt)\n",
    "\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # res56    ./ckpt/teacher/resnet56/center_emb_train.json\n",
    "    # res32x4  ./ckpt/teacher/resnet32x4/center_emb_train.json\n",
    "    # wrn40_2  ./ckpt/teacher/wrn_40_2/center_emb_train.json\n",
    "    # res50    ./ckpt/teacher/resnet50/center_emb_train.json\n",
    "    # class-mean\n",
    "    with open(\"./ckpt/teacher/wrn_40_2/center_emb_train.json\", 'r') as f:\n",
    "        T_EMB = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    logger.info(colorstr('green', 'Use ' + args.teacher + ' Training ' + args.model_name + ' ...'))\n",
    "    # Train the model\n",
    "    epoch_loop(model=model, teacher=teacher, train_set=train_set, test_set=test_set, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1698431953791,
     "user": {
      "displayName": "Michelle Bolner",
      "userId": "00246950705303252689"
     },
     "user_tz": 240
    },
    "id": "xw0qxWg1bUqV",
    "outputId": "d6f46c2a-d983-4544-bfdf-b3db585580fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../ckpt/cifar_teachers/resnet32x4_vanilla/ckpt_epoch_240.pth\n"
     ]
    }
   ],
   "source": [
    "!ls ../ckpt/cifar_teachers/resnet32x4_vanilla/ckpt_epoch_240.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71I3J4sLbETO"
   },
   "source": [
    "### leader board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "executionInfo": {
     "elapsed": 5923,
     "status": "error",
     "timestamp": 1698436361400,
     "user": {
      "displayName": "Michelle Bolner",
      "userId": "00246950705303252689"
     },
     "user_tz": 240
    },
    "id": "Y3VNZ_aSTSUY",
    "outputId": "4c5a8f3e-8977-4e14-b321-1d2c842b1d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Load Teacher Weights\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-b07ef9888245>\u001b[0m in \u001b[0;36m<cell line: 248>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolorstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Use '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteacher\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' Training '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0mepoch_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-47-b07ef9888245>\u001b[0m in \u001b[0;36mepoch_loop\u001b[0;34m(model, teacher, train_set, test_set, args)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error = train(model=model,\n\u001b[0m\u001b[1;32m    181\u001b[0m                                                                                  \u001b[0mteacher\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m                                                                                  \u001b[0mT_EMB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mT_EMB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-b07ef9888245>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, teacher, T_EMB, train_dataloader, optimizer, criterion, nd_loss, args, epoch)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0ms_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/drive/MyDrive/210_Competition_Models/#2_Knowledge-Distillation-via-ND-main/CIFAR/ReviewKD++/models/reviewkd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mstudent_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_fea\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mis_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0memb_fea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_fea\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1,0'\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import models\n",
    "from models.reviewkd import build_review_kd, hcl\n",
    "from Dataset import CIFAR\n",
    "from utils import colorstr, Save_Checkpoint, AverageMeter, DirectNormLoss\n",
    "from torchsummary import summary\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pdb\n",
    "\n",
    "def train(model, teacher, T_EMB, train_dataloader, optimizer, criterion, nd_loss, args, epoch):\n",
    "    train_loss = AverageMeter()\n",
    "    train_error = AverageMeter()\n",
    "\n",
    "    Cls_loss = AverageMeter()\n",
    "    Div_loss = AverageMeter()\n",
    "    Norm_Dir_loss = AverageMeter()\n",
    "\n",
    "    # Model on train mode\n",
    "    model.train()\n",
    "    teacher.eval()\n",
    "    step_per_epoch = len(train_dataloader)\n",
    "\n",
    "    # pdb.set_trace()\n",
    "    for step, (images, labels) in enumerate(train_dataloader):\n",
    "        start = time.time()\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # compute output\n",
    "        s_features, s_emb, s_logits = model(images)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_features, t_emb, t_logits = teacher(images, is_feat=True, preact=True)\n",
    "            t_features = t_features[1:]\n",
    "\n",
    "        # cls loss\n",
    "        cls_loss = criterion(s_logits, labels) * args.cls_loss_factor\n",
    "        # Kd loss\n",
    "        kd_loss = hcl(s_features, t_features) * min(1, epoch/args.warm_up) * args.kd_loss_factor\n",
    "        # ND loss\n",
    "        norm_dir_loss = nd_loss(s_emb=s_emb, t_emb=t_emb, T_EMB=T_EMB, labels=labels)\n",
    "\n",
    "        loss = cls_loss + kd_loss + norm_dir_loss\n",
    "        # measure accuracy and record loss\n",
    "        batch_size = images.size(0)\n",
    "        _, pred = s_logits.data.cpu().topk(1, dim=1)\n",
    "        train_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "        train_loss.update(loss.item(), batch_size)\n",
    "\n",
    "        Cls_loss.update(cls_loss.item(), batch_size)\n",
    "        Div_loss.update(kd_loss.item(), batch_size)\n",
    "        Norm_Dir_loss.update(norm_dir_loss.item(), batch_size)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "        s1 = '\\r{} [{}/{}]'.format(t, step+1, step_per_epoch)\n",
    "        s2 = ' - {:.2f}ms/step - nd_loss: {:.3f} - div_loss: {:.3f} - cls_loss: {:.3f} - train_loss: {:.3f} - train_acc: {:.3f}'.format(\n",
    "             1000 * (time.time() - start), norm_dir_loss.item(), kd_loss.item(), cls_loss.item(), train_loss.val, 1-train_error.val)\n",
    "\n",
    "        print(s1+s2, end='', flush=True)\n",
    "\n",
    "    print()\n",
    "    return Norm_Dir_loss.avg, Div_loss.avg, Cls_loss.avg, train_loss.avg, train_error.avg\n",
    "\n",
    "\n",
    "def test(model, test_dataloader, criterion):\n",
    "    test_loss = AverageMeter()\n",
    "    test_error = AverageMeter()\n",
    "\n",
    "    # Model on eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # compute logits\n",
    "            _, _, logits = model(images)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            batch_size = images.size(0)\n",
    "            _, pred = logits.data.cpu().topk(1, dim=1)\n",
    "            test_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "            test_loss.update(loss.item(), batch_size)\n",
    "\n",
    "    return test_loss.avg, test_error.avg\n",
    "\n",
    "\n",
    "def epoch_loop(model, teacher, train_set, test_set, args):\n",
    "    # data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=args.workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=args.workers)\n",
    "\n",
    "    # model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # model = nn.DataParallel(model, device_ids=args.gpus)\n",
    "    model = nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "    # teacher = nn.DataParallel(teacher, device_ids=args.gpus)\n",
    "    teacher = nn.DataParallel(teacher)\n",
    "    teacher.to(device)\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    nd_loss = DirectNormLoss(num_class=100, nd_loss_factor=args.nd_loss_factor).to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.SGD(params=model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)\n",
    "\n",
    "    # 权重\n",
    "    save_dir = Path(args.save_dir)\n",
    "    weights = save_dir / 'weights'\n",
    "    weights.mkdir(parents=True, exist_ok=True)\n",
    "    last = weights / 'last'\n",
    "    best = weights / 'best'\n",
    "\n",
    "    # acc,loss\n",
    "    acc_loss = save_dir / 'acc_loss'\n",
    "    acc_loss.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_acc_savepath = acc_loss / 'train_acc.npy'\n",
    "    train_loss_savepath = acc_loss / 'train_loss.npy'\n",
    "    val_acc_savepath = acc_loss / 'val_acc.npy'\n",
    "    val_loss_savepath = acc_loss / 'val_loss.npy'\n",
    "\n",
    "    # tensorboard\n",
    "    logdir = save_dir / 'logs'\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "    summary_writer = SummaryWriter(logdir, flush_secs=120)\n",
    "\n",
    "    # resume\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_error = checkpoint['best_error']\n",
    "        train_acc = checkpoint['train_acc']\n",
    "        train_loss = checkpoint['train_loss']\n",
    "        test_acc = checkpoint['test_acc']\n",
    "        test_loss = checkpoint['test_loss']\n",
    "        logger.info(colorstr('green', 'Resuming training from {} epoch'.format(start_epoch)))\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_error = 0\n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        test_acc = []\n",
    "        test_loss = []\n",
    "\n",
    "    # Train model\n",
    "    best_error = 1\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        if epoch in [150, 180, 210]:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, args.epochs))\n",
    "        norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error = train(model=model,\n",
    "                                                                                 teacher=teacher,\n",
    "                                                                                 T_EMB=T_EMB,\n",
    "                                                                                 train_dataloader=train_loader,\n",
    "                                                                                 optimizer=optimizer,\n",
    "                                                                                 criterion=criterion,\n",
    "                                                                                 nd_loss=nd_loss,\n",
    "                                                                                 args=args,\n",
    "                                                                                 epoch=epoch)\n",
    "        test_epoch_loss, test_error = test(model=model,\n",
    "                                           test_dataloader=test_loader,\n",
    "                                           criterion=criterion)\n",
    "\n",
    "        s = \"Train Loss: {:.3f}, Train Acc: {:.3f}, Test Loss: {:.3f}, Test Acc: {:.3f}, lr: {:.5f}\".format(\n",
    "            train_epoch_loss, 1-train_error, test_epoch_loss, 1-test_error, optimizer.param_groups[0]['lr'])\n",
    "        logger.info(colorstr('green', s))\n",
    "\n",
    "        # save acc,loss\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        train_acc.append(1-train_error)\n",
    "        test_loss.append(test_epoch_loss)\n",
    "        test_acc.append(1-test_error)\n",
    "\n",
    "        # save model\n",
    "        is_best = test_error < best_error\n",
    "        best_error = min(best_error, test_error)\n",
    "        state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_error': best_error,\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'test_acc': test_acc,\n",
    "                'test_loss': test_loss,\n",
    "            }\n",
    "\n",
    "        last_path = last / 'epoch_{}_loss_{:.3f}_acc_{:.3f}'.format(\n",
    "            epoch + 1, test_epoch_loss, 1-test_error)\n",
    "        best_path = best / 'epoch_{}_acc_{:.3f}'.format(\n",
    "                epoch + 1, 1-best_error)\n",
    "\n",
    "        Save_Checkpoint(state, last, last_path, best, best_path, is_best)\n",
    "\n",
    "        # tensorboard\n",
    "        # pdb.set_trace()\n",
    "        if epoch == 1:\n",
    "            images, labels = next(iter(train_loader))\n",
    "            img_grid = torchvision.utils.make_grid(images)\n",
    "            summary_writer.add_image('Cifar Image', img_grid)\n",
    "        summary_writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "        summary_writer.add_scalar('train_loss', train_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('train_error', train_error, epoch)\n",
    "        summary_writer.add_scalar('val_loss', test_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('val_error', test_error, epoch)\n",
    "\n",
    "        summary_writer.add_scalar('nd_loss', norm_dir_loss, epoch)\n",
    "        summary_writer.add_scalar('kd_loss', div_loss, epoch)\n",
    "        summary_writer.add_scalar('cls_loss', cls_loss, epoch)\n",
    "\n",
    "    summary_writer.close()\n",
    "    if not os.path.exists(train_acc_savepath) or not os.path.exists(train_loss_savepath):\n",
    "        np.save(train_acc_savepath, train_acc)\n",
    "        np.save(train_loss_savepath, train_loss)\n",
    "        np.save(val_acc_savepath, test_acc)\n",
    "        np.save(val_loss_savepath, test_loss)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_names = sorted(name for name in models.__dict__\n",
    "                         if name.islower() and not name.startswith(\"__\")\n",
    "                         and callable(models.__dict__[name]))\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Cifar Training')\n",
    "    parser.add_argument('-f') # added to make this run in collab\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"shufflev2_cifar\", choices=model_names, help=\"model architecture\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default='cifar100')\n",
    "    # parser.add_argument(\"--epochs\", type=int, default=240)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=4)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size per gpu\")\n",
    "    parser.add_argument('--workers', default=8, type=int, help='number of data loading workers')\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.1)\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum')\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=5e-4)\n",
    "\n",
    "    parser.add_argument(\"--teacher\", type=str, default=\"resnet32x4_cifar\", help=\"teacher architecture\")\n",
    "    parser.add_argument(\"--teacher_weights\", type=str, default=\"../ckpt/cifar_teachers/resnet32x4_vanilla/ckpt_epoch_240.pth\", help=\"teacher weights path\")\n",
    "    parser.add_argument(\"--cls_loss_factor\", type=float, default=1.0, help=\"cls loss weight factor\")\n",
    "    parser.add_argument(\"--kd_loss_factor\", type=float, default=1.0, help=\"KL loss weight factor\")\n",
    "    parser.add_argument(\"--nd_loss_factor\", type=float, default=1.0, help=\"ND loss weight factor\")\n",
    "    parser.add_argument(\"--warm_up\", type=float, default=20.0, help='loss weight warm up epochs')\n",
    "\n",
    "    # parser.add_argument(\"--gpus\", type=list, default=[0, 1])\n",
    "    parser.add_argument('--seed', default=None, type=int, help='seed for initializing training.')\n",
    "    parser.add_argument(\"--resume\", type=str, help=\"best ckpt's path to resume most recent training\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"./run\", help=\"save path, eg, acc_loss, weights, tensorboard, and so on\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        cudnn.benchmark = False\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s [line:%(lineno)d] %(message)s',\n",
    "                        datefmt='%d %b %Y %H:%M:%S')\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # args.batch_size = args.batch_size * len(args.gpus)\n",
    "    args.batch_size = args.batch_size * 1\n",
    "\n",
    "\n",
    "    # logger.info(colorstr('green', \"Distribute train, gpus:{}, total batch size:{}, epoch:{}\".format(args.gpus, args.batch_size, args.epochs)))\n",
    "    logger.info(colorstr('green', \"Distribute train, total batch size:{}, epoch:{}\".format(args.batch_size, args.epochs)))\n",
    "\n",
    "    train_set, test_set, num_class = CIFAR(name=args.dataset)\n",
    "    model = build_review_kd(student_name=args.model_name, num_class=num_class, teacher_name=args.teacher)\n",
    "    teacher = models.__dict__[args.teacher](num_class=num_class)\n",
    "\n",
    "    if args.teacher_weights:\n",
    "        print('Load Teacher Weights')\n",
    "        teacher_ckpt = torch.load(args.teacher_weights)['model']\n",
    "        teacher.load_state_dict(teacher_ckpt)\n",
    "\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # res56    ./ckpt/teacher/resnet56/center_emb_train.json\n",
    "    # res32x4  ./ckpt/teacher/resnet32x4/center_emb_train.json\n",
    "    # wrn40_2  ./ckpt/teacher/wrn_40_2/center_emb_train.json\n",
    "    # res50    ./ckpt/teacher/resnet50/center_emb_train.json\n",
    "    # class-mean\n",
    "    with open(\"./ckpt/teacher/resnet32x4/center_emb_train.json\", 'r') as f:\n",
    "        T_EMB = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    logger.info(colorstr('green', 'Use ' + args.teacher + ' Training ' + args.model_name + ' ...'))\n",
    "    # Train the model\n",
    "    epoch_loop(model=model, teacher=teacher, train_set=train_set, test_set=test_set, args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfx9TuoAb2Nc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPRnXZW4++8nhfNzsf1CEbK",
   "collapsed_sections": [
    "DgcuAfpIWV9y",
    "tmF2q0Ql1F2h"
   ],
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
