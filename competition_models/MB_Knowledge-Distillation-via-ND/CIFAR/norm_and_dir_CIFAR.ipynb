{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90543380-54d0-4a4e-853a-d6bb9b661646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66ce7e4c-6acd-4ab7-b847-c1845b0f53d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:43:52 [line:423] \u001b[32mDistribute train, total batch size:128, epoch:240\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:43:53 [line:451] \u001b[32mUse resnet56_cifar Training resnet20_cifar ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Teacher Weights\n",
      "Epoch 1/240\n",
      "2023-12-11 19:44:18 [391/391] - 39.62ms/step - nd_loss: 0.268 - kd_loss: 0.000 - cls_loss: 3.492 - train_loss: 3.761 - train_acc: 0.162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:44:19 [line:261] \u001b[32mTrain Loss: 4.225, Train Acc: 0.101, Test Loss: 3.837, Test Acc: 0.144, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/240\n",
      "2023-12-11 19:44:44 [391/391] - 40.29ms/step - nd_loss: 0.240 - kd_loss: 0.451 - cls_loss: 2.943 - train_loss: 3.634 - train_acc: 0.250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:44:45 [line:261] \u001b[32mTrain Loss: 3.945, Train Acc: 0.223, Test Loss: 3.295, Test Acc: 0.236, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/240\n",
      "2023-12-11 19:45:11 [391/391] - 39.30ms/step - nd_loss: 0.259 - kd_loss: 1.000 - cls_loss: 2.419 - train_loss: 3.678 - train_acc: 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:45:12 [line:261] \u001b[32mTrain Loss: 3.856, Train Acc: 0.310, Test Loss: 3.461, Test Acc: 0.252, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/240\n",
      "2023-12-11 19:45:37 [391/391] - 39.39ms/step - nd_loss: 0.246 - kd_loss: 1.146 - cls_loss: 2.548 - train_loss: 3.940 - train_acc: 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:45:38 [line:261] \u001b[32mTrain Loss: 3.858, Train Acc: 0.371, Test Loss: 3.498, Test Acc: 0.257, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/240\n",
      "2023-12-11 19:46:03 [391/391] - 39.87ms/step - nd_loss: 0.247 - kd_loss: 1.514 - cls_loss: 2.186 - train_loss: 3.947 - train_acc: 0.350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:46:04 [line:261] \u001b[32mTrain Loss: 3.940, Train Acc: 0.409, Test Loss: 3.645, Test Acc: 0.274, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/240\n",
      "2023-12-11 19:46:29 [391/391] - 39.31ms/step - nd_loss: 0.242 - kd_loss: 1.624 - cls_loss: 1.571 - train_loss: 3.437 - train_acc: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:46:31 [line:261] \u001b[32mTrain Loss: 4.055, Train Acc: 0.443, Test Loss: 4.195, Test Acc: 0.271, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/240\n",
      "2023-12-11 19:46:55 [391/391] - 41.36ms/step - nd_loss: 0.234 - kd_loss: 1.752 - cls_loss: 2.166 - train_loss: 4.153 - train_acc: 0.450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:46:57 [line:261] \u001b[32mTrain Loss: 4.166, Train Acc: 0.466, Test Loss: 2.663, Test Acc: 0.386, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/240\n",
      "2023-12-11 19:47:21 [391/391] - 39.46ms/step - nd_loss: 0.250 - kd_loss: 2.471 - cls_loss: 2.116 - train_loss: 4.837 - train_acc: 0.450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:47:22 [line:261] \u001b[32mTrain Loss: 4.312, Train Acc: 0.487, Test Loss: 2.504, Test Acc: 0.424, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/240\n",
      "2023-12-11 19:47:47 [391/391] - 39.67ms/step - nd_loss: 0.247 - kd_loss: 2.375 - cls_loss: 2.180 - train_loss: 4.802 - train_acc: 0.463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:47:48 [line:261] \u001b[32mTrain Loss: 4.431, Train Acc: 0.504, Test Loss: 2.847, Test Acc: 0.398, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/240\n",
      "2023-12-11 19:48:13 [391/391] - 38.72ms/step - nd_loss: 0.262 - kd_loss: 2.608 - cls_loss: 2.301 - train_loss: 5.170 - train_acc: 0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:48:14 [line:261] \u001b[32mTrain Loss: 4.597, Train Acc: 0.515, Test Loss: 2.575, Test Acc: 0.420, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/240\n",
      "2023-12-11 19:48:38 [391/391] - 39.44ms/step - nd_loss: 0.252 - kd_loss: 2.437 - cls_loss: 1.692 - train_loss: 4.381 - train_acc: 0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:48:39 [line:261] \u001b[32mTrain Loss: 4.724, Train Acc: 0.528, Test Loss: 2.659, Test Acc: 0.425, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/240\n",
      "2023-12-11 19:49:03 [391/391] - 38.30ms/step - nd_loss: 0.262 - kd_loss: 2.908 - cls_loss: 1.595 - train_loss: 4.765 - train_acc: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:49:05 [line:261] \u001b[32mTrain Loss: 4.890, Train Acc: 0.536, Test Loss: 2.344, Test Acc: 0.467, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/240\n",
      "2023-12-11 19:49:29 [391/391] - 37.88ms/step - nd_loss: 0.281 - kd_loss: 3.362 - cls_loss: 1.391 - train_loss: 5.034 - train_acc: 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:49:30 [line:261] \u001b[32mTrain Loss: 5.021, Train Acc: 0.545, Test Loss: 2.230, Test Acc: 0.484, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/240\n",
      "2023-12-11 19:49:54 [391/391] - 38.28ms/step - nd_loss: 0.278 - kd_loss: 3.751 - cls_loss: 1.827 - train_loss: 5.856 - train_acc: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:49:55 [line:261] \u001b[32mTrain Loss: 5.189, Train Acc: 0.554, Test Loss: 2.567, Test Acc: 0.454, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/240\n",
      "2023-12-11 19:50:19 [391/391] - 37.84ms/step - nd_loss: 0.275 - kd_loss: 3.636 - cls_loss: 1.911 - train_loss: 5.822 - train_acc: 0.5121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:50:21 [line:261] \u001b[32mTrain Loss: 5.341, Train Acc: 0.560, Test Loss: 2.206, Test Acc: 0.481, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/240\n",
      "2023-12-11 19:50:44 [391/391] - 38.06ms/step - nd_loss: 0.283 - kd_loss: 3.640 - cls_loss: 1.521 - train_loss: 5.444 - train_acc: 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:50:46 [line:261] \u001b[32mTrain Loss: 5.501, Train Acc: 0.565, Test Loss: 2.098, Test Acc: 0.506, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/240\n",
      "2023-12-11 19:51:09 [391/391] - 37.66ms/step - nd_loss: 0.291 - kd_loss: 3.906 - cls_loss: 1.706 - train_loss: 5.904 - train_acc: 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:51:10 [line:261] \u001b[32mTrain Loss: 5.657, Train Acc: 0.572, Test Loss: 2.124, Test Acc: 0.504, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/240\n",
      "2023-12-11 19:51:34 [391/391] - 37.90ms/step - nd_loss: 0.314 - kd_loss: 4.441 - cls_loss: 1.572 - train_loss: 6.327 - train_acc: 0.5372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:51:35 [line:261] \u001b[32mTrain Loss: 5.831, Train Acc: 0.577, Test Loss: 2.335, Test Acc: 0.487, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/240\n",
      "2023-12-11 19:51:58 [391/391] - 37.35ms/step - nd_loss: 0.332 - kd_loss: 4.051 - cls_loss: 1.723 - train_loss: 6.106 - train_acc: 0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:52:00 [line:261] \u001b[32mTrain Loss: 6.003, Train Acc: 0.582, Test Loss: 2.719, Test Acc: 0.455, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/240\n",
      "2023-12-11 19:52:23 [391/391] - 37.65ms/step - nd_loss: 0.323 - kd_loss: 4.397 - cls_loss: 1.773 - train_loss: 6.492 - train_acc: 0.575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:52:24 [line:261] \u001b[32mTrain Loss: 6.175, Train Acc: 0.581, Test Loss: 2.363, Test Acc: 0.485, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/240\n",
      "2023-12-11 19:52:48 [391/391] - 36.92ms/step - nd_loss: 0.359 - kd_loss: 4.584 - cls_loss: 1.478 - train_loss: 6.421 - train_acc: 0.6627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:52:49 [line:261] \u001b[32mTrain Loss: 6.318, Train Acc: 0.587, Test Loss: 2.090, Test Acc: 0.520, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/240\n",
      "2023-12-11 19:53:12 [391/391] - 37.19ms/step - nd_loss: 0.344 - kd_loss: 4.398 - cls_loss: 1.665 - train_loss: 6.406 - train_acc: 0.475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:53:14 [line:261] \u001b[32mTrain Loss: 6.256, Train Acc: 0.592, Test Loss: 2.515, Test Acc: 0.485, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/240\n",
      "2023-12-11 19:53:37 [391/391] - 37.36ms/step - nd_loss: 0.363 - kd_loss: 4.758 - cls_loss: 1.868 - train_loss: 6.989 - train_acc: 0.588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:53:38 [line:261] \u001b[32mTrain Loss: 6.184, Train Acc: 0.596, Test Loss: 2.418, Test Acc: 0.488, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/240\n",
      "2023-12-11 19:54:01 [391/391] - 37.12ms/step - nd_loss: 0.345 - kd_loss: 4.961 - cls_loss: 1.382 - train_loss: 6.689 - train_acc: 0.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:54:02 [line:261] \u001b[32mTrain Loss: 6.131, Train Acc: 0.597, Test Loss: 2.317, Test Acc: 0.490, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/240\n",
      "2023-12-11 19:54:26 [391/391] - 37.23ms/step - nd_loss: 0.333 - kd_loss: 4.267 - cls_loss: 1.218 - train_loss: 5.818 - train_acc: 0.637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:54:27 [line:261] \u001b[32mTrain Loss: 6.103, Train Acc: 0.600, Test Loss: 2.549, Test Acc: 0.475, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/240\n",
      "2023-12-11 19:54:50 [391/391] - 37.29ms/step - nd_loss: 0.327 - kd_loss: 4.255 - cls_loss: 1.721 - train_loss: 6.303 - train_acc: 0.600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11 Dec 2023 19:54:51 [line:261] \u001b[32mTrain Loss: 6.079, Train Acc: 0.601, Test Loss: 2.188, Test Acc: 0.506, lr: 0.10000\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 32 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "11 Dec 2023 19:54:54 [line:333] \u001b[32m- - - - - METRICS REPORT - - - - -\u001b[0m\n",
      "11 Dec 2023 19:54:54 [line:334] \u001b[32mTEACHER: accuracy: 72.410, precision: 72.663, recall: 72.410, f1: 72.424, teacher_inf: 0.007, teacher_size: 861620.000\u001b[0m\n",
      "11 Dec 2023 19:54:54 [line:335] \u001b[32mSTUDENT: accuracy: 52.000, precision: 60.450, recall: 52.000, f1: 51.904, student_inf: 0.003, student_size: 278324.000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import Models\n",
    "from Models.embtrans_cifar import EmbTrans\n",
    "from Dataset import CIFAR\n",
    "from utils import colorstr, Save_Checkpoint, AverageMeter, DirectNormLoss, KDLoss\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import warnings\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pdb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "def compare_model_size(teacher, student):\n",
    "    teacher_params = sum(p.numel() for p in teacher.parameters())\n",
    "    student_params = sum(p.numel() for p in student.parameters())\n",
    "    return teacher_params, student_params\n",
    "\n",
    "def compare_inference_time(teacher, student, dataloader):\n",
    "    inputs, _ = next(iter(dataloader))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    teacher = teacher.to(device)\n",
    "    student = student.to(device)\n",
    "    inputs = inputs.to(device)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _, teacher_outputs = teacher(inputs)\n",
    "    teacher_time = time.time() - start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _, student_outputs = student(inputs)\n",
    "    student_time = time.time() - start_time\n",
    "    \n",
    "    return teacher_time, student_time\n",
    "\n",
    "def compare_performance_metrics(teacher, student, dataloader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    teacher.eval()\n",
    "    student.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_teacher_preds = []\n",
    "    all_student_preds = []\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        with torch.no_grad():\n",
    "            _, teacher_outputs = teacher(inputs.to(device))\n",
    "            _, student_outputs = student(inputs.to(device))\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_teacher_preds.append(torch.argmax(teacher_outputs, dim=1).cpu().numpy())\n",
    "        all_student_preds.append(torch.argmax(student_outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_teacher_preds = np.concatenate(all_teacher_preds)\n",
    "    all_student_preds = np.concatenate(all_student_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': (accuracy_score(all_labels, all_teacher_preds), accuracy_score(all_labels, all_student_preds)),\n",
    "        'precision': (precision_score(all_labels, all_teacher_preds, average='weighted', zero_division=0), precision_score(all_labels, all_student_preds, average='weighted', zero_division=0)),  # Updated line\n",
    "        'recall': (recall_score(all_labels, all_teacher_preds, average='weighted'), recall_score(all_labels, all_student_preds, average='weighted')),\n",
    "        'f1': (f1_score(all_labels, all_teacher_preds, average='weighted'), f1_score(all_labels, all_student_preds, average='weighted'))\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def train(student, teacher, T_EMB, train_dataloader, optimizer, criterion, kd_loss, nd_loss, args, epoch):\n",
    "    train_loss = AverageMeter()\n",
    "    train_error = AverageMeter()\n",
    "\n",
    "    Cls_loss = AverageMeter()\n",
    "    Div_loss = AverageMeter()\n",
    "    Norm_Dir_loss = AverageMeter()\n",
    "\n",
    "    # Model on train mode\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    step_per_epoch = len(train_dataloader)\n",
    "\n",
    "    for step, (images, labels) in enumerate(train_dataloader):\n",
    "        start = time.time()\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "        # compute output\n",
    "        s_emb, s_logits = student(images, embed=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_emb, t_logits = teacher(images, embed=True)\n",
    "\n",
    "        # cls loss\n",
    "        cls_loss = criterion(s_logits, labels) * args.cls_loss_factor\n",
    "        # KD loss\n",
    "        div_loss = kd_loss(s_logits, t_logits) * min(1.0, epoch/args.warm_up)\n",
    "        # ND loss\n",
    "        norm_dir_loss = nd_loss(s_emb=s_emb, t_emb=t_emb, T_EMB=T_EMB, labels=labels)\n",
    "\n",
    "        loss = cls_loss + div_loss + norm_dir_loss\n",
    "        # measure accuracy and record loss\n",
    "        batch_size = images.size(0)\n",
    "        _, pred = s_logits.data.cpu().topk(1, dim=1)\n",
    "        train_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "        train_loss.update(loss.item(), batch_size)\n",
    "\n",
    "        Cls_loss.update(cls_loss.item(), batch_size)\n",
    "        Div_loss.update(div_loss.item(), batch_size)\n",
    "        Norm_Dir_loss.update(norm_dir_loss.item(), batch_size)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n",
    "        s1 = '\\r{} [{}/{}]'.format(t, step+1, step_per_epoch)\n",
    "        s2 = ' - {:.2f}ms/step - nd_loss: {:.3f} - kd_loss: {:.3f} - cls_loss: {:.3f} - train_loss: {:.3f} - train_acc: {:.3f}'.format(\n",
    "             1000 * (time.time() - start), norm_dir_loss.item(), div_loss.item(), cls_loss.item(), train_loss.val, 1-train_error.val)\n",
    "\n",
    "        print(s1+s2, end='', flush=True)\n",
    "\n",
    "    print()\n",
    "    return Norm_Dir_loss.avg, Div_loss.avg, Cls_loss.avg, train_loss.avg, train_error.avg\n",
    "\n",
    "\n",
    "def test(student, test_dataloader, criterion):\n",
    "    test_loss = AverageMeter()\n",
    "    test_error = AverageMeter()\n",
    "\n",
    "    # Model on eval mode\n",
    "    student.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_dataloader:\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "            # compute logits\n",
    "            logits = student(images, embed=False)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            batch_size = images.size(0)\n",
    "            _, pred = logits.data.cpu().topk(1, dim=1)\n",
    "            test_error.update(torch.ne(pred.squeeze(), labels.cpu()).float().sum().item() / batch_size, batch_size)\n",
    "            test_loss.update(loss.item(), batch_size)\n",
    "    \n",
    "\n",
    "    return test_loss.avg, test_error.avg\n",
    "\n",
    "\n",
    "def epoch_loop(student, teacher, train_set, test_set, args):\n",
    "    # data loaders\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=args.workers)\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=args.workers)\n",
    "\n",
    "    # student\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # student = nn.DataParallel(student, device_ids=args.gpus)\n",
    "    student = nn.DataParallel(student)\n",
    "    student.to(device)\n",
    "    # teacher = nn.DataParallel(teacher, device_ids=args.gpus)\n",
    "    teacher = nn.DataParallel(teacher)\n",
    "    teacher.to(device)\n",
    "\n",
    "    # loss\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    kd_loss = KDLoss(kl_loss_factor=args.kd_loss_factor, T=args.t).to(device)\n",
    "    nd_loss = DirectNormLoss(num_class=100, nd_loss_factor=args.nd_loss_factor).to(device)\n",
    "    # optimizer\n",
    "    optimizer = torch.optim.SGD(params=student.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=True)\n",
    "\n",
    "    # weights\n",
    "    save_dir = Path(args.save_dir)\n",
    "    weights = save_dir / 'weights'\n",
    "    weights.mkdir(parents=True, exist_ok=True)\n",
    "    last = weights / 'last'\n",
    "    best = weights / 'best'\n",
    "\n",
    "    # acc,loss\n",
    "    acc_loss = save_dir / 'acc_loss'\n",
    "    acc_loss.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_acc_savepath = acc_loss / 'train_acc.npy'\n",
    "    train_loss_savepath = acc_loss / 'train_loss.npy'\n",
    "    val_acc_savepath = acc_loss / 'val_acc.npy'\n",
    "    val_loss_savepath = acc_loss / 'val_loss.npy'\n",
    "\n",
    "    # tensorboard\n",
    "    logdir = save_dir / 'logs'\n",
    "    logdir.mkdir(parents=True, exist_ok=True)\n",
    "    summary_writer = SummaryWriter(logdir, flush_secs=120)\n",
    "\n",
    "    # resume\n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        student.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        best_error = checkpoint['best_error']\n",
    "        train_acc = checkpoint['train_acc']\n",
    "        train_loss = checkpoint['train_loss']\n",
    "        test_acc = checkpoint['test_acc']\n",
    "        test_loss = checkpoint['test_loss']\n",
    "        logger.info(colorstr('green', 'Resuming training from {} epoch'.format(start_epoch)))\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        best_error = 0\n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        test_acc = []\n",
    "        test_loss = []\n",
    "        test_precision = []\n",
    "        test_recall = []\n",
    "        test_f1 = []\n",
    "\n",
    "    # Train student\n",
    "    best_error = 1\n",
    "    ##\n",
    "    patience = args.patience\n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = float('inf')\n",
    "    epoch_val_losses = []\n",
    "    epoch_val_accuracies = []\n",
    "    ##\n",
    "    for epoch in range(start_epoch, args.epochs):\n",
    "        if epoch in [150, 180, 210]:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        print(\"Epoch {}/{}\".format(epoch + 1, args.epochs))\n",
    "        norm_dir_loss, div_loss, cls_loss, train_epoch_loss, train_error = train(student=student,\n",
    "                                                                                 teacher=teacher,\n",
    "                                                                                 T_EMB=T_EMB,\n",
    "                                                                                 train_dataloader=train_loader,\n",
    "                                                                                 optimizer=optimizer,\n",
    "                                                                                 criterion=criterion,\n",
    "                                                                                 kd_loss=kd_loss,\n",
    "                                                                                 nd_loss=nd_loss,\n",
    "                                                                                 args=args,\n",
    "                                                                                 epoch=epoch)\n",
    "        test_epoch_loss, test_error = test(student=student,\n",
    "                                        test_dataloader=test_loader,\n",
    "                                        criterion=criterion)\n",
    "\n",
    "        s = \"Train Loss: {:.3f}, Train Acc: {:.3f}, Test Loss: {:.3f}, Test Acc: {:.3f}, lr: {:.5f}\".format(\n",
    "            train_epoch_loss, 1-train_error, test_epoch_loss, 1-test_error, optimizer.param_groups[0]['lr'])\n",
    "        logger.info(colorstr('green', s))\n",
    "\n",
    "        # save acc,loss\n",
    "        train_loss.append(train_epoch_loss)\n",
    "        train_acc.append(1-train_error)\n",
    "        test_loss.append(test_epoch_loss)\n",
    "        test_acc.append(1-test_error)\n",
    "\n",
    "        epoch_val_accuracies.append(1-test_error)\n",
    "        epoch_val_losses.append(test_epoch_loss)\n",
    "\n",
    "        # save student\n",
    "        is_best = test_error < best_error\n",
    "        best_error = min(best_error, test_error)\n",
    "        state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': student.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_error': best_error,\n",
    "                'train_acc': train_acc,\n",
    "                'train_loss': train_loss,\n",
    "                'test_acc': test_acc,\n",
    "                'test_loss': test_loss,\n",
    "            }\n",
    "\n",
    "        # last_path = last / 'epoch_{}_loss_{:.3f}_acc_{:.3f}'.format(\n",
    "        last_path = last / 'last_'\n",
    "        # best_path = best / 'epoch_{}_acc_{:.3f}'.format(\n",
    "        best_path = best / 'best_'\n",
    "\n",
    "        Save_Checkpoint(state, last, last_path, best, best_path, is_best)\n",
    "\n",
    "        # tensorboard\n",
    "        if epoch == 1:\n",
    "            images, labels = next(iter(train_loader))\n",
    "            img_grid = torchvision.utils.make_grid(images)\n",
    "            summary_writer.add_image('Cifar Image', img_grid)\n",
    "        summary_writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n",
    "        summary_writer.add_scalar('train_loss', train_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('train_error', train_error, epoch)\n",
    "        summary_writer.add_scalar('val_loss', test_epoch_loss, epoch)\n",
    "        summary_writer.add_scalar('val_error', test_error, epoch)\n",
    "\n",
    "        summary_writer.add_scalar('nd_loss', norm_dir_loss, epoch)\n",
    "        summary_writer.add_scalar('kd_loss', div_loss, epoch)\n",
    "        summary_writer.add_scalar('cls_loss', cls_loss, epoch)\n",
    "\n",
    "        ### \n",
    "        # Check if current validation combined loss is lower than the best combined loss\n",
    "        if test_epoch_loss < best_val_loss:\n",
    "            best_val_loss = test_epoch_loss\n",
    "            best_val_accuracy = 1-test_error\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "            # print results\n",
    "            best_checkpoint = torch.load(os.path.join(best_path, 'ckpt.pth'))['model_state_dict']\n",
    "            student.load_state_dict(best_checkpoint)\n",
    "            metrics = compare_performance_metrics(teacher, student, test_loader)\n",
    "            teacher_time, student_time = compare_inference_time(teacher, student, test_loader)\n",
    "            teacher_size, student_size = compare_model_size(teacher, student)\n",
    "\n",
    "            final_report_banner = '- - - - - METRICS REPORT - - - - -'\n",
    "            teacher_metrics = \"TEACHER: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, teacher_inf: {:.3f}, teacher_size: {:.3f}\".format(\n",
    "                100*metrics['accuracy'][0], 100*metrics['precision'][0], 100*metrics['recall'][0], 100*metrics['f1'][0], \n",
    "                teacher_time, teacher_size,)\n",
    "            student_metrics = \"STUDENT: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, student_inf: {:.3f}, student_size: {:.3f}\".format(\n",
    "                100*metrics['accuracy'][1], 100*metrics['precision'][1], 100*metrics['recall'][1], 100*metrics['f1'][1], \n",
    "                 student_time, student_size)\n",
    "            logger.info(colorstr('green', final_report_banner))\n",
    "            logger.info(colorstr('green', teacher_metrics))\n",
    "            logger.info(colorstr('green', student_metrics))\n",
    "            break\n",
    "\n",
    "        if epoch == (args.epochs - 1):\n",
    "            best_checkpoint = torch.load(os.path.join(best_path, 'ckpt.pth'))['model_state_dict']\n",
    "            student.load_state_dict(best_checkpoint)\n",
    "            metrics = compare_performance_metrics(teacher, student, test_loader)\n",
    "            teacher_time, student_time = compare_inference_time(teacher, student, test_loader)\n",
    "            teacher_size, student_size = compare_model_size(teacher, student)\n",
    "\n",
    "            final_report_banner = '- - - - - METRICS REPORT - - - - -'\n",
    "            teacher_metrics = \"TEACHER: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, teacher_inf: {:.3f}, teacher_size: {:.3f}\".format(\n",
    "                100*metrics['accuracy'][0], 100*metrics['precision'][0], 100*metrics['recall'][0], 100*metrics['f1'][0], \n",
    "                teacher_time, teacher_size,)\n",
    "            student_metrics = \"STUDENT: accuracy: {:.3f}, precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, student_inf: {:.3f}, student_size: {:.3f}\".format(\n",
    "                100*metrics['accuracy'][1], 100*metrics['precision'][1], 100*metrics['recall'][1], 100*metrics['f1'][1], \n",
    "                 student_time, student_size)\n",
    "            logger.info(colorstr('green', final_report_banner))\n",
    "            logger.info(colorstr('green', teacher_metrics))\n",
    "            logger.info(colorstr('green', student_metrics))\n",
    "            \n",
    "        \n",
    "        ###\n",
    "\n",
    "    summary_writer.close()\n",
    "    if not os.path.exists(train_acc_savepath) or not os.path.exists(train_loss_savepath):\n",
    "        np.save(train_acc_savepath, train_acc)\n",
    "        np.save(train_loss_savepath, train_loss)\n",
    "        np.save(val_acc_savepath, test_acc)\n",
    "        np.save(val_loss_savepath, test_loss)\n",
    "    \n",
    "    return student, teacher, test_loader\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    student_names = sorted(name for name in Models.__dict__\n",
    "                         if name.islower() and not name.startswith(\"__\")\n",
    "                         and callable(Models.__dict__[name]))\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Cifar Training')\n",
    "    parser.add_argument('-f') # added to make this run in collab\n",
    "    parser.add_argument(\"--student_name\", type=str, default=\"resnet20_cifar\", choices=student_names, help=\"student architecture\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default='cifar100')\n",
    "    parser.add_argument(\"--epochs\", type=int, default=240)\n",
    "    # parser.add_argument(\"--epochs\", type=int, default=4)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128, help=\"batch size per gpu\")\n",
    "    parser.add_argument('--workers', default=32, type=int, help='number of data loading workers')\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.1)\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='SGD momentum')\n",
    "    parser.add_argument(\"--weight_decay\", type=float, default=5e-4)\n",
    "\n",
    "    parser.add_argument(\"--teacher\", type=str, default=\"resnet56_cifar\", help=\"teacher architecture\")\n",
    "    parser.add_argument(\"--teacher_weights\", type=str, default=\"./ckpt/cifar_teachers/resnet56_vanilla/ckpt_epoch_240.pth\", help=\"teacher weights path\")\n",
    "    parser.add_argument(\"--cls_loss_factor\", type=float, default=1.0, help=\"cls loss weight factor\")\n",
    "    parser.add_argument(\"--kd_loss_factor\", type=float, default=1.0, help=\"KD loss weight factor\")\n",
    "    parser.add_argument(\"--t\", type=float, default=4.0, help=\"temperature\")\n",
    "    parser.add_argument(\"--nd_loss_factor\", type=float, default=1.0, help=\"ND loss weight factor\")\n",
    "    parser.add_argument(\"--warm_up\", type=float, default=20.0, help='loss weight warm up epochs')\n",
    "    parser.add_argument(\"--patience\", type=int, default=5, help='loss weight warm up epochs')\n",
    "\n",
    "\n",
    "    # parser.add_argument(\"--gpus\", type=list, default=[0, 1])\n",
    "    \n",
    "    parser.add_argument('--seed', default=None, type=int, help='seed for initializing training.')\n",
    "    parser.add_argument(\"--resume\", type=str, help=\"best ckpt's path to resume most recent training\")\n",
    "    parser.add_argument(\"--save_dir\", type=str, default=\"./run/KD++\", help=\"save path, eg, acc_loss, weights, tensorboard, and so on\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        cudnn.benchmark = False\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s [line:%(lineno)d] %(message)s', \n",
    "                        datefmt='%d %b %Y %H:%M:%S')\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # args.batch_size = args.batch_size * len(args.gpus)\n",
    "    args.batch_size = args.batch_size * 1\n",
    "\n",
    "    # logger.info(colorstr('green', \"Distribute train, gpus:{}, total batch size:{}, epoch:{}\".format(args.gpus, args.batch_size, args.epochs)))\n",
    "    logger.info(colorstr('green', \"Distribute train, total batch size:{}, epoch:{}\".format(args.batch_size, args.epochs)))\n",
    "\n",
    "\n",
    "    train_set, test_set, num_class = CIFAR(name=args.dataset)\n",
    "    student = Models.__dict__[args.student_name](num_class=num_class)\n",
    "\n",
    "    if args.student_name in ['wrn40_1_cifar', 'mobilenetv2', 'shufflev1_cifar', 'shufflev2_cifar']:\n",
    "        student = EmbTrans(student=student, student_name=args.student_name)\n",
    "\n",
    "    teacher = Models.__dict__[args.teacher](num_class=num_class)\n",
    "\n",
    "    if args.teacher_weights:\n",
    "        print('Load Teacher Weights')\n",
    "        teacher_ckpt = torch.load(args.teacher_weights)['model']\n",
    "        teacher.load_state_dict(teacher_ckpt)\n",
    "\n",
    "        for param in teacher.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # res56    ./ckpt/teacher/resnet56/center_emb_train.json\n",
    "    # res32x4  ./ckpt/teacher/resnet32x4/center_emb_train.json\n",
    "    # wrn40_2  ./ckpt/teacher/wrn_40_2/center_emb_train.json\n",
    "    # res50    ./ckpt/teacher/resnet50/center_emb_train.json\n",
    "    # class-mean\n",
    "    with open(\"./ckpt/teacher/resnet56/center_emb_train.json\", 'r') as f:\n",
    "        T_EMB = json.load(f)\n",
    "    f.close()\n",
    "\n",
    "    logger.info(colorstr('green', 'Use ' + args.teacher + ' Training ' + args.student_name + ' ...'))\n",
    "    # Train the student\n",
    "    student, teacher, test_loader = epoch_loop(student=student, teacher=teacher, train_set=train_set, test_set=test_set, args=args)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8745ef21-0948-4c41-91a2-c94016d08531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
