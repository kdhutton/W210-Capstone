{"cells":[{"cell_type":"code","execution_count":null,"id":"6c0a8b3b-5189-4857-887c-e93b637bf000","metadata":{"id":"6c0a8b3b-5189-4857-887c-e93b637bf000","outputId":"f237dd81-028a-4b98-f565-4c0d4f5de7e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.0)\n","Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch) (2.1.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch) (10.3.2.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch) (2.18.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.3.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.4)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch) (8.9.2.26)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.10.0)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch) (11.4.5.107)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.8.0)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.0.106)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch) (12.1.105)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.52)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.8.2)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3)\n","Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas) (1.26.1)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (0.22.0)\n","Requirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.11.3)\n","Requirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (10.1.0)\n","Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (3.2)\n","Requirement already satisfied: packaging>=21 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (23.2)\n","Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2.31.5)\n","Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (0.3)\n","Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (2023.9.26)\n","Requirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from scikit-image) (1.26.1)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.3.1)\n","Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.3)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: numpy<2.0,>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.26.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.8.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (23.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.1.1)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.1.0)\n","Requirement already satisfied: numpy<2,>=1.21 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.26.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (10.1.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.1)\n","Requirement already satisfied: torch==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.1.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (3.12.4)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (3.1.2)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (1.12)\n","Requirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (2.1.0)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (12.1.105)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (12.1.3.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (4.8.0)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (8.9.2.26)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (12.1.0.106)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (10.3.2.106)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (12.1.105)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (12.1.105)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (2.18.1)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (3.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (2023.10.0)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (12.1.105)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0->torchvision) (11.4.5.107)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->torchvision) (12.3.52)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.14)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.0->torchvision) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.0->torchvision) (1.3.0)\n","Requirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (2023.10.0)\n","Requirement already satisfied: aiobotocore~=2.7.0 in /opt/conda/lib/python3.10/site-packages (from s3fs) (2.7.0)\n","Requirement already satisfied: fsspec==2023.10.0 in /opt/conda/lib/python3.10/site-packages (from s3fs) (2023.10.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from s3fs) (3.8.6)\n","Collecting botocore<1.31.65,>=1.31.16\n","  Using cached botocore-1.31.64-py3-none-any.whl (11.3 MB)\n","Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.7.0->s3fs) (1.15.0)\n","Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore~=2.7.0->s3fs) (0.11.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.1.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.4)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.1.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.9.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs) (2.8.2)\n","Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs) (1.26.14)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs) (1.0.1)\n","Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.4)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.31.65,>=1.31.16->aiobotocore~=2.7.0->s3fs) (1.16.0)\n","Installing collected packages: botocore\n","  Attempting uninstall: botocore\n","    Found existing installation: botocore 1.31.68\n","    Uninstalling botocore-1.31.68:\n","      Successfully uninstalled botocore-1.31.68\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","boto3 1.28.68 requires botocore<1.32.0,>=1.31.68, but you have botocore 1.31.64 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed botocore-1.31.64\n","Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (1.28.68)\n","Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from boto3) (0.7.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3) (1.0.1)\n","Collecting botocore<1.32.0,>=1.31.68\n","  Using cached botocore-1.31.68-py3-none-any.whl (11.3 MB)\n","Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.68->boto3) (1.26.14)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.32.0,>=1.31.68->boto3) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.68->boto3) (1.16.0)\n","Installing collected packages: botocore\n","  Attempting uninstall: botocore\n","    Found existing installation: botocore 1.31.64\n","    Uninstalling botocore-1.31.64:\n","      Successfully uninstalled botocore-1.31.64\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","aiobotocore 2.7.0 requires botocore<1.31.65,>=1.31.16, but you have botocore 1.31.68 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed botocore-1.31.68\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.64.1)\n","Requirement already satisfied: fiftyone in /opt/conda/lib/python3.10/site-packages (0.22.2)\n","Requirement already satisfied: mongoengine==0.24.2 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.24.2)\n","Requirement already satisfied: motor>=2.5 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (3.3.1)\n","Requirement already satisfied: aiofiles in /opt/conda/lib/python3.10/site-packages (from fiftyone) (23.2.1)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from fiftyone) (3.8.0)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from fiftyone) (5.9.6)\n","Requirement already satisfied: sseclient-py<2,>=1.7.2 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.8.0)\n","Requirement already satisfied: plotly>=4.14 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (5.17.0)\n","Requirement already satisfied: Jinja2>=3 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (3.1.2)\n","Requirement already satisfied: sse-starlette<1,>=0.10.3 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.10.3)\n","Requirement already satisfied: universal-analytics-python3<2,>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.1.1)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.3.1)\n","Requirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.28.68)\n","Requirement already satisfied: cachetools in /opt/conda/lib/python3.10/site-packages (from fiftyone) (5.3.1)\n","Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.9.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fiftyone) (23.2)\n","Requirement already satisfied: Pillow>=6.2 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (10.1.0)\n","Requirement already satisfied: voxel51-eta~=0.12 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.12.0)\n","Requirement already satisfied: strawberry-graphql==0.138.1 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.138.1)\n","Requirement already satisfied: retrying in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.3.4)\n","Requirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from fiftyone) (6.1.1)\n","Requirement already satisfied: pytz in /opt/conda/lib/python3.10/site-packages (from fiftyone) (2023.3.post1)\n","Requirement already satisfied: hypercorn>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.14.4)\n","Requirement already satisfied: starlette>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.31.1)\n","Requirement already satisfied: fiftyone-brain~=0.13.2 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.13.2)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from fiftyone) (2.1.1)\n","Requirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.22.0)\n","Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (4.12.2)\n","Requirement already satisfied: xmltodict in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.13.0)\n","Requirement already satisfied: dacite<1.8.0,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.7.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from fiftyone) (65.6.3)\n","Requirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from fiftyone) (6.0.1)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.26.1)\n","Requirement already satisfied: Deprecated in /opt/conda/lib/python3.10/site-packages (from fiftyone) (1.2.14)\n","Requirement already satisfied: fiftyone-db~=0.4 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.4.0)\n","Requirement already satisfied: argcomplete in /opt/conda/lib/python3.10/site-packages (from fiftyone) (3.1.2)\n","Requirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from fiftyone) (2023.10.3)\n","Requirement already satisfied: pprintpp in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.4.0)\n","Requirement already satisfied: kaleido!=0.2.1.post1 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (0.2.1)\n","Requirement already satisfied: pymongo>=3.12 in /opt/conda/lib/python3.10/site-packages (from fiftyone) (4.5.0)\n","Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (from fiftyone) (4.8.1.78)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /opt/conda/lib/python3.10/site-packages (from strawberry-graphql==0.138.1->fiftyone) (2.8.2)\n","Requirement already satisfied: graphql-core<3.3.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from strawberry-graphql==0.138.1->fiftyone) (3.2.3)\n","Requirement already satisfied: typing_extensions<5.0.0,>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from strawberry-graphql==0.138.1->fiftyone) (4.8.0)\n","Requirement already satisfied: scipy>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from fiftyone-brain~=0.13.2->fiftyone) (1.11.3)\n","Requirement already satisfied: h2>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (4.1.0)\n","Requirement already satisfied: wsproto>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (1.2.0)\n","Requirement already satisfied: h11 in /opt/conda/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (0.14.0)\n","Requirement already satisfied: priority in /opt/conda/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (2.0.0)\n","Requirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from hypercorn>=0.13.2->fiftyone) (2.0.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3->fiftyone) (2.1.3)\n","Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly>=4.14->fiftyone) (8.2.3)\n","Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from pymongo>=3.12->fiftyone) (2.4.2)\n","Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette>=0.24.0->fiftyone) (4.0.0)\n","Requirement already satisfied: httpx>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from universal-analytics-python3<2,>=1.0.1->fiftyone) (0.25.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from voxel51-eta~=0.12->fiftyone) (2.31.0)\n","Requirement already satisfied: sortedcontainers in /opt/conda/lib/python3.10/site-packages (from voxel51-eta~=0.12->fiftyone) (2.4.0)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from voxel51-eta~=0.12->fiftyone) (0.3.7)\n","Requirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from voxel51-eta~=0.12->fiftyone) (0.18.3)\n","Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from voxel51-eta~=0.12->fiftyone) (1.26.14)\n","Requirement already satisfied: glob2 in /opt/conda/lib/python3.10/site-packages (from voxel51-eta~=0.12->fiftyone) (0.7)\n","Requirement already satisfied: jsonlines in /opt/conda/lib/python3.10/site-packages (from voxel51-eta~=0.12->fiftyone) (4.0.0)\n","Requirement already satisfied: py7zr in /opt/conda/lib/python3.10/site-packages (from voxel51-eta~=0.12->fiftyone) (0.20.6)\n","Requirement already satisfied: tzlocal in /opt/conda/lib/python3.10/site-packages (from voxel51-eta~=0.12->fiftyone) (5.2)\n","Requirement already satisfied: rarfile in /opt/conda/lib/python3.10/site-packages (from voxel51-eta~=0.12->fiftyone) (4.1)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from voxel51-eta~=0.12->fiftyone) (1.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->fiftyone) (2.5)\n","Requirement already satisfied: botocore<1.32.0,>=1.31.68 in /opt/conda/lib/python3.10/site-packages (from boto3->fiftyone) (1.31.68)\n","Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from boto3->fiftyone) (0.7.0)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->fiftyone) (1.0.1)\n","Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from Deprecated->fiftyone) (1.15.0)\n","Requirement already satisfied: wcwidth>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from ftfy->fiftyone) (0.2.8)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fiftyone) (0.12.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fiftyone) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fiftyone) (3.1.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fiftyone) (1.1.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->fiftyone) (4.43.1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->fiftyone) (2023.3)\n","Requirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->fiftyone) (2.31.5)\n","Requirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image->fiftyone) (0.3)\n","Requirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->fiftyone) (2023.9.26)\n","Requirement already satisfied: networkx>=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->fiftyone) (3.2)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->fiftyone) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->fiftyone) (3.2.0)\n","Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (3.4)\n","Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (1.3.0)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (1.1.3)\n","Requirement already satisfied: hyperframe<7,>=6.0 in /opt/conda/lib/python3.10/site-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (6.0.1)\n","Requirement already satisfied: hpack<5,>=4.0 in /opt/conda/lib/python3.10/site-packages (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone) (4.0.0)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2022.12.7)\n","Requirement already satisfied: httpcore<0.19.0,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (0.18.0)\n","Requirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonlines->voxel51-eta~=0.12->fiftyone) (23.1.0)\n","Requirement already satisfied: inflate64>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (0.3.1)\n","Requirement already satisfied: multivolumefile>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (0.2.3)\n","Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.0.0)\n","Requirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.7.0)\n","Requirement already satisfied: pycryptodomex>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (3.19.0)\n","Requirement already satisfied: pyzstd>=0.14.4 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (0.15.9)\n","Requirement already satisfied: brotli>=1.0.9 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.1.0)\n","Requirement already satisfied: pybcj>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from py7zr->voxel51-eta~=0.12->fiftyone) (1.0.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->voxel51-eta~=0.12->fiftyone) (2.1.1)\n","Requirement already satisfied: pycocotools in /opt/conda/lib/python3.10/site-packages (2.0.7)\n","Requirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools) (3.8.0)\n","Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pycocotools) (1.26.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (2.8.2)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (10.1.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (4.43.1)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (0.12.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (3.1.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.1.1)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (23.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools) (1.4.5)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools) (1.16.0)\n"]}],"source":["# install for AWS\n","!pip install torch --quiet\n","!pip install pandas --quiet\n","!pip install scikit-image --quiet\n","!pip install scikit-learn --quiet\n","!pip install matplotlib --quiet\n","!pip install torchvision --quiet\n","!pip install s3fs --quiet\n","!pip install boto3 --quiet\n","!pip install tqdm --quiet\n","!pip install fiftyone --quiet\n","!pip install pycocotools --quiet"]},{"cell_type":"code","execution_count":null,"id":"76c928e2-c18b-457c-8640-f03b4b11b979","metadata":{"id":"76c928e2-c18b-457c-8640-f03b4b11b979"},"outputs":[],"source":["import os\n","import torch\n","import tarfile\n","import shutil\n","import torchvision\n","import random\n","import warnings\n","import boto3\n","import s3fs\n","import io\n","import time\n","import botocore.exceptions\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import getpass\n","import json\n","# import torch.jit as jit\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torchvision import transforms, utils, models, datasets\n","from torch import nn, optim\n","from torch.optim import lr_scheduler\n","from io import BytesIO\n","from tqdm import tqdm\n","from skimage import io, transform\n","from PIL import Image\n","from pycocotools import mask as maskUtils\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","import fiftyone as fo\n","import fiftyone.brain as fob\n","import fiftyone.zoo as foz\n","from fiftyone import ViewField as F"]},{"cell_type":"code","execution_count":null,"id":"7f329f00-bc92-4a2c-9f76-9d7a8f3aff44","metadata":{"id":"7f329f00-bc92-4a2c-9f76-9d7a8f3aff44"},"outputs":[],"source":["########## LM ##########\n","\n","# access_key = getpass.getpass(\"Enter your access: \")\n","\n","# secret_key = password = getpass.getpass(\"Enter your secret: \")\n","\n","# bucket_name = 'w210facetdata'\n","# annotations_prefix = 'annotations/'\n","# images_prefix = '/home/ubuntu/W210-Capstone'\n","\n","# s3 = s3fs.S3FileSystem(key=access_key, secret=secret_key)\n","\n","# # Use s3.open to open the CSV file and read its content into a Pandas DataFrame\n","# with s3.open(f's3://{bucket_name}/{annotations_prefix}annotations.csv', 'rb') as file:\n","#     gt_df = pd.read_csv(file)\n","\n","# ## use relative paths to your image dirs\n","# # dataset = fo.Dataset(name = \"FACET14\", persistent=True)\n","# dataset = fo.load_dataset('FACET14')\n","# # dataset.add_images_dir(images_prefix)\n","# dataset.compute_metadata()"]},{"cell_type":"code","execution_count":null,"id":"bf7a9c86-d180-4065-b902-93d75cf2515e","metadata":{"id":"bf7a9c86-d180-4065-b902-93d75cf2515e"},"outputs":[],"source":["########## KH ##########\n","\n","# Initialize S3 client\n","s3_client = boto3.client('s3', region_name='us-west-2')\n","\n","# Define the S3 bucket name and prefixes\n","bucket_name = 'w210facetdata'\n","annotations_prefix = 'annotations/'\n","images_prefix = 'images/'\n","\n","# Load CSV annotations from S3\n","annotations_s3_path = f's3://{bucket_name}/{annotations_prefix}'\n","gt_df = pd.read_csv(f'{annotations_s3_path}annotations.csv')"]},{"cell_type":"code","execution_count":null,"id":"04df1e08-51f4-42ee-9459-9f3fa86b1340","metadata":{"id":"04df1e08-51f4-42ee-9459-9f3fa86b1340","outputId":"a551f989-c0ac-4709-e03a-6e0807ac0e44"},"outputs":[{"name":"stdout","output_type":"stream","text":["31702\n"]}],"source":["########## KH ##########\n","local_images_dir = 'local_images_dir'\n","os.makedirs(local_images_dir, exist_ok=True)\n","\n","# Create a paginator to handle pagination of the results\n","paginator = s3_client.get_paginator('list_objects_v2')\n","\n","# Use the paginator to retrieve all objects\n","for page in paginator.paginate(Bucket=bucket_name, Prefix=images_prefix):\n","    for obj in page.get('Contents', []):\n","        # Skip the prefix itself\n","        if obj['Key'] == images_prefix:\n","            continue\n","        local_file_path = os.path.join(local_images_dir, os.path.basename(obj['Key']))\n","        s3_client.download_file(bucket_name, obj['Key'], local_file_path)\n"]},{"cell_type":"code","execution_count":null,"id":"3dc58bbf-ed5e-4f3a-b86a-77e58abb1ad2","metadata":{"id":"3dc58bbf-ed5e-4f3a-b86a-77e58abb1ad2","outputId":"4edb43e6-4d83-4418-b6b0-83526e476033"},"outputs":[{"name":"stdout","output_type":"stream","text":["31702\n"]}],"source":["# Count the number of files in the local_images_dir\n","num_files = len([f for f in os.listdir(local_images_dir) if os.path.isfile(os.path.join(local_images_dir, f))])\n","print(num_files)"]},{"cell_type":"code","execution_count":null,"id":"3f0cfd8a-0b73-4f7b-8b4a-33af7033b4cf","metadata":{"id":"3f0cfd8a-0b73-4f7b-8b4a-33af7033b4cf","outputId":"5265f540-a2b7-408d-845e-dbc534cf38fc"},"outputs":[{"name":"stdout","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":[" 100% |█████████████| 31703/31703 [4.9s elapsed, 0s remaining, 6.8K samples/s]      \n","Computing metadata...\n"," 100% |█████████████| 31703/31703 [1.3m elapsed, 0s remaining, 553.3 samples/s]      \n"]}],"source":["########## KH ##########\n","local_images_dir = 'local_images_dir'\n","\n","fo.delete_dataset('local_images_dir')\n","dataset = fo.Dataset(name='local_images_dir')\n","\n","dataset.add_images_dir(local_images_dir)\n","dataset.compute_metadata()"]},{"cell_type":"markdown","id":"47c721ef-146f-4f48-a411-7fb93bca9d84","metadata":{"id":"47c721ef-146f-4f48-a411-7fb93bca9d84"},"source":["# Object Detection Functions"]},{"cell_type":"code","execution_count":null,"id":"be99a8b0-a415-4560-a578-2b9f8bba27ae","metadata":{"id":"be99a8b0-a415-4560-a578-2b9f8bba27ae"},"outputs":[],"source":["BOOLEAN_PERSONAL_ATTRS = (\n","    \"has_facial_hair\",\n","    \"has_tattoo\",\n","    \"has_cap\",\n","    \"has_mask\",\n","    \"has_headscarf\",\n","    \"has_eyeware\",\n",")\n","def add_boolean_person_attributes(detection, row_index):\n","    for attr in BOOLEAN_PERSONAL_ATTRS:\n","        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"]},{"cell_type":"code","execution_count":null,"id":"892ad9c7-cb94-49ee-bbe6-caf72e3e7bf9","metadata":{"id":"892ad9c7-cb94-49ee-bbe6-caf72e3e7bf9"},"outputs":[],"source":["def get_hairtype(row_index):\n","    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hairtype')]\n","    hairtype = hair_info[hair_info == 1]\n","    if len(hairtype) == 0:\n","        return None\n","    return hairtype.index[0].split('_')[1]\n","\n","def get_haircolor(row_index):\n","    hair_info = gt_df.loc[row_index, gt_df.columns.str.startswith('hair_color')]\n","    haircolor = hair_info[hair_info == 1]\n","    if len(haircolor) == 0:\n","        return None\n","    return haircolor.index[0].split('_')[2]"]},{"cell_type":"code","execution_count":null,"id":"d8fc5510-2fd8-42f9-a58a-c301286b3f24","metadata":{"id":"d8fc5510-2fd8-42f9-a58a-c301286b3f24"},"outputs":[],"source":["def add_person_attributes(detection, row_index):\n","    detection[\"hairtype\"] = get_hairtype(row_index)\n","    detection[\"haircolor\"] = get_haircolor(row_index)\n","    add_boolean_person_attributes(detection, row_index)"]},{"cell_type":"code","execution_count":null,"id":"0091b7eb-d570-4b21-8290-17fee15d226f","metadata":{"id":"0091b7eb-d570-4b21-8290-17fee15d226f"},"outputs":[],"source":["def get_perceived_gender_presentation(row_index):\n","    gender_info = gt_df.loc[row_index, gt_df.columns.str.startswith('gender')]\n","    pgp = gender_info[gender_info == 1]\n","    if len(pgp) == 0:\n","        return None\n","    return pgp.index[0].replace(\"gender_presentation_\", \"\").replace(\"_\", \" \")\n","\n","def get_perceived_age_presentation(row_index):\n","    age_info = gt_df.loc[row_index, gt_df.columns.str.startswith('age')]\n","    pap = age_info[age_info == 1]\n","    if len(pap) == 0:\n","        return None\n","    return pap.index[0].split('_')[2]"]},{"cell_type":"code","execution_count":null,"id":"c49856f0-9c40-44f1-b8dc-afc8a0c80786","metadata":{"id":"c49856f0-9c40-44f1-b8dc-afc8a0c80786"},"outputs":[],"source":["def get_skintone(row_index):\n","    skin_info = gt_df.loc[row_index, gt_df.columns.str.startswith('skin_tone')]\n","    return skin_info.to_dict()"]},{"cell_type":"code","execution_count":null,"id":"1a702e8b-93a2-4e8b-879c-f82be93830ea","metadata":{"id":"1a702e8b-93a2-4e8b-879c-f82be93830ea"},"outputs":[],"source":["def add_protected_attributes(detection, row_index):\n","    detection[\"perceived_age_presentation\"] = get_perceived_age_presentation(row_index)\n","    detection[\"perceived_gender_presentation\"] = get_perceived_gender_presentation(row_index)\n","    detection[\"skin_tone\"] = get_skintone(row_index)"]},{"cell_type":"code","execution_count":null,"id":"01f3a95d-e491-4de4-be5a-d53de547f0d4","metadata":{"id":"01f3a95d-e491-4de4-be5a-d53de547f0d4"},"outputs":[],"source":["VISIBILITY_ATTRS = (\"visible_torso\", \"visible_face\", \"visible_minimal\")"]},{"cell_type":"code","execution_count":null,"id":"cc9ed1ed-c71d-4583-963f-bfa23563309c","metadata":{"id":"cc9ed1ed-c71d-4583-963f-bfa23563309c"},"outputs":[],"source":["def get_lighting(row_index):\n","    lighting_info = gt_df.loc[row_index, gt_df.columns.str.startswith('lighting')]\n","    lighting = lighting_info[lighting_info == 1]\n","    if len(lighting) == 0:\n","        return None\n","    lighting = lighting.index[0].replace(\"lighting_\", \"\").replace(\"_\", \" \")\n","    return lighting\n","\n","def add_other_attributes(detection, row_index):\n","    detection[\"lighting\"] = get_lighting(row_index)\n","    for attr in VISIBILITY_ATTRS:\n","        detection[attr] = gt_df.loc[row_index, attr].astype(bool)"]},{"cell_type":"code","execution_count":null,"id":"3de3c768-bfa3-4727-95ed-59346a5788a9","metadata":{"id":"3de3c768-bfa3-4727-95ed-59346a5788a9"},"outputs":[],"source":["def create_detection(row_index, sample):\n","    bbox_dict = json.loads(gt_df.loc[row_index, \"bounding_box\"])\n","    x, y, w, h = bbox_dict[\"x\"], bbox_dict[\"y\"], bbox_dict[\"width\"], bbox_dict[\"height\"]\n","    cat1, cat2 = bbox_dict[\"dict_attributes\"][\"cat1\"], bbox_dict[\"dict_attributes\"][\"cat2\"]\n","\n","    person_id = gt_df.loc[row_index, \"person_id\"]\n","\n","    img_width, img_height = sample.metadata.width, sample.metadata.height\n","\n","    bounding_box = [x/img_width, y/img_height, w/img_width, h/img_height]\n","    detection = fo.Detection(\n","        label=cat1,\n","        bounding_box=bounding_box,\n","        person_id=person_id,\n","        )\n","    if cat2 != 'none':\n","        detection[\"class2\"] = cat2\n","\n","    add_person_attributes(detection, row_index)\n","    add_protected_attributes(detection, row_index)\n","    add_other_attributes(detection, row_index)\n","\n","    return detection"]},{"cell_type":"code","execution_count":null,"id":"d1869986-c9eb-454e-b164-cd5f8b598286","metadata":{"id":"d1869986-c9eb-454e-b164-cd5f8b598286"},"outputs":[],"source":["def add_ground_truth_labels(dataset):\n","    for sample in dataset.iter_samples(autosave=True, progress=True):\n","        sample_annos = gt_df[gt_df['filename'] == sample.filename]\n","        detections = []\n","        for row in sample_annos.iterrows():\n","            row_index = row[0]\n","            detection = create_detection(row_index, sample)\n","            detections.append(detection)\n","        sample[\"ground_truth\"] = fo.Detections(detections=detections)\n","    dataset.add_dynamic_sample_fields()\n","\n"]},{"cell_type":"markdown","id":"bec896ec-9aa5-4033-839d-ced0b1ae54d6","metadata":{"id":"bec896ec-9aa5-4033-839d-ced0b1ae54d6"},"source":["# Add labels"]},{"cell_type":"code","execution_count":null,"id":"d068d1d0-38e0-43b7-b827-5006503182d3","metadata":{"id":"d068d1d0-38e0-43b7-b827-5006503182d3","outputId":"87b675d0-8acc-409f-b3dc-e701feac6af7"},"outputs":[{"name":"stdout","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":[" 100% |█████████████| 31703/31703 [11.8m elapsed, 0s remaining, 43.3 samples/s]      \n"]}],"source":["## add all of the ground truth labels\n","add_ground_truth_labels(dataset)"]},{"cell_type":"markdown","id":"e19e53d3-c90d-467e-83c2-6b1bc5717fdd","metadata":{"id":"e19e53d3-c90d-467e-83c2-6b1bc5717fdd"},"source":["# Add Masks"]},{"cell_type":"code","execution_count":null,"id":"2b485560-887e-47f9-b980-a9460a481942","metadata":{"id":"2b485560-887e-47f9-b980-a9460a481942","outputId":"c1a9d9c9-30bf-49e1-c7a9-8aaa906f335f"},"outputs":[{"name":"stdout","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":[" 100% |█████████████| 31703/31703 [33.8m elapsed, 0s remaining, 13.8 samples/s]      \n"]}],"source":["def add_coco_masks_to_dataset(dataset):\n","    ########## LM ##########\n","    # with s3.open(f's3://{bucket_name}/{annotations_prefix}coco_masks.json', 'rb') as file:\n","    #     coco_masks = json.load(file)\n","\n","    ########## KH ##########\n","    s3 = boto3.client('s3')\n","    bucket_name = 'w210facetdata'\n","    object_key = 'annotations/coco_masks.json'\n","    s3_object = s3.get_object(Bucket=bucket_name, Key=object_key)\n","    s3_file_content = s3_object['Body'].read().decode('utf-8')\n","    coco_masks = json.loads(s3_file_content)\n","\n","\n","    cmas = coco_masks[\"annotations\"]\n","\n","    FILENAME_TO_ID = {\n","        img[\"file_name\"]: img[\"id\"]\n","        for img in coco_masks[\"images\"]\n","    }\n","\n","    CAT_TO_LABEL = {cat[\"id\"]: cat[\"name\"] for cat in coco_masks[\"categories\"]}\n","\n","    for sample in dataset.iter_samples(autosave=True, progress=True):\n","        fn = sample.filename\n","\n","        if fn not in FILENAME_TO_ID:\n","            continue\n","\n","        img_id = FILENAME_TO_ID[fn]\n","        img_width, img_height = sample.metadata.width, sample.metadata.height\n","        sample_annos = [a for a in cmas if a[\"image_id\"] == img_id]\n","        if len(sample_annos) == 0:\n","            continue\n","\n","        coco_detections = []\n","        for ann in sample_annos:\n","            label = CAT_TO_LABEL[ann[\"category_id\"]]\n","            bbox = ann['bbox']\n","            ann_id = ann['ann_id']\n","            person_id = ann['facet_person_id']\n","\n","            mask = maskUtils.decode(ann[\"segmentation\"])\n","            mask = Image.fromarray(255*mask)\n","\n","            ## Change bbox to be in the format [x, y, x, y]\n","            bbox[2] = bbox[0] + bbox[2]\n","            bbox[3] = bbox[1] + bbox[3]\n","\n","            ## Get the cropped image\n","            cropped_mask = np.array(mask.crop(bbox)).astype(bool)\n","\n","            ## Convert to relative [x, y, w, h] coordinates\n","            bbox[2] = bbox[2] - bbox[0]\n","            bbox[3] = bbox[3] - bbox[1]\n","\n","            bbox[0] = bbox[0]/img_width\n","            bbox[1] = bbox[1]/img_height\n","            bbox[2] = bbox[2]/img_width\n","            bbox[3] = bbox[3]/img_height\n","\n","            new_detection = fo.Detection(\n","                label=label,\n","                bounding_box=bbox,\n","                person_id=person_id,\n","                ann_id=ann_id,\n","                mask=cropped_mask,\n","                )\n","            coco_detections.append(new_detection)\n","        sample[\"coco_masks\"] = fo.Detections(detections=coco_detections)\n","\n","## add the masks\n","add_coco_masks_to_dataset(dataset)"]},{"cell_type":"markdown","id":"97a7cb6d-c307-434a-9500-15ae6a4ccadb","metadata":{"id":"97a7cb6d-c307-434a-9500-15ae6a4ccadb"},"source":["# Import Yolo"]},{"cell_type":"code","execution_count":null,"id":"57ed90e2-cd2c-4b69-a7a3-d63e9effea28","metadata":{"id":"57ed90e2-cd2c-4b69-a7a3-d63e9effea28","outputId":"af81de2e-6e6f-4b3e-ed73-95f319637e8b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in /home/ubuntu/.cache/torch/hub/ultralytics_yolov5_master\n","YOLOv5 🚀 2023-10-22 Python-3.10.9 torch-2.1.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients, 48.9 GFLOPs\n","Adding AutoShape... \n"]}],"source":["yolov5 = foz.load_zoo_model('yolov5m-coco-torch')"]},{"cell_type":"code","execution_count":null,"id":"5be206d9-d465-4094-8aff-7b8ede9ccf7f","metadata":{"id":"5be206d9-d465-4094-8aff-7b8ede9ccf7f","outputId":"128bfac3-5365-4373-a32c-126e019c1c93"},"outputs":[{"name":"stdout","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":[" 100% |█████████████| 31703/31703 [24.5m elapsed, 0s remaining, 19.6 samples/s]      \n"]}],"source":["dataset.apply_model(yolov5, label_field=\"yolov5m\")\n","### Just retain the \"person\" detections\n","people_view_values = dataset.filter_labels(\"yolov5m\", F(\"label\") == \"person\").values(\"yolov5m\")\n","dataset.set_values(\"yolov5m\", people_view_values)\n","dataset.save()"]},{"cell_type":"markdown","id":"fa22f25c-8c22-4c4a-ae09-05e07ec88e17","metadata":{"id":"fa22f25c-8c22-4c4a-ae09-05e07ec88e17"},"source":["# Clip classification model --> Replace with teacher/student"]},{"cell_type":"code","execution_count":null,"id":"7880f2c8-3d3a-4744-89cb-b69ca608520c","metadata":{"id":"7880f2c8-3d3a-4744-89cb-b69ca608520c"},"outputs":[],"source":["## get a list of all 52 classes\n","facet_classes = dataset.distinct(\"ground_truth.detections.label\")\n","\n","## instantiate a CLIP model with these classes\n","clip = foz.load_zoo_model(\n","    \"clip-vit-base32-torch\",\n","    text_prompt=\"A photo of a\",\n","    classes=facet_classes,\n",")"]},{"cell_type":"code","execution_count":null,"id":"58a70100-8fa6-4b34-a754-f86b4b3a52b4","metadata":{"id":"58a70100-8fa6-4b34-a754-f86b4b3a52b4","outputId":"7767f9fa-c14b-4537-f660-b95009fbc5fa"},"outputs":[{"name":"stdout","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":[" 100% |█████████████| 49551/49551 [26.3m elapsed, 0s remaining, 31.6 samples/s]      \n"]}],"source":["patch_view = dataset.to_patches(\"ground_truth\")\n","patch_view.apply_model(clip, label_field=\"clip\")\n","dataset.save_view(\"patch_view\", patch_view)"]},{"cell_type":"code","execution_count":null,"id":"339f2b7d-5058-4278-81be-32515887d1c4","metadata":{"id":"339f2b7d-5058-4278-81be-32515887d1c4"},"outputs":[],"source":["IOU_THRESHS = np.round(np.arange(0.5, 1.0, 0.05), 2)"]},{"cell_type":"code","execution_count":null,"id":"14507c5d-78b9-4895-9232-a277c0b61aa3","metadata":{"id":"14507c5d-78b9-4895-9232-a277c0b61aa3"},"outputs":[],"source":["def _evaluate_detection_model(dataset, label_field):\n","    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n","    dataset.evaluate_detections(label_field, \"ground_truth\", eval_key=eval_key, classwise=False)\n","\n","    for sample in dataset.iter_samples(progress=True):\n","        for pred in sample[label_field].detections:\n","            iou_field = f\"{eval_key}_iou\"\n","            if iou_field not in pred:\n","                continue\n","\n","            iou = pred[iou_field]\n","            for it in IOU_THRESHS:\n","                pred[f\"{iou_field}_{str(it).replace('.', '')}\"] = iou >= it\n","        sample.save()"]},{"cell_type":"code","execution_count":null,"id":"02f4e194-37cc-4de9-a76b-51ed0de18ee7","metadata":{"id":"02f4e194-37cc-4de9-a76b-51ed0de18ee7","outputId":"39a94638-2844-40cb-ea83-19cf2e20968f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Evaluating detections...\n"," 100% |█████████████| 31703/31703 [4.6m elapsed, 0s remaining, 68.0 samples/s]       \n"," 100% |█████████████| 31703/31703 [8.5m elapsed, 0s remaining, 48.4 samples/s]       \n"]}],"source":["_evaluate_detection_model(dataset, 'yolov5m')"]},{"cell_type":"code","execution_count":null,"id":"e682ce1b-787e-47db-95b8-fb17ed5d9edd","metadata":{"id":"e682ce1b-787e-47db-95b8-fb17ed5d9edd"},"outputs":[],"source":["def _compute_detection_mAR(sample_collection, label_field):\n","    \"\"\"Computes the mean average recall of the specified detection field.\n","    -- computed as the average over iou thresholds of the recall at\n","    each threshold.\n","    \"\"\"\n","    eval_key = \"eval_\" + label_field.replace(\"-\", \"_\")\n","    iou_recalls = []\n","    for it in IOU_THRESHS:\n","        field_str = f\"{label_field}.detections.{eval_key}_iou_{str(it).replace('.', '')}\"\n","        counts = sample_collection.count_values(field_str)\n","        tp, fn = counts.get(True, 0), counts.get(False, 0)\n","        recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n","        iou_recalls.append(recall)\n","\n","    return np.mean(iou_recalls)"]},{"cell_type":"code","execution_count":null,"id":"8eb73ee8-28ac-4416-8d7a-791959206e45","metadata":{"id":"8eb73ee8-28ac-4416-8d7a-791959206e45"},"outputs":[],"source":["def get_concept_attr_detection_mAR(dataset, label_field, concept, attributes):\n","    sub_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == concept)\n","    for attribute in attributes.items():\n","        if \"skin_tone\" in attribute[0]:\n","            sub_view = sub_view.filter_labels(\"ground_truth\", F(f\"skin_tone.{attribute[0]}\") != 0)\n","        else:\n","            sub_view = sub_view.filter_labels(\"ground_truth\", F(attribute[0]) == attribute[1])\n","    return _compute_detection_mAR(sub_view, label_field)"]},{"cell_type":"code","execution_count":null,"id":"d651365b-d969-456b-aa02-78a47f6d4d48","metadata":{"id":"d651365b-d969-456b-aa02-78a47f6d4d48","outputId":"313f5b96-33e4-41f9-80fb-4321e30d79cf"},"outputs":[{"data":{"text/plain":["0.4259259259259259"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["concept = 'lawman'\n","attributes = {\"hairtype\": \"straight\", \"haircolor\": \"brown\"}\n","get_concept_attr_detection_mAR(dataset, \"yolov5m\", concept, attributes)\n"]},{"cell_type":"code","execution_count":null,"id":"320aa339-ddd0-4e4d-800e-aa6ac8d31b55","metadata":{"id":"320aa339-ddd0-4e4d-800e-aa6ac8d31b55"},"outputs":[],"source":["def _evaluate_classification_model(dataset, prediction_field):\n","    patch_view = dataset.load_saved_view(\"patch_view\")\n","    eval_key = \"eval_\" + prediction_field\n","\n","    for sample in patch_view.iter_samples(progress=True):\n","        sample[eval_key] = (\n","            sample.ground_truth.label == sample[prediction_field].label\n","        )\n","        sample.save()\n","    dataset.save_view(\"patch_view\", patch_view, overwrite=True)"]},{"cell_type":"code","execution_count":null,"id":"42ffe5b3-dfa1-42ad-bb88-52b04622bdf6","metadata":{"id":"42ffe5b3-dfa1-42ad-bb88-52b04622bdf6","outputId":"1481f945-492b-40fb-d655-54e3d0b5fa7b"},"outputs":[{"name":"stdout","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":[" 100% |█████████████| 49551/49551 [4.5m elapsed, 0s remaining, 132.8 samples/s]      \n"]}],"source":["_evaluate_classification_model(dataset, 'clip')"]},{"cell_type":"code","execution_count":null,"id":"f830a851-7ff9-4ee1-9082-3810dc511cbc","metadata":{"id":"f830a851-7ff9-4ee1-9082-3810dc511cbc"},"outputs":[],"source":["def _compute_classification_recall(patch_collection, label_field):\n","    eval_key = \"eval_\" + label_field.split(\"_\")[0]\n","    counts = patch_collection.count_values(eval_key)\n","    tp, fn = counts.get(True, 0), counts.get(False, 0)\n","    recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n","    return recall"]},{"cell_type":"code","execution_count":null,"id":"4ae5f107-e21f-46de-a8d3-3a329aa88916","metadata":{"id":"4ae5f107-e21f-46de-a8d3-3a329aa88916"},"outputs":[],"source":["def get_concept_attr_classification_recall(dataset, label_field, concept, attributes):\n","    patch_view = dataset.load_saved_view(\"patch_view\")\n","    sub_patch_view = patch_view.match(F(\"ground_truth.label\") == concept)\n","    for attribute in attributes.items():\n","        if \"skin_tone\" in attribute[0]:\n","            sub_patch_view = sub_patch_view.match(F(f\"ground_truth.skin_tone.{attribute[0]}\") != 0)\n","        else:\n","            sub_patch_view = sub_patch_view.match(F(f\"ground_truth.{attribute[0]}\") == attribute[1])\n","    return _compute_classification_recall(sub_patch_view, label_field)"]},{"cell_type":"code","execution_count":null,"id":"62bf03f9-f46d-4ffd-bb0f-e868ad9116ca","metadata":{"id":"62bf03f9-f46d-4ffd-bb0f-e868ad9116ca"},"outputs":[],"source":["attribute = {'hairtype': 'curly'}"]},{"cell_type":"code","execution_count":null,"id":"60239f5d-239e-4b3d-af81-a81936cab0eb","metadata":{"id":"60239f5d-239e-4b3d-af81-a81936cab0eb","outputId":"623060ab-03a9-4a35-bb2c-8ffba1e6e7d1"},"outputs":[{"data":{"text/plain":["0.12"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["get_concept_attr_classification_recall(dataset, \"clip\", concept, attribute)\n"]},{"cell_type":"code","execution_count":null,"id":"1c238e25-c163-4774-82dd-f9a96bb3e71c","metadata":{"id":"1c238e25-c163-4774-82dd-f9a96bb3e71c"},"outputs":[],"source":["def get_concept_attr_recall(dataset, label_field, concept, attribute):\n","    if label_field in dataset.get_field_schema().keys():\n","        return get_concept_attr_detection_mAR(dataset, label_field, concept, attribute)\n","    else:\n","        return get_concept_attr_classification_recall(dataset, label_field, concept, attribute)"]},{"cell_type":"code","execution_count":null,"id":"8e153b95-8f68-4a3c-a562-f1eee4136004","metadata":{"id":"8e153b95-8f68-4a3c-a562-f1eee4136004"},"outputs":[],"source":["def compute_disparity(dataset, label_field, concept, attribute1, attribute2):\n","    recall1 = get_concept_attr_recall(dataset, label_field, concept, attribute1)\n","    recall2 = get_concept_attr_recall(dataset, label_field, concept, attribute2)\n","    return recall1 - recall2"]},{"cell_type":"code","execution_count":null,"id":"55758f91-25ff-4b29-80b2-8e82878221af","metadata":{"id":"55758f91-25ff-4b29-80b2-8e82878221af","outputId":"aca0c994-2f50-4148-f842-33915332626b"},"outputs":[{"name":"stdout","output_type":"stream","text":["astronaut: 0.030219780219780223\n","singer: 0.0045246758093104855\n","judge: 0.007407407407407404\n","student: 0.0719835227865262\n"]}],"source":["attrs1 = {\"perceived_gender_presentation\": \"fem\"}\n","attrs2 = {\"hairtype\": \"straight\"}\n","for concept in [\"astronaut\", \"singer\", \"judge\", \"student\"]:\n","    disparity = compute_disparity(dataset, \"clip\", concept, attrs1, attrs2)\n","    print(f\"{concept}: {disparity}\")"]},{"cell_type":"code","execution_count":null,"id":"9936a1d6-090a-4310-9001-de6b5b39c2aa","metadata":{"id":"9936a1d6-090a-4310-9001-de6b5b39c2aa","outputId":"eba23902-a28b-4d8a-ccda-f721fab35553"},"outputs":[{"data":{"text/plain":["0.5415617128463476"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["get_concept_attr_classification_recall(dataset, 'clip', 'singer', attrs1)"]},{"cell_type":"markdown","id":"434a7918-51d0-41f4-a53f-a2324a250437","metadata":{"id":"434a7918-51d0-41f4-a53f-a2324a250437"},"source":["# Experimenting with uploading custom models"]},{"cell_type":"code","execution_count":null,"id":"c9a1dc98-17cc-4059-ae0a-df8e32a22ad3","metadata":{"id":"c9a1dc98-17cc-4059-ae0a-df8e32a22ad3"},"outputs":[],"source":["import fiftyone.utils.torch as fout\n","from torchvision.models import resnet50\n","import torchvision\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import fiftyone.core.expressions as foe\n","from fiftyone import ViewField as VF\n","import json"]},{"cell_type":"code","execution_count":null,"id":"f06058c4-f25f-4905-8c3f-e82e714e67e6","metadata":{"id":"f06058c4-f25f-4905-8c3f-e82e714e67e6"},"outputs":[],"source":["# Load the model architecture\n","exec(open('model_arch.py').read())\n","\n","# Create an instance of the model and load the saved weights\n","model = StudentCNN()\n","model.load_state_dict(torch.load('student_model.pth'))\n","model.eval()\n","\n","def make_data_loader(image_paths, sample_ids, batch_size):\n","    mean = [0.4914, 0.4822, 0.4465]\n","    std = [0.2023, 0.1994, 0.2010]\n","    transforms = torchvision.transforms.Compose(\n","        [\n","            torchvision.transforms.Resize((256, 256)),\n","            torchvision.transforms.ToTensor(),\n","            torchvision.transforms.Normalize(mean, std),\n","        ]\n","    )\n","    dataset = fout.TorchImageDataset(\n","        image_paths, sample_ids=sample_ids, transform=transforms\n","    )\n","    return DataLoader(dataset, batch_size=batch_size, num_workers=4)\n","\n","# Set the number of new classes\n","num_new_classes = 52\n","\n","# Move the model to the appropriate device (e.g., GPU if available)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","student = model.to(device)"]},{"cell_type":"code","execution_count":null,"id":"314e09e7-065a-4a02-9d35-b2ea2eba1dcb","metadata":{"id":"314e09e7-065a-4a02-9d35-b2ea2eba1dcb"},"outputs":[],"source":["# # Load the saved architecture\n","# with open('student_architecture.json', 'r') as f:\n","#     student_architecture = json.load(f)\n","\n","# # Define the Student model based on the loaded architecture\n","# class Student(nn.Module):\n","#     def __init__(self, num_classes):\n","#         super(Student, self).__init__()\n","#         self.conv1 = nn.Conv2d(*student_architecture['conv1'])\n","#         self.pool = nn.MaxPool2d(*student_architecture['pool'])\n","#         self.fc1 = nn.Linear(*student_architecture['fc1'])\n","#         self.fc2 = nn.Linear(student_architecture['fc2'][0], num_classes)  # new number of classes\n","\n","#     def forward(self, x):\n","#         x = self.pool(F.relu(self.conv1(x)))\n","#         x = x.view(-1, self.fc1.in_features)\n","#         x = F.relu(self.fc1(x))\n","#         x = self.fc2(x)\n","#         return x\n","\n","# # Create a Student model instance with the new number of classes\n","# num_new_classes = 1000  # replace with the actual number of new classes\n","# student = Student(num_new_classes)\n","\n","# # Load the model weights\n","# state_dict = torch.load('student_weights.pth')\n","\n","# # Remove the weights of the fc2 layer from the saved state_dict as we are going to initialize it randomly\n","# state_dict = {k: v for k, v in state_dict.items() if \"fc2\" not in k}\n","\n","# # Load the state_dict into the student model\n","# student.load_state_dict(state_dict, strict=False)\n","\n","# # Move the model to the appropriate device (e.g., GPU if available)\n","# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# student = student.to(device)"]},{"cell_type":"code","execution_count":null,"id":"c454b58a-fc55-40c1-a873-1464f6d3c57f","metadata":{"id":"c454b58a-fc55-40c1-a873-1464f6d3c57f"},"outputs":[],"source":["# # Define the Student model\n","# class Student(nn.Module):\n","#     def __init__(self):\n","#         super(Student, self).__init__()\n","#         self.conv1 = nn.Conv2d(3, 16, 5)\n","#         self.pool = nn.MaxPool2d(2, 2)\n","#         self.fc1 = nn.Linear(16*14*14, 120)\n","#         self.fc2 = nn.Linear(120, 10)\n","\n","#     def forward(self, x):\n","#         x = self.pool(F.relu(self.conv1(x)))\n","#         x = x.view(-1, 16*14*14)\n","#         x = F.relu(self.fc1(x))\n","#         x = self.fc2(x)\n","#         return x\n","\n","# # Create a new instance of the Student model and load the pretrained weights\n","# model = Student()\n","# model.load_state_dict(torch.load('student.pth'))\n","# model.eval()\n","\n","# # Set the number of new classes\n","# num_new_classes = 52\n","\n","# # Modify the final fully connected (fc) layer\n","# # Get the number of input features to the fc2 layer\n","# in_features = model.fc2.in_features\n","\n","# # Replace the fc2 layer with a new one for the desired number of classes\n","# model.fc2 = nn.Linear(in_features, num_new_classes)\n","\n","# def make_data_loader(image_paths, sample_ids, batch_size):\n","#     mean = [0.4914, 0.4822, 0.4465]\n","#     std = [0.2023, 0.1994, 0.2010]\n","#     transforms = torchvision.transforms.Compose(\n","#         [\n","#             torchvision.transforms.Resize((256, 256)),\n","#             torchvision.transforms.ToTensor(),\n","#             torchvision.transforms.Normalize(mean, std),\n","#         ]\n","#     )\n","#     dataset = fout.TorchImageDataset(\n","#         image_paths, sample_ids=sample_ids, transform=transforms\n","#     )\n","#     return DataLoader(dataset, batch_size=batch_size, num_workers=4)"]},{"cell_type":"code","execution_count":null,"id":"cebd47cf-baf9-45d5-8f69-a4fd3f0fbb96","metadata":{"id":"cebd47cf-baf9-45d5-8f69-a4fd3f0fbb96"},"outputs":[],"source":["# model = resnet50(pretrained=True)\n","# num_new_classes = 52\n","\n","# # Modify the final fully connected (fc) layer\n","# # Get the number of input features to the fc layer\n","# in_features = model.fc.in_features\n","\n","# # Replace the fc layer with a new one for the desired number of classes\n","# model.fc = nn.Linear(in_features, num_new_classes)\n","\n","# def make_data_loader(image_paths, sample_ids, batch_size):\n","#     mean = [0.4914, 0.4822, 0.4465]\n","#     std = [0.2023, 0.1994, 0.2010]\n","#     transforms = torchvision.transforms.Compose(\n","#         [\n","#             torchvision.transforms.Resize((256, 256)),\n","#             torchvision.transforms.ToTensor(),\n","#             torchvision.transforms.Normalize(mean, std),\n","#         ]\n","#     )\n","#     dataset = fout.TorchImageDataset(\n","#         image_paths, sample_ids=sample_ids, transform=transforms\n","#     )\n","#     return DataLoader(dataset, batch_size=batch_size, num_workers=4)"]},{"cell_type":"code","execution_count":null,"id":"fff460c1-0bdf-46f4-949d-a620be5f2f6d","metadata":{"id":"fff460c1-0bdf-46f4-949d-a620be5f2f6d"},"outputs":[],"source":["def predict(model, imgs):\n","    logits = model(imgs).detach().cpu().numpy()\n","    predictions = np.argmax(logits, axis=1)\n","    # predictions = np.argmax(logits, axis=1) - 1\n","    # print(f'Max prediction: {np.max(predictions)}, Min prediction: {np.min(predictions)}')\n","    odds = np.exp(logits)\n","    confidences = np.max(odds, axis=1) / np.sum(odds, axis=1)\n","    return predictions, confidences"]},{"cell_type":"code","execution_count":null,"id":"5d813418-51c8-442b-b2da-5e58d5d32886","metadata":{"id":"5d813418-51c8-442b-b2da-5e58d5d32886"},"outputs":[],"source":["num_samples = 1000\n","batch_size = 5\n","\n","view = dataset.take(num_samples, seed=51)\n","classes = []\n","for i in view.iter_samples():\n","    classes.append(i.ground_truth.detections[0].label)\n","\n","image_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view])\n","data_loader = make_data_loader(image_paths, sample_ids, batch_size)\n"]},{"cell_type":"code","execution_count":null,"id":"37b6016d-aa18-4fc5-832b-f71909a6184b","metadata":{"id":"37b6016d-aa18-4fc5-832b-f71909a6184b"},"outputs":[],"source":["# num_samples = 1000\n","# batch_size = 5\n","\n","# view = dataset.take(num_samples, seed=51)\n","# classes = set()  # Use a set to collect unique classes\n","# for i in view.iter_samples():\n","#     if i.ground_truth.detections:  # Check if detections is not empty\n","#         classes.add(i.ground_truth.detections[0].label)\n","#     else:\n","#         print(f\"Sample {i.id} has no detections\")\n","\n","# image_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view])\n","# data_loader = make_data_loader(image_paths, sample_ids, batch_size)"]},{"cell_type":"code","execution_count":null,"id":"af312860-74ff-4f0d-abb1-d05ffa7dbb28","metadata":{"id":"af312860-74ff-4f0d-abb1-d05ffa7dbb28"},"outputs":[],"source":["# # Convert classes set to a list and create a mapping from class labels to indices\n","# classes = list(classes)\n","# class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n","\n","# image_paths, sample_ids = zip(*[(s.filepath, s.id) for s in view])\n","# data_loader = make_data_loader(image_paths, sample_ids, batch_size)"]},{"cell_type":"code","execution_count":null,"id":"0ecb3bdd-31ef-4a66-afee-f6ed696f08cc","metadata":{"id":"0ecb3bdd-31ef-4a66-afee-f6ed696f08cc","outputId":"2110eb89-7cd0-4b1e-a111-103d7787ae76"},"outputs":[{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (5x1032256 and 14400x100)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[75], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m imgs, sample_ids \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m      6\u001b[0m     imgs \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     predictions, confidences \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Add predictions to your FiftyOne dataset\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sample_id, prediction, confidence \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m     11\u001b[0m         sample_ids, predictions, confidences\n\u001b[1;32m     12\u001b[0m     ):\n","Cell \u001b[0;32mIn[71], line 2\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(model, imgs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(model, imgs):\n\u001b[0;32m----> 2\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      3\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# predictions = np.argmax(logits, axis=1) - 1\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# print(f'Max prediction: {np.max(predictions)}, Min prediction: {np.min(predictions)}')\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m<string>:29\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (5x1032256 and 14400x100)"]}],"source":["#\n","# Perform prediction and store results in dataset\n","#\n","\n","for imgs, sample_ids in data_loader:\n","    imgs = imgs.to(device)\n","    predictions, confidences = predict(model, imgs)\n","\n","    # Add predictions to your FiftyOne dataset\n","    for sample_id, prediction, confidence in zip(\n","        sample_ids, predictions, confidences\n","    ):\n","        sample = dataset[sample_id]\n","        sample[\"pred\"] = fo.Classification(\n","            label=classes[prediction],  # Use the mapping to get class labels\n","            confidence=confidence,\n","        )\n","        sample.save()"]},{"cell_type":"code","execution_count":null,"id":"364c1cc6-e500-4a32-9b5b-6425e21864db","metadata":{"id":"364c1cc6-e500-4a32-9b5b-6425e21864db"},"outputs":[],"source":["def _evaluate_classification_modelr(dataset, prediction_field):\n","    eval_key = \"eval_\" + prediction_field\n","\n","    for sample in dataset.iter_samples(progress=True):\n","        sample[eval_key] = (\n","            sample.ground_truth.detections[0].label == sample[prediction_field].label\n","        )\n","        sample.save()"]},{"cell_type":"code","execution_count":null,"id":"6e5764e5-4859-4b2b-8e04-68224437e82a","metadata":{"id":"6e5764e5-4859-4b2b-8e04-68224437e82a"},"outputs":[],"source":["# def _evaluate_classification_modelr(dataset, prediction_field):\n","#     eval_key = \"eval_\" + prediction_field\n","\n","#     for sample in dataset.iter_samples(progress=True):\n","#         # Check if ground_truth.detections is not empty\n","#         if sample.ground_truth.detections:\n","#             sample[eval_key] = (\n","#                 sample.ground_truth.detections[0].label == sample[prediction_field].label\n","#             )\n","#             sample.save()\n","#         else:\n","#             print(f\"Sample {sample.id} has no detections\")"]},{"cell_type":"code","execution_count":null,"id":"1067a92a-6d53-4d62-b512-cb43083d3777","metadata":{"id":"1067a92a-6d53-4d62-b512-cb43083d3777","outputId":"270cbd25-931a-436e-a0a7-f00c850c5ad1"},"outputs":[{"name":"stdout","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":[" 100% |███████████████| 1000/1000 [15.5s elapsed, 0s remaining, 60.6 samples/s]      \n"]}],"source":["_evaluate_classification_modelr(view, 'pred')"]},{"cell_type":"code","execution_count":null,"id":"f7f87bbe-6a3c-40da-86b3-0baee1ba3cbd","metadata":{"id":"f7f87bbe-6a3c-40da-86b3-0baee1ba3cbd"},"outputs":[],"source":["def _compute_classification_recall(patch_collection, label_field):\n","    eval_key = \"eval_\" + label_field\n","    counts = patch_collection.count_values(eval_key)\n","    tp, fn = counts.get(True, 0), counts.get(False, 0)\n","    recall = tp/float(tp + fn) if tp + fn > 0 else 0.0\n","    return recall"]},{"cell_type":"code","execution_count":null,"id":"204e607b-164f-4326-9971-f09bbd4ce18a","metadata":{"id":"204e607b-164f-4326-9971-f09bbd4ce18a"},"outputs":[],"source":["def get_concept_attr_classification_recall(dataset, label_field, concept, attributes):\n","    sub_patch_view = dataset.filter_labels(\"ground_truth\", VF(\"label\") == concept)  # Use foe instead of F\n","    for attribute in attributes.items():\n","        if \"skin_tone\" in attribute[0]:\n","            sub_patch_view = sub_patch_view.filter_labels('ground_truth', VF(f\"skin_tone.{attribute[0]}\") != 0)  # Use foe instead of F\n","        else:\n","            sub_patch_view = sub_patch_view.filter_labels('ground_truth', VF(f\"{attribute[0]}\") == attribute[1])  # Use foe instead of F\n","    return _compute_classification_recall(sub_patch_view, label_field)"]},{"cell_type":"code","execution_count":null,"id":"c2980f61-f140-4a86-9e41-fee7161fb3f3","metadata":{"id":"c2980f61-f140-4a86-9e41-fee7161fb3f3","outputId":"044ff075-8850-4bb1-baba-e4a8fb69a30d"},"outputs":[{"data":{"text/plain":["0.028985507246376812"]},"execution_count":94,"metadata":{},"output_type":"execute_result"}],"source":["concept = 'lawman'\n","attributes = {\"perceived_gender_presentation\": \"masc\"}\n","\n","get_concept_attr_classification_recall(view, 'pred', concept, attributes)"]},{"cell_type":"code","execution_count":null,"id":"8fd16a3a-4212-486d-9107-5011a92feae6","metadata":{"id":"8fd16a3a-4212-486d-9107-5011a92feae6","outputId":"2d1ef140-eaed-457b-fd1b-e7c8508c64e9"},"outputs":[{"ename":"NameError","evalue":"name 'view' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mview\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'view' is not defined"]}],"source":["view"]},{"cell_type":"code","execution_count":null,"id":"1e3fc57c-4357-407f-b77e-6bedeea66241","metadata":{"id":"1e3fc57c-4357-407f-b77e-6bedeea66241","outputId":"e3499f3d-56f2-4a7a-f0f6-92d01aa7cdf4"},"outputs":[{"ename":"NameError","evalue":"name 'dataset' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\n","\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"]}],"source":["dataset"]},{"cell_type":"code","execution_count":null,"id":"e9b2d41d-ae7a-4c9a-bd2e-9c4bd988babd","metadata":{"id":"e9b2d41d-ae7a-4c9a-bd2e-9c4bd988babd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0faead1e-1785-4b48-82e2-f9201edc0657","metadata":{"id":"0faead1e-1785-4b48-82e2-f9201edc0657"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"5ef5258e-8ee2-4394-9a71-eb282c79f7f0","metadata":{"id":"5ef5258e-8ee2-4394-9a71-eb282c79f7f0"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}