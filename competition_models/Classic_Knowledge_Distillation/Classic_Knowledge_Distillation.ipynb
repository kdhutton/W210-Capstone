{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1503.02531\n",
        "\n"
      ],
      "metadata": {
        "id": "97j6YF4xoUFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/kuangliu/pytorch-cifar"
      ],
      "metadata": {
        "id": "zsa1ZJyJpUFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "tZlxirlEoeAL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
        "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, batch_size=100, shuffle=False, num_workers=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmfxDlfsofps",
        "outputId": "f4ccf66f-5308-4e4a-b6db-55c67733cdc7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define a simple ResNet model for the sake of this example (you can use a more complex one if needed)\n",
        "# Source: https://github.com/kuangliu/pytorch-cifar\n",
        "from models import ResNet18\n",
        "\n",
        "# Create the teacher and student models\n",
        "teacher = ResNet18().to(device)\n",
        "student = ResNet18().to(device)"
      ],
      "metadata": {
        "id": "Gk-qKUweohWZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assume the teacher is pre-trained, so freeze its parameters\n",
        "for param in teacher.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "Ic-Ojxg3ohPo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "epKCsOIlpLtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# NOTE: In practice, you would train the teacher first or load its pre-trained weights.\n",
        "# For the sake of this example, we'll use it without training.\n",
        "\n",
        "# Define the KD loss function\n",
        "def knowledge_distillation_loss(output, target, teacher_output, temperature, alpha):\n",
        "    hard_loss = F.cross_entropy(output, target) * alpha\n",
        "    soft_loss = (F.kl_div(F.log_softmax(output/temperature, dim=1),\n",
        "                          F.softmax(teacher_output/temperature, dim=1),\n",
        "                          reduction='batchmean') * (temperature**2) * (1. - alpha))\n",
        "    return hard_loss + soft_loss\n"
      ],
      "metadata": {
        "id": "m4g-ohx3pLo8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8tujD7BoRPj",
        "outputId": "bee4779a-0327-4b6b-ca37-2783b867707c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/782], Loss: 1.5142\n",
            "Epoch [1/5], Step [200/782], Loss: 1.9123\n",
            "Epoch [1/5], Step [300/782], Loss: 1.3289\n",
            "Epoch [1/5], Step [400/782], Loss: 1.5124\n",
            "Epoch [1/5], Step [500/782], Loss: 1.1083\n",
            "Epoch [1/5], Step [600/782], Loss: 1.1154\n",
            "Epoch [1/5], Step [700/782], Loss: 1.1642\n",
            "Epoch [2/5], Step [100/782], Loss: 0.8856\n",
            "Epoch [2/5], Step [200/782], Loss: 0.9408\n",
            "Epoch [2/5], Step [300/782], Loss: 0.9530\n",
            "Epoch [2/5], Step [400/782], Loss: 0.7551\n",
            "Epoch [2/5], Step [500/782], Loss: 0.8180\n",
            "Epoch [2/5], Step [600/782], Loss: 0.7796\n",
            "Epoch [2/5], Step [700/782], Loss: 0.7543\n",
            "Epoch [3/5], Step [100/782], Loss: 0.6007\n",
            "Epoch [3/5], Step [200/782], Loss: 0.7928\n",
            "Epoch [3/5], Step [300/782], Loss: 0.5279\n",
            "Epoch [3/5], Step [400/782], Loss: 0.8048\n",
            "Epoch [3/5], Step [500/782], Loss: 0.5594\n",
            "Epoch [3/5], Step [600/782], Loss: 0.7844\n",
            "Epoch [3/5], Step [700/782], Loss: 0.7055\n",
            "Epoch [4/5], Step [100/782], Loss: 0.5335\n",
            "Epoch [4/5], Step [200/782], Loss: 0.4404\n",
            "Epoch [4/5], Step [300/782], Loss: 0.5096\n",
            "Epoch [4/5], Step [400/782], Loss: 0.4900\n",
            "Epoch [4/5], Step [500/782], Loss: 0.5469\n",
            "Epoch [4/5], Step [600/782], Loss: 0.6741\n",
            "Epoch [4/5], Step [700/782], Loss: 0.5354\n",
            "Epoch [5/5], Step [100/782], Loss: 0.3959\n",
            "Epoch [5/5], Step [200/782], Loss: 0.3596\n",
            "Epoch [5/5], Step [300/782], Loss: 0.4582\n",
            "Epoch [5/5], Step [400/782], Loss: 0.4437\n",
            "Epoch [5/5], Step [500/782], Loss: 0.4811\n",
            "Epoch [5/5], Step [600/782], Loss: 0.5340\n",
            "Epoch [5/5], Step [700/782], Loss: 0.5177\n",
            "Finished KD Training!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Training parameters\n",
        "epochs = 5\n",
        "alpha = 0.9\n",
        "temperature = 4.0\n",
        "optimizer = optim.SGD(student.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    student.train()\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        student_outputs = student(inputs)\n",
        "        teacher_outputs = teacher(inputs)\n",
        "\n",
        "        loss = knowledge_distillation_loss(student_outputs, labels, teacher_outputs, temperature, alpha)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "print(\"Finished KD Training!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oRaXN3zzoSHS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}